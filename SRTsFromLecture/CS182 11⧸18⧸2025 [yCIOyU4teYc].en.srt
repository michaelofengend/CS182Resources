1
00:02:13,520 --> 00:02:15,589

We're going to continue. I have a couple

2
00:02:15,589 --> 00:02:15,599
We're going to continue. I have a couple
 

3
00:02:15,599 --> 00:02:18,070
We're going to continue. I have a couple
of announcements I want to make first.

4
00:02:18,070 --> 00:02:18,080
of announcements I want to make first.
 

5
00:02:18,080 --> 00:02:20,390
of announcements I want to make first.
So, uh, the main announcement I wrote up

6
00:02:20,390 --> 00:02:20,400
So, uh, the main announcement I wrote up
 

7
00:02:20,400 --> 00:02:23,430
So, uh, the main announcement I wrote up
there is we noticed that many students,

8
00:02:23,430 --> 00:02:23,440
there is we noticed that many students,
 

9
00:02:23,440 --> 00:02:25,030
there is we noticed that many students,
most students in the class have not

10
00:02:25,030 --> 00:02:25,040
most students in the class have not
 

11
00:02:25,040 --> 00:02:27,589
most students in the class have not
filled out the survey we, um, gave out.

12
00:02:27,589 --> 00:02:27,599
filled out the survey we, um, gave out.
 

13
00:02:27,599 --> 00:02:29,510
filled out the survey we, um, gave out.
It doesn't take that long, but we got

14
00:02:29,510 --> 00:02:29,520
It doesn't take that long, but we got
 

15
00:02:29,520 --> 00:02:31,270
It doesn't take that long, but we got
very useful responses from a small

16
00:02:31,270 --> 00:02:31,280
very useful responses from a small
 

17
00:02:31,280 --> 00:02:32,550
very useful responses from a small
number of students. We want to get from

18
00:02:32,550 --> 00:02:32,560
number of students. We want to get from
 

19
00:02:32,560 --> 00:02:34,550
number of students. We want to get from
everybody to see what people have to

20
00:02:34,550 --> 00:02:34,560
everybody to see what people have to
 

21
00:02:34,560 --> 00:02:37,990
everybody to see what people have to
say. So, we decided to do the thing that

22
00:02:37,990 --> 00:02:38,000
say. So, we decided to do the thing that
 

23
00:02:38,000 --> 00:02:40,470
say. So, we decided to do the thing that
many other classes do. We're kind of not

24
00:02:40,470 --> 00:02:40,480
many other classes do. We're kind of not
 

25
00:02:40,480 --> 00:02:42,790
many other classes do. We're kind of not
wanting to deal with this. Um, but we're

26
00:02:42,790 --> 00:02:42,800
wanting to deal with this. Um, but we're
 

27
00:02:42,800 --> 00:02:45,030
wanting to deal with this. Um, but we're
going to give everybody remember there's

28
00:02:45,030 --> 00:02:45,040
going to give everybody remember there's
 

29
00:02:45,040 --> 00:02:47,190
going to give everybody remember there's
the grading on this. There's no curve.

30
00:02:47,190 --> 00:02:47,200
the grading on this. There's no curve.
 

31
00:02:47,200 --> 00:02:49,830
the grading on this. There's no curve.
So, these 3% of points actually mean

32
00:02:49,830 --> 00:02:49,840
So, these 3% of points actually mean
 

33
00:02:49,840 --> 00:02:52,229
So, these 3% of points actually mean
something uh to everybody. Um, we'll

34
00:02:52,229 --> 00:02:52,239
something uh to everybody. Um, we'll
 

35
00:02:52,239 --> 00:02:54,550
something uh to everybody. Um, we'll
give it to everybody if 75% of the class

36
00:02:54,550 --> 00:02:54,560
give it to everybody if 75% of the class
 

37
00:02:54,560 --> 00:02:57,750
give it to everybody if 75% of the class
does the survey. Um, so please do the

38
00:02:57,750 --> 00:02:57,760
does the survey. Um, so please do the
 

39
00:02:57,760 --> 00:02:59,589
does the survey. Um, so please do the
survey. Encourage your friends to do the

40
00:02:59,589 --> 00:02:59,599
survey. Encourage your friends to do the
 

41
00:02:59,599 --> 00:03:03,270
survey. Encourage your friends to do the
survey. Uh, we just want to get

42
00:03:03,270 --> 00:03:03,280
survey. Uh, we just want to get
 

43
00:03:03,280 --> 00:03:06,149
survey. Uh, we just want to get
information uh from everybody to help

44
00:03:06,149 --> 00:03:06,159
information uh from everybody to help
 

45
00:03:06,159 --> 00:03:10,470
information uh from everybody to help
us. Uh, so the other announcement we

46
00:03:10,470 --> 00:03:10,480
us. Uh, so the other announcement we
 

47
00:03:10,480 --> 00:03:12,470
us. Uh, so the other announcement we
posted a couple of things on ED. I want

48
00:03:12,470 --> 00:03:12,480
posted a couple of things on ED. I want
 

49
00:03:12,480 --> 00:03:14,149
posted a couple of things on ED. I want
to make sure everyone's aware of them.

50
00:03:14,149 --> 00:03:14,159
to make sure everyone's aware of them.
 

51
00:03:14,159 --> 00:03:17,990
to make sure everyone's aware of them.
First, we uh said that we noticed that

52
00:03:17,990 --> 00:03:18,000
First, we uh said that we noticed that
 

53
00:03:18,000 --> 00:03:20,149
First, we uh said that we noticed that
the compute allocations, which we admit

54
00:03:20,149 --> 00:03:20,159
the compute allocations, which we admit
 

55
00:03:20,159 --> 00:03:22,309
the compute allocations, which we admit
our system is not the best because we

56
00:03:22,309 --> 00:03:22,319
our system is not the best because we
 

57
00:03:22,319 --> 00:03:24,630
our system is not the best because we
just have too little. But we noticed

58
00:03:24,630 --> 00:03:24,640
just have too little. But we noticed
 

59
00:03:24,640 --> 00:03:26,550
just have too little. But we noticed
that the little we did have wasn't

60
00:03:26,550 --> 00:03:26,560
that the little we did have wasn't
 

61
00:03:26,560 --> 00:03:28,710
that the little we did have wasn't
always being used. So lots of slots that

62
00:03:28,710 --> 00:03:28,720
always being used. So lots of slots that
 

63
00:03:28,720 --> 00:03:30,070
always being used. So lots of slots that
we' assigned to people weren't being

64
00:03:30,070 --> 00:03:30,080
we' assigned to people weren't being
 

65
00:03:30,080 --> 00:03:32,550
we' assigned to people weren't being
used. Um by so we decided to do is

66
00:03:32,550 --> 00:03:32,560
used. Um by so we decided to do is
 

67
00:03:32,560 --> 00:03:34,789
used. Um by so we decided to do is
remove the slot restriction. Um so you

68
00:03:34,789 --> 00:03:34,799
remove the slot restriction. Um so you
 

69
00:03:34,799 --> 00:03:36,789
remove the slot restriction. Um so you
go ahead and use it. Um if you find that

70
00:03:36,789 --> 00:03:36,799
go ahead and use it. Um if you find that
 

71
00:03:36,799 --> 00:03:38,470
go ahead and use it. Um if you find that
it's super busy or you can't get access

72
00:03:38,470 --> 00:03:38,480
it's super busy or you can't get access
 

73
00:03:38,480 --> 00:03:41,270
it's super busy or you can't get access
or it's not working then like post on

74
00:03:41,270 --> 00:03:41,280
or it's not working then like post on
 

75
00:03:41,280 --> 00:03:42,789
or it's not working then like post on
especially if you had that slot assigned

76
00:03:42,789 --> 00:03:42,799
especially if you had that slot assigned
 

77
00:03:42,799 --> 00:03:43,830
especially if you had that slot assigned
to you or something like you know post

78
00:03:43,830 --> 00:03:43,840
to you or something like you know post
 

79
00:03:43,840 --> 00:03:46,869
to you or something like you know post
on ed and say hey you know can you other

80
00:03:46,869 --> 00:03:46,879
on ed and say hey you know can you other
 

81
00:03:46,879 --> 00:03:48,710
on ed and say hey you know can you other
people get off I need to use it um kind

82
00:03:48,710 --> 00:03:48,720
people get off I need to use it um kind
 

83
00:03:48,720 --> 00:03:50,869
people get off I need to use it um kind
of thing but we we'd rather people use

84
00:03:50,869 --> 00:03:50,879
of thing but we we'd rather people use
 

85
00:03:50,879 --> 00:03:54,869
of thing but we we'd rather people use
the compute than not use the compute. So

86
00:03:54,869 --> 00:03:54,879
the compute than not use the compute. So
 

87
00:03:54,879 --> 00:03:56,949
the compute than not use the compute. So
we we've kind of removed that

88
00:03:56,949 --> 00:03:56,959
we we've kind of removed that
 

89
00:03:56,959 --> 00:03:58,149
we we've kind of removed that
restriction. Hopefully it'll work out

90
00:03:58,149 --> 00:03:58,159
restriction. Hopefully it'll work out
 

91
00:03:58,159 --> 00:04:03,509
restriction. Hopefully it'll work out
better for people who are using it. Um

92
00:04:03,509 --> 00:04:03,519

 

93
00:04:03,519 --> 00:04:05,589

the other announcement we made was we

94
00:04:05,589 --> 00:04:05,599
the other announcement we made was we
 

95
00:04:05,599 --> 00:04:07,429
the other announcement we made was we
know that we asked you for your draft

96
00:04:07,429 --> 00:04:07,439
know that we asked you for your draft
 

97
00:04:07,439 --> 00:04:11,030
know that we asked you for your draft
report to be submitted this Thursday.

98
00:04:11,030 --> 00:04:11,040
report to be submitted this Thursday.
 

99
00:04:11,040 --> 00:04:12,710
report to be submitted this Thursday.
The point of doing that is so that we

100
00:04:12,710 --> 00:04:12,720
The point of doing that is so that we
 

101
00:04:12,720 --> 00:04:15,429
The point of doing that is so that we
can quickly assign everyone to be a peer

102
00:04:15,429 --> 00:04:15,439
can quickly assign everyone to be a peer
 

103
00:04:15,439 --> 00:04:17,110
can quickly assign everyone to be a peer
reviewer and you can get your peer

104
00:04:17,110 --> 00:04:17,120
reviewer and you can get your peer
 

105
00:04:17,120 --> 00:04:19,590
reviewer and you can get your peer
review done over the weekend um before

106
00:04:19,590 --> 00:04:19,600
review done over the weekend um before
 

107
00:04:19,600 --> 00:04:21,349
review done over the weekend um before
Thanksgiving to give everyone else

108
00:04:21,349 --> 00:04:21,359
Thanksgiving to give everyone else
 

109
00:04:21,359 --> 00:04:22,790
Thanksgiving to give everyone else
feedback. So you can keep people can

110
00:04:22,790 --> 00:04:22,800
feedback. So you can keep people can
 

111
00:04:22,800 --> 00:04:24,070
feedback. So you can keep people can
keep working on their projects and get

112
00:04:24,070 --> 00:04:24,080
keep working on their projects and get
 

113
00:04:24,080 --> 00:04:26,230
keep working on their projects and get
get some useful feedback.

114
00:04:26,230 --> 00:04:26,240
get some useful feedback.
 

115
00:04:26,240 --> 00:04:28,550
get some useful feedback.
We've posted a description of what we're

116
00:04:28,550 --> 00:04:28,560
We've posted a description of what we're
 

117
00:04:28,560 --> 00:04:31,350
We've posted a description of what we're
expecting from the report and as well as

118
00:04:31,350 --> 00:04:31,360
expecting from the report and as well as
 

119
00:04:31,360 --> 00:04:35,189
expecting from the report and as well as
a link that can be used to submit the um

120
00:04:35,189 --> 00:04:35,199
a link that can be used to submit the um
 

121
00:04:35,199 --> 00:04:38,629
a link that can be used to submit the um
draft. We will we're adding everybody

122
00:04:38,629 --> 00:04:38,639
draft. We will we're adding everybody
 

123
00:04:38,639 --> 00:04:41,510
draft. We will we're adding everybody
there manually using the class upload.

124
00:04:41,510 --> 00:04:41,520
there manually using the class upload.
 

125
00:04:41,520 --> 00:04:44,150
there manually using the class upload.
So you should get an email. So if you're

126
00:04:44,150 --> 00:04:44,160
So you should get an email. So if you're
 

127
00:04:44,160 --> 00:04:46,710
So you should get an email. So if you're
if it's not working for you uh you know

128
00:04:46,710 --> 00:04:46,720
if it's not working for you uh you know
 

129
00:04:46,720 --> 00:04:50,550
if it's not working for you uh you know
tomorrow uh you know please you know

130
00:04:50,550 --> 00:04:50,560
tomorrow uh you know please you know
 

131
00:04:50,560 --> 00:04:52,150
tomorrow uh you know please you know
ping on it and we'll we'll we'll make

132
00:04:52,150 --> 00:04:52,160
ping on it and we'll we'll we'll make
 

133
00:04:52,160 --> 00:04:54,230
ping on it and we'll we'll we'll make
sure it's working.

134
00:04:54,230 --> 00:04:54,240
sure it's working.
 

135
00:04:54,240 --> 00:04:56,469
sure it's working.
So, um, hopefully everyone by now is

136
00:04:56,469 --> 00:04:56,479
So, um, hopefully everyone by now is
 

137
00:04:56,479 --> 00:04:58,150
So, um, hopefully everyone by now is
working very hard on their projects and

138
00:04:58,150 --> 00:04:58,160
working very hard on their projects and
 

139
00:04:58,160 --> 00:05:00,230
working very hard on their projects and
is seeing, oh no, the amount of stuff I

140
00:05:00,230 --> 00:05:00,240
is seeing, oh no, the amount of stuff I
 

141
00:05:00,240 --> 00:05:01,830
is seeing, oh no, the amount of stuff I
have to do is a lot more than what I

142
00:05:01,830 --> 00:05:01,840
have to do is a lot more than what I
 

143
00:05:01,840 --> 00:05:04,790
have to do is a lot more than what I
thought. And, uh, oh no, how am I going

144
00:05:04,790 --> 00:05:04,800
thought. And, uh, oh no, how am I going
 

145
00:05:04,800 --> 00:05:06,390
thought. And, uh, oh no, how am I going
to get this done? That was the intended

146
00:05:06,390 --> 00:05:06,400
to get this done? That was the intended
 

147
00:05:06,400 --> 00:05:10,070
to get this done? That was the intended
goal. So, it's fine. We expected this.

148
00:05:10,070 --> 00:05:10,080
goal. So, it's fine. We expected this.
 

149
00:05:10,080 --> 00:05:11,909
goal. So, it's fine. We expected this.
Uh, that's why we're asking for a draft

150
00:05:11,909 --> 00:05:11,919
Uh, that's why we're asking for a draft
 

151
00:05:11,919 --> 00:05:14,550
Uh, that's why we're asking for a draft
now and not the full final. So, tell

152
00:05:14,550 --> 00:05:14,560
now and not the full final. So, tell
 

153
00:05:14,560 --> 00:05:16,710
now and not the full final. So, tell
what you've done.

154
00:05:16,710 --> 00:05:16,720
what you've done.
 

155
00:05:16,720 --> 00:05:19,670
what you've done.
We expect to see a good good writing for

156
00:05:19,670 --> 00:05:19,680
We expect to see a good good writing for
 

157
00:05:19,680 --> 00:05:21,350
We expect to see a good good writing for
the parts which you we reasonably can

158
00:05:21,350 --> 00:05:21,360
the parts which you we reasonably can
 

159
00:05:21,360 --> 00:05:23,350
the parts which you we reasonably can
expect you to have done. stuff that

160
00:05:23,350 --> 00:05:23,360
expect you to have done. stuff that
 

161
00:05:23,360 --> 00:05:25,590
expect you to have done. stuff that
doesn't involve a training run and

162
00:05:25,590 --> 00:05:25,600
doesn't involve a training run and
 

163
00:05:25,600 --> 00:05:26,950
doesn't involve a training run and
descriptions of things. We expect you to

164
00:05:26,950 --> 00:05:26,960
descriptions of things. We expect you to
 

165
00:05:26,960 --> 00:05:28,790
descriptions of things. We expect you to
be able to do that. But we understand if

166
00:05:28,790 --> 00:05:28,800
be able to do that. But we understand if
 

167
00:05:28,800 --> 00:05:30,390
be able to do that. But we understand if
some training runs haven't finished,

168
00:05:30,390 --> 00:05:30,400
some training runs haven't finished,
 

169
00:05:30,400 --> 00:05:32,870
some training runs haven't finished,
you're waiting on some results.

170
00:05:32,870 --> 00:05:32,880
you're waiting on some results.
 

171
00:05:32,880 --> 00:05:34,790
you're waiting on some results.
Make a report that has what you can't

172
00:05:34,790 --> 00:05:34,800
Make a report that has what you can't
 

173
00:05:34,800 --> 00:05:37,189
Make a report that has what you can't
has what you have and then, you know,

174
00:05:37,189 --> 00:05:37,199
has what you have and then, you know,
 

175
00:05:37,199 --> 00:05:38,950
has what you have and then, you know,
hopefully you'll be able to get more of

176
00:05:38,950 --> 00:05:38,960
hopefully you'll be able to get more of
 

177
00:05:38,960 --> 00:05:40,870
hopefully you'll be able to get more of
it done ahead of the time of the final

178
00:05:40,870 --> 00:05:40,880
it done ahead of the time of the final
 

179
00:05:40,880 --> 00:05:44,070
it done ahead of the time of the final
um report submission.

180
00:05:44,070 --> 00:05:44,080
um report submission.
 

181
00:05:44,080 --> 00:05:45,909
um report submission.
So those are the announcements we wanted

182
00:05:45,909 --> 00:05:45,919
So those are the announcements we wanted
 

183
00:05:45,919 --> 00:05:48,070
So those are the announcements we wanted
to make. So our goal today is to

184
00:05:48,070 --> 00:05:48,080
to make. So our goal today is to
 

185
00:05:48,080 --> 00:05:50,629
to make. So our goal today is to
continue where we left off. And I would

186
00:05:50,629 --> 00:05:50,639
continue where we left off. And I would
 

187
00:05:50,639 --> 00:05:53,270
continue where we left off. And I would
like to because because I really want to

188
00:05:53,270 --> 00:05:53,280
like to because because I really want to
 

189
00:05:53,280 --> 00:05:55,189
like to because because I really want to
get to being able to teach you about

190
00:05:55,189 --> 00:05:55,199
get to being able to teach you about
 

191
00:05:55,199 --> 00:05:57,110
get to being able to teach you about
post- training of models. I want to try

192
00:05:57,110 --> 00:05:57,120
post- training of models. I want to try
 

193
00:05:57,120 --> 00:06:00,469
post- training of models. I want to try
to accelerate a little bit to get uh

194
00:06:00,469 --> 00:06:00,479
to accelerate a little bit to get uh
 

195
00:06:00,479 --> 00:06:04,150
to accelerate a little bit to get uh
into this uh the generative model stuff

196
00:06:04,150 --> 00:06:04,160
into this uh the generative model stuff
 

197
00:06:04,160 --> 00:06:06,550
into this uh the generative model stuff
today.

198
00:06:06,550 --> 00:06:06,560
today.
 

199
00:06:06,560 --> 00:06:10,230
today.
So um recall we were talking about uh

200
00:06:10,230 --> 00:06:10,240
So um recall we were talking about uh
 

201
00:06:10,240 --> 00:06:12,309
So um recall we were talking about uh
fine-tuning and adapting a model to a

202
00:06:12,309 --> 00:06:12,319
fine-tuning and adapting a model to a
 

203
00:06:12,319 --> 00:06:14,469
fine-tuning and adapting a model to a
new task. So we have these approaches.

204
00:06:14,469 --> 00:06:14,479
new task. So we have these approaches.
 

205
00:06:14,479 --> 00:06:17,029
new task. So we have these approaches.
So this is just recap. If your model is

206
00:06:17,029 --> 00:06:17,039
So this is just recap. If your model is
 

207
00:06:17,039 --> 00:06:18,710
So this is just recap. If your model is
promptable, one thing you can do is just

208
00:06:18,710 --> 00:06:18,720
promptable, one thing you can do is just
 

209
00:06:18,720 --> 00:06:20,309
promptable, one thing you can do is just
prompt it. Okay, this is the new thing

210
00:06:20,309 --> 00:06:20,319
prompt it. Okay, this is the new thing
 

211
00:06:20,319 --> 00:06:22,070
prompt it. Okay, this is the new thing
that was opened up by these large

212
00:06:22,070 --> 00:06:22,080
that was opened up by these large
 

213
00:06:22,080 --> 00:06:23,510
that was opened up by these large
language models and instruction tuning

214
00:06:23,510 --> 00:06:23,520
language models and instruction tuning
 

215
00:06:23,520 --> 00:06:26,550
language models and instruction tuning
and so on and in context learning. And I

216
00:06:26,550 --> 00:06:26,560
and so on and in context learning. And I
 

217
00:06:26,560 --> 00:06:28,710
and so on and in context learning. And I
want to reme remind you all practically

218
00:06:28,710 --> 00:06:28,720
want to reme remind you all practically
 

219
00:06:28,720 --> 00:06:31,510
want to reme remind you all practically
speaking, if you have information that

220
00:06:31,510 --> 00:06:31,520
speaking, if you have information that
 

221
00:06:31,520 --> 00:06:34,309
speaking, if you have information that
lets you use a prompt optimizer, you can

222
00:06:34,309 --> 00:06:34,319
lets you use a prompt optimizer, you can
 

223
00:06:34,319 --> 00:06:35,990
lets you use a prompt optimizer, you can
should use a prompt optimizer for your

224
00:06:35,990 --> 00:06:36,000
should use a prompt optimizer for your
 

225
00:06:36,000 --> 00:06:39,830
should use a prompt optimizer for your
task. No reason not to do that.

226
00:06:39,830 --> 00:06:39,840
task. No reason not to do that.
 

227
00:06:39,840 --> 00:06:43,189
task. No reason not to do that.
If you have a pre-trained model, you can

228
00:06:43,189 --> 00:06:43,199
If you have a pre-trained model, you can
 

229
00:06:43,199 --> 00:06:44,950
If you have a pre-trained model, you can
just use it as an embed or a feature

230
00:06:44,950 --> 00:06:44,960
just use it as an embed or a feature
 

231
00:06:44,960 --> 00:06:46,070
just use it as an embed or a feature
extractor. Remember this is how like

232
00:06:46,070 --> 00:06:46,080
extractor. Remember this is how like
 

233
00:06:46,080 --> 00:06:49,270
extractor. Remember this is how like
BERT was used uh and revolutionized NLP

234
00:06:49,270 --> 00:06:49,280
BERT was used uh and revolutionized NLP
 

235
00:06:49,280 --> 00:06:50,790
BERT was used uh and revolutionized NLP
and this strategy is called linear

236
00:06:50,790 --> 00:06:50,800
and this strategy is called linear
 

237
00:06:50,800 --> 00:06:52,150
and this strategy is called linear
probing if we haven't used this word

238
00:06:52,150 --> 00:06:52,160
probing if we haven't used this word
 

239
00:06:52,160 --> 00:06:54,309
probing if we haven't used this word
before because what you usually do is

240
00:06:54,309 --> 00:06:54,319
before because what you usually do is
 

241
00:06:54,319 --> 00:06:57,590
before because what you usually do is
you train uh you take them extract these

242
00:06:57,590 --> 00:06:57,600
you train uh you take them extract these
 

243
00:06:57,600 --> 00:06:59,990
you train uh you take them extract these
embeddings from the model and you train

244
00:06:59,990 --> 00:07:00,000
embeddings from the model and you train
 

245
00:07:00,000 --> 00:07:02,150
embeddings from the model and you train
a new task specific head for this task

246
00:07:02,150 --> 00:07:02,160
a new task specific head for this task
 

247
00:07:02,160 --> 00:07:04,390
a new task specific head for this task
and it's usually a linear thing you do

248
00:07:04,390 --> 00:07:04,400
and it's usually a linear thing you do
 

249
00:07:04,400 --> 00:07:06,469
and it's usually a linear thing you do
like a linear classifier or regression

250
00:07:06,469 --> 00:07:06,479
like a linear classifier or regression
 

251
00:07:06,479 --> 00:07:09,189
like a linear classifier or regression
things like this.

252
00:07:09,189 --> 00:07:09,199
things like this.
 

253
00:07:09,199 --> 00:07:11,749
things like this.
So

254
00:07:11,749 --> 00:07:11,759
So
 

255
00:07:11,759 --> 00:07:14,870
So
interestingly this works quite well in

256
00:07:14,870 --> 00:07:14,880
interestingly this works quite well in
 

257
00:07:14,880 --> 00:07:17,189
interestingly this works quite well in
many models uh even ones that are

258
00:07:17,189 --> 00:07:17,199
many models uh even ones that are
 

259
00:07:17,199 --> 00:07:19,670
many models uh even ones that are
promptable. Uh people have found that

260
00:07:19,670 --> 00:07:19,680
promptable. Uh people have found that
 

261
00:07:19,680 --> 00:07:23,270
promptable. Uh people have found that
doing a linear probe on the model can

262
00:07:23,270 --> 00:07:23,280
doing a linear probe on the model can
 

263
00:07:23,280 --> 00:07:26,230
doing a linear probe on the model can
perform better than a prompt uh for a

264
00:07:26,230 --> 00:07:26,240
perform better than a prompt uh for a
 

265
00:07:26,240 --> 00:07:28,870
perform better than a prompt uh for a
task and this is very interesting. Uh

266
00:07:28,870 --> 00:07:28,880
task and this is very interesting. Uh
 

267
00:07:28,880 --> 00:07:30,710
task and this is very interesting. Uh
larger models the disc discrepancy is

268
00:07:30,710 --> 00:07:30,720
larger models the disc discrepancy is
 

269
00:07:30,720 --> 00:07:33,430
larger models the disc discrepancy is
less but it it still exists. And there's

270
00:07:33,430 --> 00:07:33,440
less but it it still exists. And there's
 

271
00:07:33,440 --> 00:07:36,150
less but it it still exists. And there's
even more very interesting things that

272
00:07:36,150 --> 00:07:36,160
even more very interesting things that
 

273
00:07:36,160 --> 00:07:38,309
even more very interesting things that
people have found in this which is that

274
00:07:38,309 --> 00:07:38,319
people have found in this which is that
 

275
00:07:38,319 --> 00:07:41,350
people have found in this which is that
sometimes you can a model is doing quite

276
00:07:41,350 --> 00:07:41,360
sometimes you can a model is doing quite
 

277
00:07:41,360 --> 00:07:42,950
sometimes you can a model is doing quite
badly but a linear probe is doing very

278
00:07:42,950 --> 00:07:42,960
badly but a linear probe is doing very
 

279
00:07:42,960 --> 00:07:46,469
badly but a linear probe is doing very
well. Uh so the model in principle has

280
00:07:46,469 --> 00:07:46,479
well. Uh so the model in principle has
 

281
00:07:46,479 --> 00:07:48,070
well. Uh so the model in principle has
the information and the embeddings to be

282
00:07:48,070 --> 00:07:48,080
the information and the embeddings to be
 

283
00:07:48,080 --> 00:07:49,909
the information and the embeddings to be
able to disentangle things but it can't

284
00:07:49,909 --> 00:07:49,919
able to disentangle things but it can't
 

285
00:07:49,919 --> 00:07:53,909
able to disentangle things but it can't
do it uh in a prompted way.

286
00:07:53,909 --> 00:07:53,919
do it uh in a prompted way.
 

287
00:07:53,919 --> 00:07:55,990
do it uh in a prompted way.
Um advantage

288
00:07:55,990 --> 00:07:56,000
Um advantage
 

289
00:07:56,000 --> 00:07:58,070
Um advantage
it's pretty easy to do. You don't need

290
00:07:58,070 --> 00:07:58,080
it's pretty easy to do. You don't need
 

291
00:07:58,080 --> 00:08:00,230
it's pretty easy to do. You don't need
any data beyond the task you want to get

292
00:08:00,230 --> 00:08:00,240
any data beyond the task you want to get
 

293
00:08:00,240 --> 00:08:03,029
any data beyond the task you want to get
good at. Um and the disadvantage is this

294
00:08:03,029 --> 00:08:03,039
good at. Um and the disadvantage is this
 

295
00:08:03,039 --> 00:08:04,790
good at. Um and the disadvantage is this
sometimes doesn't work as well as a full

296
00:08:04,790 --> 00:08:04,800
sometimes doesn't work as well as a full
 

297
00:08:04,800 --> 00:08:07,029
sometimes doesn't work as well as a full
fine tune. So full fine tune I just

298
00:08:07,029 --> 00:08:07,039
fine tune. So full fine tune I just
 

299
00:08:07,039 --> 00:08:08,950
fine tune. So full fine tune I just
marked in green what you're tuning is

300
00:08:08,950 --> 00:08:08,960
marked in green what you're tuning is
 

301
00:08:08,960 --> 00:08:10,710
marked in green what you're tuning is
you take the model you have a task

302
00:08:10,710 --> 00:08:10,720
you take the model you have a task
 

303
00:08:10,720 --> 00:08:12,710
you take the model you have a task
specific uh head and you train

304
00:08:12,710 --> 00:08:12,720
specific uh head and you train
 

305
00:08:12,720 --> 00:08:15,589
specific uh head and you train
everything uh together on your data for

306
00:08:15,589 --> 00:08:15,599
everything uh together on your data for
 

307
00:08:15,599 --> 00:08:17,670
everything uh together on your data for
your new task.

308
00:08:17,670 --> 00:08:17,680
your new task.
 

309
00:08:17,680 --> 00:08:20,230
your new task.
Um so

310
00:08:20,230 --> 00:08:20,240
Um so
 

311
00:08:20,240 --> 00:08:22,790
Um so
again this tends to work better um on

312
00:08:22,790 --> 00:08:22,800
again this tends to work better um on
 

313
00:08:22,800 --> 00:08:25,029
again this tends to work better um on
new tasks. it can be a much much bigger

314
00:08:25,029 --> 00:08:25,039
new tasks. it can be a much much bigger
 

315
00:08:25,039 --> 00:08:26,790
new tasks. it can be a much much bigger
training job because your pre-trained

316
00:08:26,790 --> 00:08:26,800
training job because your pre-trained
 

317
00:08:26,800 --> 00:08:28,869
training job because your pre-trained
model might have lots and lots of

318
00:08:28,869 --> 00:08:28,879
model might have lots and lots of
 

319
00:08:28,879 --> 00:08:30,550
model might have lots and lots of
parameters in it and so this is a much

320
00:08:30,550 --> 00:08:30,560
parameters in it and so this is a much
 

321
00:08:30,560 --> 00:08:34,310
parameters in it and so this is a much
bigger thing to uh to fine-tune and as a

322
00:08:34,310 --> 00:08:34,320
bigger thing to uh to fine-tune and as a
 

323
00:08:34,320 --> 00:08:35,190
bigger thing to uh to fine-tune and as a
result it's harder to do the

324
00:08:35,190 --> 00:08:35,200
result it's harder to do the
 

325
00:08:35,200 --> 00:08:36,469
result it's harder to do the
hyperparameter search and everything

326
00:08:36,469 --> 00:08:36,479
hyperparameter search and everything
 

327
00:08:36,479 --> 00:08:38,469
hyperparameter search and everything
that you need to do to do a good job and

328
00:08:38,469 --> 00:08:38,479
that you need to do to do a good job and
 

329
00:08:38,479 --> 00:08:40,070
that you need to do to do a good job and
there's also a risk of overfitting.

330
00:08:40,070 --> 00:08:40,080
there's also a risk of overfitting.
 

331
00:08:40,080 --> 00:08:41,750
there's also a risk of overfitting.
We're going to talk more about this when

332
00:08:41,750 --> 00:08:41,760
We're going to talk more about this when
 

333
00:08:41,760 --> 00:08:43,589
We're going to talk more about this when
we talk about forgetting kind of a more

334
00:08:43,589 --> 00:08:43,599
we talk about forgetting kind of a more
 

335
00:08:43,599 --> 00:08:45,670
we talk about forgetting kind of a more
specific kind of overfitting that can

336
00:08:45,670 --> 00:08:45,680
specific kind of overfitting that can
 

337
00:08:45,680 --> 00:08:47,430
specific kind of overfitting that can
happen

338
00:08:47,430 --> 00:08:47,440
happen
 

339
00:08:47,440 --> 00:08:48,790
happen
but this is typically what people

340
00:08:48,790 --> 00:08:48,800
but this is typically what people
 

341
00:08:48,800 --> 00:08:49,990
but this is typically what people
compare to. We talked last time about

342
00:08:49,990 --> 00:08:50,000
compare to. We talked last time about
 

343
00:08:50,000 --> 00:08:51,110
compare to. We talked last time about
parameter efficient fine-tuning

344
00:08:51,110 --> 00:08:51,120
parameter efficient fine-tuning
 

345
00:08:51,120 --> 00:08:53,110
parameter efficient fine-tuning
approaches that can kind of are in

346
00:08:53,110 --> 00:08:53,120
approaches that can kind of are in
 

347
00:08:53,120 --> 00:08:55,350
approaches that can kind of are in
between uh these two by letting you use

348
00:08:55,350 --> 00:08:55,360
between uh these two by letting you use
 

349
00:08:55,360 --> 00:08:57,110
between uh these two by letting you use
Lauras and soft prompts and things like

350
00:08:57,110 --> 00:08:57,120
Lauras and soft prompts and things like
 

351
00:08:57,120 --> 00:09:00,550
Lauras and soft prompts and things like
this. If you want to you can think about

352
00:09:00,550 --> 00:09:00,560
this. If you want to you can think about
 

353
00:09:00,560 --> 00:09:04,949
this. If you want to you can think about
a soft prompt as being the flip version

354
00:09:04,949 --> 00:09:04,959
a soft prompt as being the flip version
 

355
00:09:04,959 --> 00:09:07,670
a soft prompt as being the flip version
of one.

356
00:09:07,670 --> 00:09:07,680
of one.
 

357
00:09:07,680 --> 00:09:10,550
of one.
Okay. So in one you treat the model as

358
00:09:10,550 --> 00:09:10,560
Okay. So in one you treat the model as
 

359
00:09:10,560 --> 00:09:13,829
Okay. So in one you treat the model as
an embed and you train a new thing at

360
00:09:13,829 --> 00:09:13,839
an embed and you train a new thing at
 

361
00:09:13,839 --> 00:09:16,470
an embed and you train a new thing at
the top. A soft prompt you can think of

362
00:09:16,470 --> 00:09:16,480
the top. A soft prompt you can think of
 

363
00:09:16,480 --> 00:09:19,590
the top. A soft prompt you can think of
as being flipped. you treat the top of

364
00:09:19,590 --> 00:09:19,600
as being flipped. you treat the top of
 

365
00:09:19,600 --> 00:09:22,230
as being flipped. you treat the top of
the model the same. You're going to use

366
00:09:22,230 --> 00:09:22,240
the model the same. You're going to use
 

367
00:09:22,240 --> 00:09:23,670
the model the same. You're going to use
the model's generative capacity to

368
00:09:23,670 --> 00:09:23,680
the model's generative capacity to
 

369
00:09:23,680 --> 00:09:25,110
the model's generative capacity to
generate answers, but you'll put

370
00:09:25,110 --> 00:09:25,120
generate answers, but you'll put
 

371
00:09:25,120 --> 00:09:27,670
generate answers, but you'll put
something at the bottom,

372
00:09:27,670 --> 00:09:27,680
something at the bottom,
 

373
00:09:27,680 --> 00:09:29,190
something at the bottom,
right, with the instructions, with the

374
00:09:29,190 --> 00:09:29,200
right, with the instructions, with the
 

375
00:09:29,200 --> 00:09:31,350
right, with the instructions, with the
soft instructions. Anyway, so this is

376
00:09:31,350 --> 00:09:31,360
soft instructions. Anyway, so this is
 

377
00:09:31,360 --> 00:09:35,269
soft instructions. Anyway, so this is
like recap. I wanted to hit some uh

378
00:09:35,269 --> 00:09:35,279
like recap. I wanted to hit some uh
 

379
00:09:35,279 --> 00:09:37,750
like recap. I wanted to hit some uh
practical tips here.

380
00:09:37,750 --> 00:09:37,760
practical tips here.
 

381
00:09:37,760 --> 00:09:38,870
practical tips here.
So, if you're going to do a full

382
00:09:38,870 --> 00:09:38,880
So, if you're going to do a full
 

383
00:09:38,880 --> 00:09:41,269
So, if you're going to do a full
fine-tune and your full fine-tune

384
00:09:41,269 --> 00:09:41,279
fine-tune and your full fine-tune
 

385
00:09:41,279 --> 00:09:44,150
fine-tune and your full fine-tune
requires you to have a new task specific

386
00:09:44,150 --> 00:09:44,160
requires you to have a new task specific
 

387
00:09:44,160 --> 00:09:46,630
requires you to have a new task specific
head, right, a classifier. there's new

388
00:09:46,630 --> 00:09:46,640
head, right, a classifier. there's new
 

389
00:09:46,640 --> 00:09:49,030
head, right, a classifier. there's new
categories for your classifier or you

390
00:09:49,030 --> 00:09:49,040
categories for your classifier or you
 

391
00:09:49,040 --> 00:09:51,269
categories for your classifier or you
know a regression that you want to do

392
00:09:51,269 --> 00:09:51,279
know a regression that you want to do
 

393
00:09:51,279 --> 00:09:53,910
know a regression that you want to do
that there's nothing you can start with.

394
00:09:53,910 --> 00:09:53,920
that there's nothing you can start with.
 

395
00:09:53,920 --> 00:09:56,470
that there's nothing you can start with.
Don't initialize for a full fine-tune

396
00:09:56,470 --> 00:09:56,480
Don't initialize for a full fine-tune
 

397
00:09:56,480 --> 00:09:58,310
Don't initialize for a full fine-tune
your head to some random parameters and

398
00:09:58,310 --> 00:09:58,320
your head to some random parameters and
 

399
00:09:58,320 --> 00:10:01,590
your head to some random parameters and
then do fine-tuning. This is not a good

400
00:10:01,590 --> 00:10:01,600
then do fine-tuning. This is not a good
 

401
00:10:01,600 --> 00:10:03,910
then do fine-tuning. This is not a good
way to do it. In general, you can do

402
00:10:03,910 --> 00:10:03,920
way to do it. In general, you can do
 

403
00:10:03,920 --> 00:10:06,790
way to do it. In general, you can do
better than this. So why is this not

404
00:10:06,790 --> 00:10:06,800
better than this. So why is this not
 

405
00:10:06,800 --> 00:10:08,949
better than this. So why is this not
good? The reason is that at the

406
00:10:08,949 --> 00:10:08,959
good? The reason is that at the
 

407
00:10:08,959 --> 00:10:12,230
good? The reason is that at the
beginning your head is initialized to

408
00:10:12,230 --> 00:10:12,240
beginning your head is initialized to
 

409
00:10:12,240 --> 00:10:14,069
beginning your head is initialized to
some random thing. It's not good at

410
00:10:14,069 --> 00:10:14,079
some random thing. It's not good at
 

411
00:10:14,079 --> 00:10:15,910
some random thing. It's not good at
doing its job at all. it doesn't know

412
00:10:15,910 --> 00:10:15,920
doing its job at all. it doesn't know
 

413
00:10:15,920 --> 00:10:19,350
doing its job at all. it doesn't know
what to look for. Uh when you do a full

414
00:10:19,350 --> 00:10:19,360
what to look for. Uh when you do a full
 

415
00:10:19,360 --> 00:10:21,509
what to look for. Uh when you do a full
fine-tune on that first few training

416
00:10:21,509 --> 00:10:21,519
fine-tune on that first few training
 

417
00:10:21,519 --> 00:10:23,190
fine-tune on that first few training
examples you give it, it's going to be

418
00:10:23,190 --> 00:10:23,200
examples you give it, it's going to be
 

419
00:10:23,200 --> 00:10:25,670
examples you give it, it's going to be
sending gradients through this head back

420
00:10:25,670 --> 00:10:25,680
sending gradients through this head back
 

421
00:10:25,680 --> 00:10:27,829
sending gradients through this head back
into the model. But these gradients

422
00:10:27,829 --> 00:10:27,839
into the model. But these gradients
 

423
00:10:27,839 --> 00:10:31,350
into the model. But these gradients
aren't very good because the head isn't

424
00:10:31,350 --> 00:10:31,360
aren't very good because the head isn't
 

425
00:10:31,360 --> 00:10:33,509
aren't very good because the head isn't
very good at doing what it's doing. And

426
00:10:33,509 --> 00:10:33,519
very good at doing what it's doing. And
 

427
00:10:33,519 --> 00:10:35,190
very good at doing what it's doing. And
so it's going to sort of throw random

428
00:10:35,190 --> 00:10:35,200
so it's going to sort of throw random
 

429
00:10:35,200 --> 00:10:38,150
so it's going to sort of throw random
noise into the pre-trained model and

430
00:10:38,150 --> 00:10:38,160
noise into the pre-trained model and
 

431
00:10:38,160 --> 00:10:39,509
noise into the pre-trained model and
move it around, shuffle it around. This

432
00:10:39,509 --> 00:10:39,519
move it around, shuffle it around. This
 

433
00:10:39,519 --> 00:10:41,829
move it around, shuffle it around. This
is generally not a good idea. Okay, this

434
00:10:41,829 --> 00:10:41,839
is generally not a good idea. Okay, this
 

435
00:10:41,839 --> 00:10:44,790
is generally not a good idea. Okay, this
is why this is not the practically best

436
00:10:44,790 --> 00:10:44,800
is why this is not the practically best
 

437
00:10:44,800 --> 00:10:47,430
is why this is not the practically best
way of doing this.

438
00:10:47,430 --> 00:10:47,440
way of doing this.
 

439
00:10:47,440 --> 00:10:51,110
way of doing this.
So a better choice is to initialize the

440
00:10:51,110 --> 00:10:51,120
So a better choice is to initialize the
 

441
00:10:51,120 --> 00:10:52,870
So a better choice is to initialize the
head to a linear thing after all to

442
00:10:52,870 --> 00:10:52,880
head to a linear thing after all to
 

443
00:10:52,880 --> 00:10:56,470
head to a linear thing after all to
zero. That way at the beginning it's not

444
00:10:56,470 --> 00:10:56,480
zero. That way at the beginning it's not
 

445
00:10:56,480 --> 00:11:00,710
zero. That way at the beginning it's not
throwing gradients back, right? Because

446
00:11:00,710 --> 00:11:00,720
throwing gradients back, right? Because
 

447
00:11:00,720 --> 00:11:02,630
throwing gradients back, right? Because
if this is zero, the gradients don't go

448
00:11:02,630 --> 00:11:02,640
if this is zero, the gradients don't go
 

449
00:11:02,640 --> 00:11:03,910
if this is zero, the gradients don't go
through. So at least the first few

450
00:11:03,910 --> 00:11:03,920
through. So at least the first few
 

451
00:11:03,920 --> 00:11:06,230
through. So at least the first few
examples have very little effect on the

452
00:11:06,230 --> 00:11:06,240
examples have very little effect on the
 

453
00:11:06,240 --> 00:11:07,750
examples have very little effect on the
model.

454
00:11:07,750 --> 00:11:07,760
model.
 

455
00:11:07,760 --> 00:11:09,910
model.
Everyone understand why this is a better

456
00:11:09,910 --> 00:11:09,920
Everyone understand why this is a better
 

457
00:11:09,920 --> 00:11:11,670
Everyone understand why this is a better
idea for how to initialize a head for a

458
00:11:11,670 --> 00:11:11,680
idea for how to initialize a head for a
 

459
00:11:11,680 --> 00:11:13,670
idea for how to initialize a head for a
full fine tune?

460
00:11:13,670 --> 00:11:13,680
full fine tune?
 

461
00:11:13,680 --> 00:11:15,990
full fine tune?
Okay, this of course is the context of

462
00:11:15,990 --> 00:11:16,000
Okay, this of course is the context of
 

463
00:11:16,000 --> 00:11:17,670
Okay, this of course is the context of
the head is for a new task. Like there's

464
00:11:17,670 --> 00:11:17,680
the head is for a new task. Like there's
 

465
00:11:17,680 --> 00:11:19,269
the head is for a new task. Like there's
nothing you can use for the head to

466
00:11:19,269 --> 00:11:19,279
nothing you can use for the head to
 

467
00:11:19,279 --> 00:11:23,750
nothing you can use for the head to
start it off. So even better than

468
00:11:23,750 --> 00:11:23,760
start it off. So even better than
 

469
00:11:23,760 --> 00:11:27,509
start it off. So even better than
initializing to zero is to do a

470
00:11:27,509 --> 00:11:27,519
initializing to zero is to do a
 

471
00:11:27,519 --> 00:11:30,230
initializing to zero is to do a
combination of this approach and this

472
00:11:30,230 --> 00:11:30,240
combination of this approach and this
 

473
00:11:30,240 --> 00:11:32,710
combination of this approach and this
approach. So what does that mean? Take

474
00:11:32,710 --> 00:11:32,720
approach. So what does that mean? Take
 

475
00:11:32,720 --> 00:11:35,509
approach. So what does that mean? Take
some of your data and just train ahead

476
00:11:35,509 --> 00:11:35,519
some of your data and just train ahead
 

477
00:11:35,519 --> 00:11:38,069
some of your data and just train ahead
first.

478
00:11:38,069 --> 00:11:38,079
first.
 

479
00:11:38,079 --> 00:11:40,550
first.
Okay, freeze the model. Now the head is

480
00:11:40,550 --> 00:11:40,560
Okay, freeze the model. Now the head is
 

481
00:11:40,560 --> 00:11:43,110
Okay, freeze the model. Now the head is
somewhat good at doing this task and now

482
00:11:43,110 --> 00:11:43,120
somewhat good at doing this task and now
 

483
00:11:43,120 --> 00:11:45,509
somewhat good at doing this task and now
do a full fine tune because now the

484
00:11:45,509 --> 00:11:45,519
do a full fine tune because now the
 

485
00:11:45,519 --> 00:11:47,269
do a full fine tune because now the
gradients are going to be better and

486
00:11:47,269 --> 00:11:47,279
gradients are going to be better and
 

487
00:11:47,279 --> 00:11:50,550
gradients are going to be better and
you'll get less damage to the u the

488
00:11:50,550 --> 00:11:50,560
you'll get less damage to the u the
 

489
00:11:50,560 --> 00:11:52,949
you'll get less damage to the u the
pre-trained model if you do it this way.

490
00:11:52,949 --> 00:11:52,959
pre-trained model if you do it this way.
 

491
00:11:52,959 --> 00:11:54,470
pre-trained model if you do it this way.
This is like a practical tip. There's

492
00:11:54,470 --> 00:11:54,480
This is like a practical tip. There's
 

493
00:11:54,480 --> 00:11:55,829
This is like a practical tip. There's
been theoretical work on this too, but

494
00:11:55,829 --> 00:11:55,839
been theoretical work on this too, but
 

495
00:11:55,839 --> 00:11:59,030
been theoretical work on this too, but
this is sort it it is practical. Any

496
00:11:59,030 --> 00:11:59,040
this is sort it it is practical. Any
 

497
00:11:59,040 --> 00:12:02,630
this is sort it it is practical. Any
questions on this basic stuff?

498
00:12:02,630 --> 00:12:02,640
questions on this basic stuff?
 

499
00:12:02,640 --> 00:12:04,310
questions on this basic stuff?
Okay, great.

500
00:12:04,310 --> 00:12:04,320
Okay, great.
 

501
00:12:04,320 --> 00:12:07,829
Okay, great.
So again you can do all this stuff and

502
00:12:07,829 --> 00:12:07,839
So again you can do all this stuff and
 

503
00:12:07,839 --> 00:12:09,910
So again you can do all this stuff and
take the spirit and combine it with uh

504
00:12:09,910 --> 00:12:09,920
take the spirit and combine it with uh
 

505
00:12:09,920 --> 00:12:11,590
take the spirit and combine it with uh
you know Laura as soft prompting that

506
00:12:11,590 --> 00:12:11,600
you know Laura as soft prompting that
 

507
00:12:11,600 --> 00:12:21,990
you know Laura as soft prompting that
should be clear to everybody. Yeah.

508
00:12:21,990 --> 00:12:22,000

 

509
00:12:22,000 --> 00:12:24,389

>> So the reason why initializing to zero

510
00:12:24,389 --> 00:12:24,399
>> So the reason why initializing to zero
 

511
00:12:24,399 --> 00:12:26,389
>> So the reason why initializing to zero
is a better idea is this is the head.

512
00:12:26,389 --> 00:12:26,399
is a better idea is this is the head.
 

513
00:12:26,399 --> 00:12:27,670
is a better idea is this is the head.
This is the final thing. So it's like a

514
00:12:27,670 --> 00:12:27,680
This is the final thing. So it's like a
 

515
00:12:27,680 --> 00:12:29,829
This is the final thing. So it's like a
linear classifier. So when you're doing

516
00:12:29,829 --> 00:12:29,839
linear classifier. So when you're doing
 

517
00:12:29,839 --> 00:12:32,550
linear classifier. So when you're doing
a linear classifier

518
00:12:32,550 --> 00:12:32,560
a linear classifier
 

519
00:12:32,560 --> 00:12:35,829
a linear classifier
typically you initialize it to zero like

520
00:12:35,829 --> 00:12:35,839
typically you initialize it to zero like
 

521
00:12:35,839 --> 00:12:37,110
typically you initialize it to zero like
you're doing if you're doing training an

522
00:12:37,110 --> 00:12:37,120
you're doing if you're doing training an
 

523
00:12:37,120 --> 00:12:39,350
you're doing if you're doing training an
SVM or anything you don't actually

524
00:12:39,350 --> 00:12:39,360
SVM or anything you don't actually
 

525
00:12:39,360 --> 00:12:41,990
SVM or anything you don't actually
initialize it to something random. So

526
00:12:41,990 --> 00:12:42,000
initialize it to something random. So
 

527
00:12:42,000 --> 00:12:44,310
initialize it to something random. So
for just from that point of view you

528
00:12:44,310 --> 00:12:44,320
for just from that point of view you
 

529
00:12:44,320 --> 00:12:46,710
for just from that point of view you
should think if you're just going to be

530
00:12:46,710 --> 00:12:46,720
should think if you're just going to be
 

531
00:12:46,720 --> 00:12:48,949
should think if you're just going to be
doing this you would have initialized

532
00:12:48,949 --> 00:12:48,959
doing this you would have initialized
 

533
00:12:48,959 --> 00:12:51,829
doing this you would have initialized
this probably to zero.

534
00:12:51,829 --> 00:12:51,839
this probably to zero.
 

535
00:12:51,839 --> 00:12:53,829
this probably to zero.
Okay does that is that that clear? The

536
00:12:53,829 --> 00:12:53,839
Okay does that is that that clear? The
 

537
00:12:53,839 --> 00:12:56,069
Okay does that is that that clear? The
reason people will sometimes just

538
00:12:56,069 --> 00:12:56,079
reason people will sometimes just
 

539
00:12:56,079 --> 00:12:57,910
reason people will sometimes just
initialize it to random is just used to

540
00:12:57,910 --> 00:12:57,920
initialize it to random is just used to
 

541
00:12:57,920 --> 00:12:59,829
initialize it to random is just used to
this deep learning kind of spirit.

542
00:12:59,829 --> 00:12:59,839
this deep learning kind of spirit.
 

543
00:12:59,839 --> 00:13:01,269
this deep learning kind of spirit.
They're like how do we initialize this

544
00:13:01,269 --> 00:13:01,279
They're like how do we initialize this
 

545
00:13:01,279 --> 00:13:03,269
They're like how do we initialize this
random but you have to remember right is

546
00:13:03,269 --> 00:13:03,279
random but you have to remember right is
 

547
00:13:03,279 --> 00:13:05,030
random but you have to remember right is
that really we justify random

548
00:13:05,030 --> 00:13:05,040
that really we justify random
 

549
00:13:05,040 --> 00:13:07,030
that really we justify random
initialization to you. The justification

550
00:13:07,030 --> 00:13:07,040
initialization to you. The justification
 

551
00:13:07,040 --> 00:13:08,629
initialization to you. The justification
we use said zero doesn't work but in

552
00:13:08,629 --> 00:13:08,639
we use said zero doesn't work but in
 

553
00:13:08,639 --> 00:13:11,269
we use said zero doesn't work but in
this case zero does work so you can do

554
00:13:11,269 --> 00:13:11,279
this case zero does work so you can do
 

555
00:13:11,279 --> 00:13:17,269
this case zero does work so you can do
it. Does that make it more clear? Okay.

556
00:13:17,269 --> 00:13:17,279
it. Does that make it more clear? Okay.
 

557
00:13:17,279 --> 00:13:20,790
it. Does that make it more clear? Okay.
Um yeah so this is like the universe of

558
00:13:20,790 --> 00:13:20,800
Um yeah so this is like the universe of
 

559
00:13:20,800 --> 00:13:23,269
Um yeah so this is like the universe of
things that one you know can tend to do

560
00:13:23,269 --> 00:13:23,279
things that one you know can tend to do
 

561
00:13:23,279 --> 00:13:25,110
things that one you know can tend to do
when one is fine-tuning a model for a

562
00:13:25,110 --> 00:13:25,120
when one is fine-tuning a model for a
 

563
00:13:25,120 --> 00:13:27,670
when one is fine-tuning a model for a
new task. So now last time we started

564
00:13:27,670 --> 00:13:27,680
new task. So now last time we started
 

565
00:13:27,680 --> 00:13:29,670
new task. So now last time we started
talking about metalarning which was to

566
00:13:29,670 --> 00:13:29,680
talking about metalarning which was to
 

567
00:13:29,680 --> 00:13:32,389
talking about metalarning which was to
say what if your goal was to make a

568
00:13:32,389 --> 00:13:32,399
say what if your goal was to make a
 

569
00:13:32,399 --> 00:13:34,389
say what if your goal was to make a
model better at being fine-tuned for

570
00:13:34,389 --> 00:13:34,399
model better at being fine-tuned for
 

571
00:13:34,399 --> 00:13:37,030
model better at being fine-tuned for
tasks.

572
00:13:37,030 --> 00:13:37,040
tasks.
 

573
00:13:37,040 --> 00:13:39,430
tasks.
So this is like categories two and three

574
00:13:39,430 --> 00:13:39,440
So this is like categories two and three
 

575
00:13:39,440 --> 00:13:42,230
So this is like categories two and three
up here. Okay

576
00:13:42,230 --> 00:13:42,240
up here. Okay
 

577
00:13:42,240 --> 00:13:44,230
up here. Okay
for for we're thinking of that about

578
00:13:44,230 --> 00:13:44,240
for for we're thinking of that about
 

579
00:13:44,240 --> 00:13:45,590
for for we're thinking of that about
that right now. We're going to switch to

580
00:13:45,590 --> 00:13:45,600
that right now. We're going to switch to
 

581
00:13:45,600 --> 00:13:47,829
that right now. We're going to switch to
thinking about category one in a little

582
00:13:47,829 --> 00:13:47,839
thinking about category one in a little
 

583
00:13:47,839 --> 00:13:50,230
thinking about category one in a little
bit, but I just in in for a short time

584
00:13:50,230 --> 00:13:50,240
bit, but I just in in for a short time
 

585
00:13:50,240 --> 00:13:51,829
bit, but I just in in for a short time
later, but I just want to think about

586
00:13:51,829 --> 00:13:51,839
later, but I just want to think about
 

587
00:13:51,839 --> 00:13:55,110
later, but I just want to think about
two and three right now. So last time we

588
00:13:55,110 --> 00:13:55,120
two and three right now. So last time we
 

589
00:13:55,120 --> 00:13:57,430
two and three right now. So last time we
said that well

590
00:13:57,430 --> 00:13:57,440
said that well
 

591
00:13:57,440 --> 00:13:59,189
said that well
one thing you can do for any you know

592
00:13:59,189 --> 00:13:59,199
one thing you can do for any you know
 

593
00:13:59,199 --> 00:14:01,110
one thing you can do for any you know
being fine tunable for a task you can do

594
00:14:01,110 --> 00:14:01,120
being fine tunable for a task you can do
 

595
00:14:01,120 --> 00:14:03,670
being fine tunable for a task you can do
nothing just have random initialization.

596
00:14:03,670 --> 00:14:03,680
nothing just have random initialization.
 

597
00:14:03,680 --> 00:14:05,590
nothing just have random initialization.
We're assuming that that uh you want to

598
00:14:05,590 --> 00:14:05,600
We're assuming that that uh you want to
 

599
00:14:05,600 --> 00:14:07,750
We're assuming that that uh you want to
beat that and it's possible to beat it.

600
00:14:07,750 --> 00:14:07,760
beat that and it's possible to beat it.
 

601
00:14:07,760 --> 00:14:09,030
beat that and it's possible to beat it.
You could have a general foundation

602
00:14:09,030 --> 00:14:09,040
You could have a general foundation
 

603
00:14:09,040 --> 00:14:10,710
You could have a general foundation
model that's just good at lots and lots

604
00:14:10,710 --> 00:14:10,720
model that's just good at lots and lots
 

605
00:14:10,720 --> 00:14:12,629
model that's just good at lots and lots
of tasks, not anything for the specific

606
00:14:12,629 --> 00:14:12,639
of tasks, not anything for the specific
 

607
00:14:12,639 --> 00:14:14,949
of tasks, not anything for the specific
task family that you're interested in.

608
00:14:14,949 --> 00:14:14,959
task family that you're interested in.
 

609
00:14:14,959 --> 00:14:16,310
task family that you're interested in.
And then the next thing that we're

610
00:14:16,310 --> 00:14:16,320
And then the next thing that we're
 

611
00:14:16,320 --> 00:14:18,150
And then the next thing that we're
talking about is this idea called MAML,

612
00:14:18,150 --> 00:14:18,160
talking about is this idea called MAML,
 

613
00:14:18,160 --> 00:14:19,750
talking about is this idea called MAML,
which is stands for model agnostic

614
00:14:19,750 --> 00:14:19,760
which is stands for model agnostic
 

615
00:14:19,760 --> 00:14:21,670
which is stands for model agnostic
metalarning, which is for the case where

616
00:14:21,670 --> 00:14:21,680
metalarning, which is for the case where
 

617
00:14:21,680 --> 00:14:25,030
metalarning, which is for the case where
you have a collection of tasks that you

618
00:14:25,030 --> 00:14:25,040
you have a collection of tasks that you
 

619
00:14:25,040 --> 00:14:27,750
you have a collection of tasks that you
think are related to each other and you

620
00:14:27,750 --> 00:14:27,760
think are related to each other and you
 

621
00:14:27,760 --> 00:14:29,829
think are related to each other and you
want to be good at fine-tuning within

622
00:14:29,829 --> 00:14:29,839
want to be good at fine-tuning within
 

623
00:14:29,839 --> 00:14:31,829
want to be good at fine-tuning within
this collection of task. Some of which

624
00:14:31,829 --> 00:14:31,839
this collection of task. Some of which
 

625
00:14:31,839 --> 00:14:33,030
this collection of task. Some of which
you don't actually have example

626
00:14:33,030 --> 00:14:33,040
you don't actually have example
 

627
00:14:33,040 --> 00:14:34,790
you don't actually have example
exemplars of. You have some examples and

628
00:14:34,790 --> 00:14:34,800
exemplars of. You have some examples and
 

629
00:14:34,800 --> 00:14:38,150
exemplars of. You have some examples and
you have some you don't have.

630
00:14:38,150 --> 00:14:38,160
you have some you don't have.
 

631
00:14:38,160 --> 00:14:41,590
you have some you don't have.
So to do this approach you need to

632
00:14:41,590 --> 00:14:41,600
So to do this approach you need to
 

633
00:14:41,600 --> 00:14:43,189
So to do this approach you need to
actually have training data. We talked

634
00:14:43,189 --> 00:14:43,199
actually have training data. We talked
 

635
00:14:43,199 --> 00:14:44,069
actually have training data. We talked
about this last time. So you have to

636
00:14:44,069 --> 00:14:44,079
about this last time. So you have to
 

637
00:14:44,079 --> 00:14:45,670
about this last time. So you have to
have some collection of tasks from this

638
00:14:45,670 --> 00:14:45,680
have some collection of tasks from this
 

639
00:14:45,680 --> 00:14:47,670
have some collection of tasks from this
family. And that means you need to have

640
00:14:47,670 --> 00:14:47,680
family. And that means you need to have
 

641
00:14:47,680 --> 00:14:49,189
family. And that means you need to have
you know training data and loss

642
00:14:49,189 --> 00:14:49,199
you know training data and loss
 

643
00:14:49,199 --> 00:14:51,829
you know training data and loss
functions that you could use.

644
00:14:51,829 --> 00:14:51,839
functions that you could use.
 

645
00:14:51,839 --> 00:14:53,590
functions that you could use.
You have to have some approach to doing

646
00:14:53,590 --> 00:14:53,600
You have to have some approach to doing
 

647
00:14:53,600 --> 00:14:56,069
You have to have some approach to doing
fine-tuning. For example three with a

648
00:14:56,069 --> 00:14:56,079
fine-tuning. For example three with a
 

649
00:14:56,079 --> 00:14:58,150
fine-tuning. For example three with a
Laura or just a full fine tune. We're

650
00:14:58,150 --> 00:14:58,160
Laura or just a full fine tune. We're
 

651
00:14:58,160 --> 00:14:59,430
Laura or just a full fine tune. We're
going to talk mostly from the

652
00:14:59,430 --> 00:14:59,440
going to talk mostly from the
 

653
00:14:59,440 --> 00:15:01,110
going to talk mostly from the
perspective of full fine tunes but you

654
00:15:01,110 --> 00:15:01,120
perspective of full fine tunes but you
 

655
00:15:01,120 --> 00:15:03,030
perspective of full fine tunes but you
should also think about Lauras as a a

656
00:15:03,030 --> 00:15:03,040
should also think about Lauras as a a
 

657
00:15:03,040 --> 00:15:05,910
should also think about Lauras as a a
choice.

658
00:15:05,910 --> 00:15:05,920
choice.
 

659
00:15:05,920 --> 00:15:07,990
choice.
and you have some approach to

660
00:15:07,990 --> 00:15:08,000
and you have some approach to
 

661
00:15:08,000 --> 00:15:10,069
and you have some approach to
evaluating. If you're gonna get if you

662
00:15:10,069 --> 00:15:10,079
evaluating. If you're gonna get if you
 

663
00:15:10,079 --> 00:15:11,509
evaluating. If you're gonna get if you
want to be able to get good at doing

664
00:15:11,509 --> 00:15:11,519
want to be able to get good at doing
 

665
00:15:11,519 --> 00:15:12,710
want to be able to get good at doing
something, you have to know if you've

666
00:15:12,710 --> 00:15:12,720
something, you have to know if you've
 

667
00:15:12,720 --> 00:15:15,350
something, you have to know if you've
gotten good at it. Okay, so that just

668
00:15:15,350 --> 00:15:15,360
gotten good at it. Okay, so that just
 

669
00:15:15,360 --> 00:15:18,069
gotten good at it. Okay, so that just
means some kind of hold out set, some

670
00:15:18,069 --> 00:15:18,079
means some kind of hold out set, some
 

671
00:15:18,079 --> 00:15:19,509
means some kind of hold out set, some
way of evaluating performance. So this

672
00:15:19,509 --> 00:15:19,519
way of evaluating performance. So this
 

673
00:15:19,519 --> 00:15:21,030
way of evaluating performance. So this
is all you need this to, you know, start

674
00:15:21,030 --> 00:15:21,040
is all you need this to, you know, start
 

675
00:15:21,040 --> 00:15:23,670
is all you need this to, you know, start
doing it. And so we closed last time

676
00:15:23,670 --> 00:15:23,680
doing it. And so we closed last time
 

677
00:15:23,680 --> 00:15:26,069
doing it. And so we closed last time
with these like two key insights that

678
00:15:26,069 --> 00:15:26,079
with these like two key insights that
 

679
00:15:26,079 --> 00:15:28,550
with these like two key insights that
I'm going to build out this time. And

680
00:15:28,550 --> 00:15:28,560
I'm going to build out this time. And
 

681
00:15:28,560 --> 00:15:31,670
I'm going to build out this time. And
one key insight behind MAML was to say

682
00:15:31,670 --> 00:15:31,680
one key insight behind MAML was to say
 

683
00:15:31,680 --> 00:15:34,470
one key insight behind MAML was to say
that in general for machine learning,

684
00:15:34,470 --> 00:15:34,480
that in general for machine learning,
 

685
00:15:34,480 --> 00:15:37,110
that in general for machine learning,
the default approach is if you're going

686
00:15:37,110 --> 00:15:37,120
the default approach is if you're going
 

687
00:15:37,120 --> 00:15:39,350
the default approach is if you're going
to be doing something at test time, you

688
00:15:39,350 --> 00:15:39,360
to be doing something at test time, you
 

689
00:15:39,360 --> 00:15:40,949
to be doing something at test time, you
should do something like it at train

690
00:15:40,949 --> 00:15:40,959
should do something like it at train
 

691
00:15:40,959 --> 00:15:43,189
should do something like it at train
time if you can.

692
00:15:43,189 --> 00:15:43,199
time if you can.
 

693
00:15:43,199 --> 00:15:45,430
time if you can.
Okay, you do what you're going to do at

694
00:15:45,430 --> 00:15:45,440
Okay, you do what you're going to do at
 

695
00:15:45,440 --> 00:15:47,750
Okay, you do what you're going to do at
testing, do it at training except make

696
00:15:47,750 --> 00:15:47,760
testing, do it at training except make
 

697
00:15:47,760 --> 00:15:49,990
testing, do it at training except make
sure you can take gradient steps, right?

698
00:15:49,990 --> 00:15:50,000
sure you can take gradient steps, right?
 

699
00:15:50,000 --> 00:15:51,509
sure you can take gradient steps, right?
That's kind of how we justified

700
00:15:51,509 --> 00:15:51,519
That's kind of how we justified
 

701
00:15:51,519 --> 00:15:52,710
That's kind of how we justified
everything. Then we said you could go

702
00:15:52,710 --> 00:15:52,720
everything. Then we said you could go
 

703
00:15:52,720 --> 00:15:53,829
everything. Then we said you could go
beyond, right, with certain

704
00:15:53,829 --> 00:15:53,839
beyond, right, with certain
 

705
00:15:53,839 --> 00:15:55,749
beyond, right, with certain
augmentations and things like this. But

706
00:15:55,749 --> 00:15:55,759
augmentations and things like this. But
 

707
00:15:55,759 --> 00:15:58,069
augmentations and things like this. But
the basic approach is do whatever you're

708
00:15:58,069 --> 00:15:58,079
the basic approach is do whatever you're
 

709
00:15:58,079 --> 00:16:00,150
the basic approach is do whatever you're
doing at test time, do that at training

710
00:16:00,150 --> 00:16:00,160
doing at test time, do that at training
 

711
00:16:00,160 --> 00:16:01,749
doing at test time, do that at training
time to get good at and you'll do it at

712
00:16:01,749 --> 00:16:01,759
time to get good at and you'll do it at
 

713
00:16:01,759 --> 00:16:03,910
time to get good at and you'll do it at
test time.

714
00:16:03,910 --> 00:16:03,920
test time.
 

715
00:16:03,920 --> 00:16:06,949
test time.
So that was one. And so

716
00:16:06,949 --> 00:16:06,959
So that was one. And so
 

717
00:16:06,959 --> 00:16:09,910
So that was one. And so
what the next thing is the other key

718
00:16:09,910 --> 00:16:09,920
what the next thing is the other key
 

719
00:16:09,920 --> 00:16:12,550
what the next thing is the other key
insight is to say that what you're going

720
00:16:12,550 --> 00:16:12,560
insight is to say that what you're going
 

721
00:16:12,560 --> 00:16:14,949
insight is to say that what you're going
to be doing at test time here, the goal

722
00:16:14,949 --> 00:16:14,959
to be doing at test time here, the goal
 

723
00:16:14,959 --> 00:16:17,590
to be doing at test time here, the goal
is to get good at learning a new task.

724
00:16:17,590 --> 00:16:17,600
is to get good at learning a new task.
 

725
00:16:17,600 --> 00:16:19,030
is to get good at learning a new task.
So at test time, you're going to be

726
00:16:19,030 --> 00:16:19,040
So at test time, you're going to be
 

727
00:16:19,040 --> 00:16:21,590
So at test time, you're going to be
learning a new task, which means you're

728
00:16:21,590 --> 00:16:21,600
learning a new task, which means you're
 

729
00:16:21,600 --> 00:16:24,069
learning a new task, which means you're
doing SGD steps and so on.

730
00:16:24,069 --> 00:16:24,079
doing SGD steps and so on.
 

731
00:16:24,079 --> 00:16:26,629
doing SGD steps and so on.
Everyone with me? So the insight was

732
00:16:26,629 --> 00:16:26,639
Everyone with me? So the insight was
 

733
00:16:26,639 --> 00:16:28,389
Everyone with me? So the insight was
that

734
00:16:28,389 --> 00:16:28,399
that
 

735
00:16:28,399 --> 00:16:32,790
that
doing SGD steps is like an RNN.

736
00:16:32,790 --> 00:16:32,800
doing SGD steps is like an RNN.
 

737
00:16:32,800 --> 00:16:35,670
doing SGD steps is like an RNN.
I'll make this much more clear in a bit.

738
00:16:35,670 --> 00:16:35,680
I'll make this much more clear in a bit.
 

739
00:16:35,680 --> 00:16:38,310
I'll make this much more clear in a bit.
And because we know we can train RNN's,

740
00:16:38,310 --> 00:16:38,320
And because we know we can train RNN's,
 

741
00:16:38,320 --> 00:16:41,269
And because we know we can train RNN's,
we can train this. Okay. So this is kind

742
00:16:41,269 --> 00:16:41,279
we can train this. Okay. So this is kind
 

743
00:16:41,279 --> 00:16:43,670
we can train this. Okay. So this is kind
of the

744
00:16:43,670 --> 00:16:43,680
of the
 

745
00:16:43,680 --> 00:16:45,749
of the
insight. So before I go into this, I

746
00:16:45,749 --> 00:16:45,759
insight. So before I go into this, I
 

747
00:16:45,759 --> 00:16:47,030
insight. So before I go into this, I
wanted to comment a little bit about

748
00:16:47,030 --> 00:16:47,040
wanted to comment a little bit about
 

749
00:16:47,040 --> 00:16:50,150
wanted to comment a little bit about
this that mammal was very important

750
00:16:50,150 --> 00:16:50,160
this that mammal was very important
 

751
00:16:50,160 --> 00:16:51,749
this that mammal was very important
intellectually for helping us understand

752
00:16:51,749 --> 00:16:51,759
intellectually for helping us understand
 

753
00:16:51,759 --> 00:16:54,389
intellectually for helping us understand
what was going on. And although in

754
00:16:54,389 --> 00:16:54,399
what was going on. And although in
 

755
00:16:54,399 --> 00:16:57,030
what was going on. And although in
practice today for many of these uh

756
00:16:57,030 --> 00:16:57,040
practice today for many of these uh
 

757
00:16:57,040 --> 00:16:59,990
practice today for many of these uh
approaches, one tends to be following

758
00:16:59,990 --> 00:17:00,000
approaches, one tends to be following
 

759
00:17:00,000 --> 00:17:01,910
approaches, one tends to be following
this general foundation model approach

760
00:17:01,910 --> 00:17:01,920
this general foundation model approach
 

761
00:17:01,920 --> 00:17:04,470
this general foundation model approach
as opposed to mammal, mammal had lots of

762
00:17:04,470 --> 00:17:04,480
as opposed to mammal, mammal had lots of
 

763
00:17:04,480 --> 00:17:08,069
as opposed to mammal, mammal had lots of
impact on the entire ecosystem. It could

764
00:17:08,069 --> 00:17:08,079
impact on the entire ecosystem. It could
 

765
00:17:08,079 --> 00:17:10,870
impact on the entire ecosystem. It could
be argued that the development of modern

766
00:17:10,870 --> 00:17:10,880
be argued that the development of modern
 

767
00:17:10,880 --> 00:17:13,429
be argued that the development of modern
libraries like Jax and then the updates

768
00:17:13,429 --> 00:17:13,439
libraries like Jax and then the updates
 

769
00:17:13,439 --> 00:17:15,829
libraries like Jax and then the updates
to PyTorch that came from competing with

770
00:17:15,829 --> 00:17:15,839
to PyTorch that came from competing with
 

771
00:17:15,839 --> 00:17:18,230
to PyTorch that came from competing with
Jax, a lot of this was prompted by the

772
00:17:18,230 --> 00:17:18,240
Jax, a lot of this was prompted by the
 

773
00:17:18,240 --> 00:17:19,909
Jax, a lot of this was prompted by the
need to be able to do things like mammal

774
00:17:19,909 --> 00:17:19,919
need to be able to do things like mammal
 

775
00:17:19,919 --> 00:17:22,870
need to be able to do things like mammal
which was very very clunky in old

776
00:17:22,870 --> 00:17:22,880
which was very very clunky in old
 

777
00:17:22,880 --> 00:17:25,990
which was very very clunky in old
PyTorch and old TensorFlow.

778
00:17:25,990 --> 00:17:26,000
PyTorch and old TensorFlow.
 

779
00:17:26,000 --> 00:17:28,309
PyTorch and old TensorFlow.
Okay, so had a huge effect on the

780
00:17:28,309 --> 00:17:28,319
Okay, so had a huge effect on the
 

781
00:17:28,319 --> 00:17:29,990
Okay, so had a huge effect on the
ecosystem.

782
00:17:29,990 --> 00:17:30,000
ecosystem.
 

783
00:17:30,000 --> 00:17:32,150
ecosystem.
Okay, so let me be precise about this.

784
00:17:32,150 --> 00:17:32,160
Okay, so let me be precise about this.
 

785
00:17:32,160 --> 00:17:35,270
Okay, so let me be precise about this.
So when I say we have a task,

786
00:17:35,270 --> 00:17:35,280
So when I say we have a task,
 

787
00:17:35,280 --> 00:17:37,510
So when I say we have a task,
I'm going to say that a task has some

788
00:17:37,510 --> 00:17:37,520
I'm going to say that a task has some
 

789
00:17:37,520 --> 00:17:40,870
I'm going to say that a task has some
training data. So task I has training

790
00:17:40,870 --> 00:17:40,880
training data. So task I has training
 

791
00:17:40,880 --> 00:17:42,549
training data. So task I has training
data associated with it. It has some

792
00:17:42,549 --> 00:17:42,559
data associated with it. It has some
 

793
00:17:42,559 --> 00:17:44,470
data associated with it. It has some
training loss associated with it and has

794
00:17:44,470 --> 00:17:44,480
training loss associated with it and has
 

795
00:17:44,480 --> 00:17:46,630
training loss associated with it and has
some test loss associated with it. Okay,

796
00:17:46,630 --> 00:17:46,640
some test loss associated with it. Okay,
 

797
00:17:46,640 --> 00:17:48,070
some test loss associated with it. Okay,
that's what it that's what we mean by a

798
00:17:48,070 --> 00:17:48,080
that's what it that's what we mean by a
 

799
00:17:48,080 --> 00:17:51,909
that's what it that's what we mean by a
task having an example of a task.

800
00:17:51,909 --> 00:17:51,919
task having an example of a task.
 

801
00:17:51,919 --> 00:17:53,430
task having an example of a task.
We have a whole bunch of these tasks,

802
00:17:53,430 --> 00:17:53,440
We have a whole bunch of these tasks,
 

803
00:17:53,440 --> 00:17:55,110
We have a whole bunch of these tasks,
whole family of these tasks and we have

804
00:17:55,110 --> 00:17:55,120
whole family of these tasks and we have
 

805
00:17:55,120 --> 00:17:56,630
whole family of these tasks and we have
a whole bunch of examples of this. We

806
00:17:56,630 --> 00:17:56,640
a whole bunch of examples of this. We
 

807
00:17:56,640 --> 00:17:58,789
a whole bunch of examples of this. We
want to train to do well on all tasks

808
00:17:58,789 --> 00:17:58,799
want to train to do well on all tasks
 

809
00:17:58,799 --> 00:18:00,549
want to train to do well on all tasks
from this family including held out

810
00:18:00,549 --> 00:18:00,559
from this family including held out
 

811
00:18:00,559 --> 00:18:02,150
from this family including held out
tasks. task that we will never see

812
00:18:02,150 --> 00:18:02,160
tasks. task that we will never see
 

813
00:18:02,160 --> 00:18:04,710
tasks. task that we will never see
during training is exactly modeling what

814
00:18:04,710 --> 00:18:04,720
during training is exactly modeling what
 

815
00:18:04,720 --> 00:18:08,950
during training is exactly modeling what
you do in machine learning generally.

816
00:18:08,950 --> 00:18:08,960

 

817
00:18:08,960 --> 00:18:11,750

So here there's a little note I want to

818
00:18:11,750 --> 00:18:11,760
So here there's a little note I want to
 

819
00:18:11,760 --> 00:18:16,150
So here there's a little note I want to
make. So let me just make this uh note

820
00:18:16,150 --> 00:18:16,160
make. So let me just make this uh note
 

821
00:18:16,160 --> 00:18:18,789
make. So let me just make this uh note
when I say collection of tasks there

822
00:18:18,789 --> 00:18:18,799
when I say collection of tasks there
 

823
00:18:18,799 --> 00:18:20,549
when I say collection of tasks there
they can be of different types just

824
00:18:20,549 --> 00:18:20,559
they can be of different types just
 

825
00:18:20,559 --> 00:18:22,150
they can be of different types just
related to the same domain. So you could

826
00:18:22,150 --> 00:18:22,160
related to the same domain. So you could
 

827
00:18:22,160 --> 00:18:24,230
related to the same domain. So you could
have a classification task. You could

828
00:18:24,230 --> 00:18:24,240
have a classification task. You could
 

829
00:18:24,240 --> 00:18:26,710
have a classification task. You could
have a regression task. You could have a

830
00:18:26,710 --> 00:18:26,720
have a regression task. You could have a
 

831
00:18:26,720 --> 00:18:29,909
have a regression task. You could have a
classification task that is classifying

832
00:18:29,909 --> 00:18:29,919
classification task that is classifying
 

833
00:18:29,919 --> 00:18:32,630
classification task that is classifying
into you know cats and dogs. You could

834
00:18:32,630 --> 00:18:32,640
into you know cats and dogs. You could
 

835
00:18:32,640 --> 00:18:34,390
into you know cats and dogs. You could
have a different classification task

836
00:18:34,390 --> 00:18:34,400
have a different classification task
 

837
00:18:34,400 --> 00:18:36,549
have a different classification task
that's classifying you know birds and

838
00:18:36,549 --> 00:18:36,559
that's classifying you know birds and
 

839
00:18:36,559 --> 00:18:38,070
that's classifying you know birds and
airplanes.

840
00:18:38,070 --> 00:18:38,080
airplanes.
 

841
00:18:38,080 --> 00:18:39,669
airplanes.
Okay. Somehow they're all related to

842
00:18:39,669 --> 00:18:39,679
Okay. Somehow they're all related to
 

843
00:18:39,679 --> 00:18:41,430
Okay. Somehow they're all related to
each other. So you can have different

844
00:18:41,430 --> 00:18:41,440
each other. So you can have different
 

845
00:18:41,440 --> 00:18:44,230
each other. So you can have different
such things. So

846
00:18:44,230 --> 00:18:44,240
such things. So
 

847
00:18:44,240 --> 00:18:48,470
such things. So
when you have tasks that don't share, if

848
00:18:48,470 --> 00:18:48,480
when you have tasks that don't share, if
 

849
00:18:48,480 --> 00:18:49,990
when you have tasks that don't share, if
all your tasks don't share what their

850
00:18:49,990 --> 00:18:50,000
all your tasks don't share what their
 

851
00:18:50,000 --> 00:18:52,150
all your tasks don't share what their
output looks like, then all of these

852
00:18:52,150 --> 00:18:52,160
output looks like, then all of these
 

853
00:18:52,160 --> 00:18:54,789
output looks like, then all of these
tasks you're going to tune by starting

854
00:18:54,789 --> 00:18:54,799
tasks you're going to tune by starting
 

855
00:18:54,799 --> 00:18:58,870
tasks you're going to tune by starting
with a head that's just zeros.

856
00:18:58,870 --> 00:18:58,880
with a head that's just zeros.
 

857
00:18:58,880 --> 00:19:00,070
with a head that's just zeros.
Okay? Because there's nothing there's

858
00:19:00,070 --> 00:19:00,080
Okay? Because there's nothing there's
 

859
00:19:00,080 --> 00:19:02,470
Okay? Because there's nothing there's
nothing you could already have to start

860
00:19:02,470 --> 00:19:02,480
nothing you could already have to start
 

861
00:19:02,480 --> 00:19:04,310
nothing you could already have to start
ahead with because every task is totally

862
00:19:04,310 --> 00:19:04,320
ahead with because every task is totally
 

863
00:19:04,320 --> 00:19:05,750
ahead with because every task is totally
different.

864
00:19:05,750 --> 00:19:05,760
different.
 

865
00:19:05,760 --> 00:19:09,029
different.
If however they do share some things

866
00:19:09,029 --> 00:19:09,039
If however they do share some things
 

867
00:19:09,039 --> 00:19:11,830
If however they do share some things
then you can have some kind of you could

868
00:19:11,830 --> 00:19:11,840
then you can have some kind of you could
 

869
00:19:11,840 --> 00:19:13,510
then you can have some kind of you could
hope for having a good initialization

870
00:19:13,510 --> 00:19:13,520
hope for having a good initialization
 

871
00:19:13,520 --> 00:19:16,150
hope for having a good initialization
for the head itself. So if many times

872
00:19:16,150 --> 00:19:16,160
for the head itself. So if many times
 

873
00:19:16,160 --> 00:19:17,669
for the head itself. So if many times
you're going to be looking for birds for

874
00:19:17,669 --> 00:19:17,679
you're going to be looking for birds for
 

875
00:19:17,679 --> 00:19:19,750
you're going to be looking for birds for
example there's like a bird score vector

876
00:19:19,750 --> 00:19:19,760
example there's like a bird score vector
 

877
00:19:19,760 --> 00:19:22,549
example there's like a bird score vector
that you probably want to initialize.

878
00:19:22,549 --> 00:19:22,559
that you probably want to initialize.
 

879
00:19:22,559 --> 00:19:24,950
that you probably want to initialize.
Everyone with me on what we're trying to

880
00:19:24,950 --> 00:19:24,960
Everyone with me on what we're trying to
 

881
00:19:24,960 --> 00:19:27,350
Everyone with me on what we're trying to
capture here. So this is very general

882
00:19:27,350 --> 00:19:27,360
capture here. So this is very general
 

883
00:19:27,360 --> 00:19:29,270
capture here. So this is very general
okay like there's many many different uh

884
00:19:29,270 --> 00:19:29,280
okay like there's many many different uh
 

885
00:19:29,280 --> 00:19:32,230
okay like there's many many different uh
tasks and so

886
00:19:32,230 --> 00:19:32,240
tasks and so
 

887
00:19:32,240 --> 00:19:32,950
tasks and so
yeah

888
00:19:32,950 --> 00:19:32,960
yeah
 

889
00:19:32,960 --> 00:19:37,190
yeah
>> so learnable is just the head.

890
00:19:37,190 --> 00:19:37,200
>> so learnable is just the head.
 

891
00:19:37,200 --> 00:19:38,470
>> so learnable is just the head.
>> So what we're going to be doing right

892
00:19:38,470 --> 00:19:38,480
>> So what we're going to be doing right
 

893
00:19:38,480 --> 00:19:40,070
>> So what we're going to be doing right
now is you think about we're doing a

894
00:19:40,070 --> 00:19:40,080
now is you think about we're doing a
 

895
00:19:40,080 --> 00:19:42,870
now is you think about we're doing a
full fine tune of a model for learning a

896
00:19:42,870 --> 00:19:42,880
full fine tune of a model for learning a
 

897
00:19:42,880 --> 00:19:46,150
full fine tune of a model for learning a
new task. So there's the shared part of

898
00:19:46,150 --> 00:19:46,160
new task. So there's the shared part of
 

899
00:19:46,160 --> 00:19:47,830
new task. So there's the shared part of
the model which we're always going to be

900
00:19:47,830 --> 00:19:47,840
the model which we're always going to be
 

901
00:19:47,840 --> 00:19:49,669
the model which we're always going to be
fine-tuning and there's also going to be

902
00:19:49,669 --> 00:19:49,679
fine-tuning and there's also going to be
 

903
00:19:49,679 --> 00:19:53,110
fine-tuning and there's also going to be
a head for the task. The task heads

904
00:19:53,110 --> 00:19:53,120
a head for the task. The task heads
 

905
00:19:53,120 --> 00:19:55,029
a head for the task. The task heads
across the different tasks may be

906
00:19:55,029 --> 00:19:55,039
across the different tasks may be
 

907
00:19:55,039 --> 00:19:56,549
across the different tasks may be
shared.

908
00:19:56,549 --> 00:19:56,559
shared.
 

909
00:19:56,559 --> 00:19:58,230
shared.
They have some shared commonalities to

910
00:19:58,230 --> 00:19:58,240
They have some shared commonalities to
 

911
00:19:58,240 --> 00:20:00,710
They have some shared commonalities to
them. Meaning that certain tasks might

912
00:20:00,710 --> 00:20:00,720
them. Meaning that certain tasks might
 

913
00:20:00,720 --> 00:20:01,990
them. Meaning that certain tasks might
be looking for the same kind of thing

914
00:20:01,990 --> 00:20:02,000
be looking for the same kind of thing
 

915
00:20:02,000 --> 00:20:04,310
be looking for the same kind of thing
like birds. In which case we want to

916
00:20:04,310 --> 00:20:04,320
like birds. In which case we want to
 

917
00:20:04,320 --> 00:20:05,990
like birds. In which case we want to
have a good initialization for the bird

918
00:20:05,990 --> 00:20:06,000
have a good initialization for the bird
 

919
00:20:06,000 --> 00:20:07,669
have a good initialization for the bird
score.

920
00:20:07,669 --> 00:20:07,679
score.
 

921
00:20:07,679 --> 00:20:10,390
score.
But other tasks we expect will be fresh

922
00:20:10,390 --> 00:20:10,400
But other tasks we expect will be fresh
 

923
00:20:10,400 --> 00:20:12,549
But other tasks we expect will be fresh
looking for a totally new thing that

924
00:20:12,549 --> 00:20:12,559
looking for a totally new thing that
 

925
00:20:12,559 --> 00:20:13,909
looking for a totally new thing that
you've never looked for before. And so

926
00:20:13,909 --> 00:20:13,919
you've never looked for before. And so
 

927
00:20:13,919 --> 00:20:14,870
you've never looked for before. And so
there's no way to have a good

928
00:20:14,870 --> 00:20:14,880
there's no way to have a good
 

929
00:20:14,880 --> 00:20:17,190
there's no way to have a good
initialization for that part of the head

930
00:20:17,190 --> 00:20:17,200
initialization for that part of the head
 

931
00:20:17,200 --> 00:20:19,270
initialization for that part of the head
which case we could initialize at zero.

932
00:20:19,270 --> 00:20:19,280
which case we could initialize at zero.
 

933
00:20:19,280 --> 00:20:21,990
which case we could initialize at zero.
Does that make sense? The goal is to

934
00:20:21,990 --> 00:20:22,000
Does that make sense? The goal is to
 

935
00:20:22,000 --> 00:20:23,990
Does that make sense? The goal is to
basically have a model that lets us

936
00:20:23,990 --> 00:20:24,000
basically have a model that lets us
 

937
00:20:24,000 --> 00:20:26,310
basically have a model that lets us
fine-tune it.

938
00:20:26,310 --> 00:20:26,320
fine-tune it.
 

939
00:20:26,320 --> 00:20:28,870
fine-tune it.
Well,

940
00:20:28,870 --> 00:20:28,880
Well,
 

941
00:20:28,880 --> 00:20:31,669
Well,
okay. So,

942
00:20:31,669 --> 00:20:31,679
okay. So,
 

943
00:20:31,679 --> 00:20:33,430
okay. So,
this is just an aside. From now on,

944
00:20:33,430 --> 00:20:33,440
this is just an aside. From now on,
 

945
00:20:33,440 --> 00:20:35,590
this is just an aside. From now on,
we're just going to you can think about

946
00:20:35,590 --> 00:20:35,600
we're just going to you can think about
 

947
00:20:35,600 --> 00:20:38,310
we're just going to you can think about
everything we do as if you want to, you

948
00:20:38,310 --> 00:20:38,320
everything we do as if you want to, you
 

949
00:20:38,320 --> 00:20:41,909
everything we do as if you want to, you
can either assume that we have all

950
00:20:41,909 --> 00:20:41,919
can either assume that we have all
 

951
00:20:41,919 --> 00:20:44,870
can either assume that we have all
shared heads, so there's only the model

952
00:20:44,870 --> 00:20:44,880
shared heads, so there's only the model
 

953
00:20:44,880 --> 00:20:47,029
shared heads, so there's only the model
with its weights, or every task is

954
00:20:47,029 --> 00:20:47,039
with its weights, or every task is
 

955
00:20:47,039 --> 00:20:48,630
with its weights, or every task is
totally different and we always start

956
00:20:48,630 --> 00:20:48,640
totally different and we always start
 

957
00:20:48,640 --> 00:20:50,710
totally different and we always start
with a zero head.

958
00:20:50,710 --> 00:20:50,720
with a zero head.
 

959
00:20:50,720 --> 00:20:52,070
with a zero head.
It's easier to think of it about it that

960
00:20:52,070 --> 00:20:52,080
It's easier to think of it about it that
 

961
00:20:52,080 --> 00:20:53,430
It's easier to think of it about it that
way, but nothing we're saying requires

962
00:20:53,430 --> 00:20:53,440
way, but nothing we're saying requires
 

963
00:20:53,440 --> 00:20:54,710
way, but nothing we're saying requires
that. You could have any kind of

964
00:20:54,710 --> 00:20:54,720
that. You could have any kind of
 

965
00:20:54,720 --> 00:20:56,549
that. You could have any kind of
complicated overlapping structure and

966
00:20:56,549 --> 00:20:56,559
complicated overlapping structure and
 

967
00:20:56,559 --> 00:20:57,990
complicated overlapping structure and
just learn the appropriate

968
00:20:57,990 --> 00:20:58,000
just learn the appropriate
 

969
00:20:58,000 --> 00:21:00,470
just learn the appropriate
initializations for those parts that are

970
00:21:00,470 --> 00:21:00,480
initializations for those parts that are
 

971
00:21:00,480 --> 00:21:03,430
initializations for those parts that are
overlapping.

972
00:21:03,430 --> 00:21:03,440
overlapping.
 

973
00:21:03,440 --> 00:21:07,270
overlapping.
Okay, so now we want to train like we

974
00:21:07,270 --> 00:21:07,280
Okay, so now we want to train like we
 

975
00:21:07,280 --> 00:21:11,909
Okay, so now we want to train like we
test. Okay, so suppose that we succeeded

976
00:21:11,909 --> 00:21:11,919
test. Okay, so suppose that we succeeded
 

977
00:21:11,919 --> 00:21:13,909
test. Okay, so suppose that we succeeded
in getting a good initialization. Let's

978
00:21:13,909 --> 00:21:13,919
in getting a good initialization. Let's
 

979
00:21:13,919 --> 00:21:16,070
in getting a good initialization. Let's
call it theta kn.

980
00:21:16,070 --> 00:21:16,080
call it theta kn.
 

981
00:21:16,080 --> 00:21:19,590
call it theta kn.
How do we use it for our new task? Okay,

982
00:21:19,590 --> 00:21:19,600
How do we use it for our new task? Okay,
 

983
00:21:19,600 --> 00:21:22,950
How do we use it for our new task? Okay,
let's be precise about this.

984
00:21:22,950 --> 00:21:22,960
let's be precise about this.
 

985
00:21:22,960 --> 00:21:27,590
let's be precise about this.
We'll start our model at theta 0.

986
00:21:27,590 --> 00:21:27,600

 

987
00:21:27,600 --> 00:21:29,830

We will then do gradient descent steps

988
00:21:29,830 --> 00:21:29,840
We will then do gradient descent steps
 

989
00:21:29,840 --> 00:21:32,310
We will then do gradient descent steps
using our task specific data and our

990
00:21:32,310 --> 00:21:32,320
using our task specific data and our
 

991
00:21:32,320 --> 00:21:35,029
using our task specific data and our
task specific training loss to get a new

992
00:21:35,029 --> 00:21:35,039
task specific training loss to get a new
 

993
00:21:35,039 --> 00:21:37,270
task specific training loss to get a new
set of parameters theta final. Everyone

994
00:21:37,270 --> 00:21:37,280
set of parameters theta final. Everyone
 

995
00:21:37,280 --> 00:21:41,270
set of parameters theta final. Everyone
with me? Then we will evaluate theta

996
00:21:41,270 --> 00:21:41,280
with me? Then we will evaluate theta
 

997
00:21:41,280 --> 00:21:44,470
with me? Then we will evaluate theta
final using some held out

998
00:21:44,470 --> 00:21:44,480
final using some held out
 

999
00:21:44,480 --> 00:21:48,230
final using some held out
d tilda and some test loss l tilda to

1000
00:21:48,230 --> 00:21:48,240
d tilda and some test loss l tilda to
 

1001
00:21:48,240 --> 00:21:49,990
d tilda and some test loss l tilda to
get a loss. That'll tell us how well we

1002
00:21:49,990 --> 00:21:50,000
get a loss. That'll tell us how well we
 

1003
00:21:50,000 --> 00:21:52,230
get a loss. That'll tell us how well we
do. Everyone good with this? I just want

1004
00:21:52,230 --> 00:21:52,240
do. Everyone good with this? I just want
 

1005
00:21:52,240 --> 00:21:53,909
do. Everyone good with this? I just want
to be very precise about what we're

1006
00:21:53,909 --> 00:21:53,919
to be very precise about what we're
 

1007
00:21:53,919 --> 00:21:55,830
to be very precise about what we're
doing at

1008
00:21:55,830 --> 00:21:55,840
doing at
 

1009
00:21:55,840 --> 00:21:58,630
doing at
uh when we when we when we finished

1010
00:21:58,630 --> 00:21:58,640
uh when we when we when we finished
 

1011
00:21:58,640 --> 00:22:00,870
uh when we when we when we finished
metalarning.

1012
00:22:00,870 --> 00:22:00,880
metalarning.
 

1013
00:22:00,880 --> 00:22:02,870
metalarning.
Any questions on this? This is standard.

1014
00:22:02,870 --> 00:22:02,880
Any questions on this? This is standard.
 

1015
00:22:02,880 --> 00:22:05,190
Any questions on this? This is standard.
This is like what you would do in your

1016
00:22:05,190 --> 00:22:05,200
This is like what you would do in your
 

1017
00:22:05,200 --> 00:22:08,470
This is like what you would do in your
in any ML class.

1018
00:22:08,470 --> 00:22:08,480
in any ML class.
 

1019
00:22:08,480 --> 00:22:10,789
in any ML class.
I'm giving you initialization. Train it

1020
00:22:10,789 --> 00:22:10,799
I'm giving you initialization. Train it
 

1021
00:22:10,799 --> 00:22:12,630
I'm giving you initialization. Train it
and see if you how well you did. That's

1022
00:22:12,630 --> 00:22:12,640
and see if you how well you did. That's
 

1023
00:22:12,640 --> 00:22:15,190
and see if you how well you did. That's
it.

1024
00:22:15,190 --> 00:22:15,200
it.
 

1025
00:22:15,200 --> 00:22:18,710
it.
Okay. So now,

1026
00:22:18,710 --> 00:22:18,720
Okay. So now,
 

1027
00:22:18,720 --> 00:22:22,710
Okay. So now,
>> yeah.

1028
00:22:22,710 --> 00:22:22,720

 

1029
00:22:22,720 --> 00:22:24,630

>> Ah, great. So, let me make me annotate

1030
00:22:24,630 --> 00:22:24,640
>> Ah, great. So, let me make me annotate
 

1031
00:22:24,640 --> 00:22:28,149
>> Ah, great. So, let me make me annotate
this for you.

1032
00:22:28,149 --> 00:22:28,159

 

1033
00:22:28,159 --> 00:22:30,310

This is

1034
00:22:30,310 --> 00:22:30,320
This is
 

1035
00:22:30,320 --> 00:22:33,909
This is
all

1036
00:22:33,909 --> 00:22:33,919

 

1037
00:22:33,919 --> 00:22:36,870

parameters

1038
00:22:36,870 --> 00:22:36,880
parameters
 

1039
00:22:36,880 --> 00:22:39,669
parameters
in the model. The entire par all the

1040
00:22:39,669 --> 00:22:39,679
in the model. The entire par all the
 

1041
00:22:39,679 --> 00:22:41,110
in the model. The entire par all the
parameters you're fine-tuning all the

1042
00:22:41,110 --> 00:22:41,120
parameters you're fine-tuning all the
 

1043
00:22:41,120 --> 00:22:43,190
parameters you're fine-tuning all the
learnable parameters. So all learnable

1044
00:22:43,190 --> 00:22:43,200
learnable parameters. So all learnable
 

1045
00:22:43,200 --> 00:22:47,669
learnable parameters. So all learnable
parameters.

1046
00:22:47,669 --> 00:22:47,679

 

1047
00:22:47,679 --> 00:22:48,870

Okay, that's what we're thinking about

1048
00:22:48,870 --> 00:22:48,880
Okay, that's what we're thinking about
 

1049
00:22:48,880 --> 00:22:51,350
Okay, that's what we're thinking about
here. Now for all these learnable

1050
00:22:51,350 --> 00:22:51,360
here. Now for all these learnable
 

1051
00:22:51,360 --> 00:22:53,110
here. Now for all these learnable
parameters of the model, if you include

1052
00:22:53,110 --> 00:22:53,120
parameters of the model, if you include
 

1053
00:22:53,120 --> 00:22:54,470
parameters of the model, if you include
a head, the head also has to be

1054
00:22:54,470 --> 00:22:54,480
a head, the head also has to be
 

1055
00:22:54,480 --> 00:22:56,870
a head, the head also has to be
initialized to something. So depending

1056
00:22:56,870 --> 00:22:56,880
initialized to something. So depending
 

1057
00:22:56,880 --> 00:22:58,710
initialized to something. So depending
on the setting, you can think of it as

1058
00:22:58,710 --> 00:22:58,720
on the setting, you can think of it as
 

1059
00:22:58,720 --> 00:23:01,590
on the setting, you can think of it as
always initialized to zero. Okay, every

1060
00:23:01,590 --> 00:23:01,600
always initialized to zero. Okay, every
 

1061
00:23:01,600 --> 00:23:03,750
always initialized to zero. Okay, every
task comes with its own task specific

1062
00:23:03,750 --> 00:23:03,760
task comes with its own task specific
 

1063
00:23:03,760 --> 00:23:07,669
task comes with its own task specific
head or all the tasks share the same

1064
00:23:07,669 --> 00:23:07,679
head or all the tasks share the same
 

1065
00:23:07,679 --> 00:23:09,029
head or all the tasks share the same
head structure. They're always going to

1066
00:23:09,029 --> 00:23:09,039
head structure. They're always going to
 

1067
00:23:09,039 --> 00:23:11,750
head structure. They're always going to
be classification among five classes.

1068
00:23:11,750 --> 00:23:11,760
be classification among five classes.
 

1069
00:23:11,760 --> 00:23:13,110
be classification among five classes.
Okay, then you could have a start with

1070
00:23:13,110 --> 00:23:13,120
Okay, then you could have a start with
 

1071
00:23:13,120 --> 00:23:17,029
Okay, then you could have a start with
something. Either way, it it just you

1072
00:23:17,029 --> 00:23:17,039
something. Either way, it it just you
 

1073
00:23:17,039 --> 00:23:20,470
something. Either way, it it just you
have we want is a way to start.

1074
00:23:20,470 --> 00:23:20,480
have we want is a way to start.
 

1075
00:23:20,480 --> 00:23:24,710
have we want is a way to start.
Okay, any questions on this? So when you

1076
00:23:24,710 --> 00:23:24,720
Okay, any questions on this? So when you
 

1077
00:23:24,720 --> 00:23:27,190
Okay, any questions on this? So when you
look at this as what you do, this looks

1078
00:23:27,190 --> 00:23:27,200
look at this as what you do, this looks
 

1079
00:23:27,200 --> 00:23:29,270
look at this as what you do, this looks
very different than we normally do in

1080
00:23:29,270 --> 00:23:29,280
very different than we normally do in
 

1081
00:23:29,280 --> 00:23:32,070
very different than we normally do in
machine learning. Okay, in machine

1082
00:23:32,070 --> 00:23:32,080
machine learning. Okay, in machine
 

1083
00:23:32,080 --> 00:23:34,390
machine learning. Okay, in machine
learning we have a standard form which

1084
00:23:34,390 --> 00:23:34,400
learning we have a standard form which
 

1085
00:23:34,400 --> 00:23:36,789
learning we have a standard form which
is we do this.

1086
00:23:36,789 --> 00:23:36,799
is we do this.
 

1087
00:23:36,799 --> 00:23:39,190
is we do this.
Okay, at test time this is what we do.

1088
00:23:39,190 --> 00:23:39,200
Okay, at test time this is what we do.
 

1089
00:23:39,200 --> 00:23:43,270
Okay, at test time this is what we do.
We have a model that has a parameter. We

1090
00:23:43,270 --> 00:23:43,280
We have a model that has a parameter. We
 

1091
00:23:43,280 --> 00:23:45,909
We have a model that has a parameter. We
feed an input into it.

1092
00:23:45,909 --> 00:23:45,919
feed an input into it.
 

1093
00:23:45,919 --> 00:23:49,029
feed an input into it.
We run to a loss layer with a y and it

1094
00:23:49,029 --> 00:23:49,039
We run to a loss layer with a y and it
 

1095
00:23:49,039 --> 00:23:51,669
We run to a loss layer with a y and it
gives us a real valued loss.

1096
00:23:51,669 --> 00:23:51,679
gives us a real valued loss.
 

1097
00:23:51,679 --> 00:23:53,110
gives us a real valued loss.
And then you train this using gradient

1098
00:23:53,110 --> 00:23:53,120
And then you train this using gradient
 

1099
00:23:53,120 --> 00:23:54,950
And then you train this using gradient
descent. Everyone with me? This is very

1100
00:23:54,950 --> 00:23:54,960
descent. Everyone with me? This is very
 

1101
00:23:54,960 --> 00:23:59,350
descent. Everyone with me? This is very
familiar. That does not look like this.

1102
00:23:59,350 --> 00:23:59,360
familiar. That does not look like this.
 

1103
00:23:59,360 --> 00:24:00,710
familiar. That does not look like this.
Does everyone see how one, two, and

1104
00:24:00,710 --> 00:24:00,720
Does everyone see how one, two, and
 

1105
00:24:00,720 --> 00:24:03,590
Does everyone see how one, two, and
three don't look like this?

1106
00:24:03,590 --> 00:24:03,600
three don't look like this?
 

1107
00:24:03,600 --> 00:24:06,070
three don't look like this?
We know how to do this.

1108
00:24:06,070 --> 00:24:06,080
We know how to do this.
 

1109
00:24:06,080 --> 00:24:08,070
We know how to do this.
So our strategy is we're going to force

1110
00:24:08,070 --> 00:24:08,080
So our strategy is we're going to force
 

1111
00:24:08,080 --> 00:24:10,230
So our strategy is we're going to force
one, two, and three to look like this.

1112
00:24:10,230 --> 00:24:10,240
one, two, and three to look like this.
 

1113
00:24:10,240 --> 00:24:13,669
one, two, and three to look like this.
And then we'll do this.

1114
00:24:13,669 --> 00:24:13,679
And then we'll do this.
 

1115
00:24:13,679 --> 00:24:15,110
And then we'll do this.
Everyone see the big picture of how you

1116
00:24:15,110 --> 00:24:15,120
Everyone see the big picture of how you
 

1117
00:24:15,120 --> 00:24:17,750
Everyone see the big picture of how you
think about things. It's like standard

1118
00:24:17,750 --> 00:24:17,760
think about things. It's like standard
 

1119
00:24:17,760 --> 00:24:19,510
think about things. It's like standard
way of thinking. I don't know how to do

1120
00:24:19,510 --> 00:24:19,520
way of thinking. I don't know how to do
 

1121
00:24:19,520 --> 00:24:22,870
way of thinking. I don't know how to do
something. I know how to do this.

1122
00:24:22,870 --> 00:24:22,880
something. I know how to do this.
 

1123
00:24:22,880 --> 00:24:27,430
something. I know how to do this.
I will try to fit this round peg into

1124
00:24:27,430 --> 00:24:27,440
I will try to fit this round peg into
 

1125
00:24:27,440 --> 00:24:29,750
I will try to fit this round peg into
this square hole.

1126
00:24:29,750 --> 00:24:29,760
this square hole.
 

1127
00:24:29,760 --> 00:24:33,269
this square hole.
Okay. And just see what I can do.

1128
00:24:33,269 --> 00:24:33,279
Okay. And just see what I can do.
 

1129
00:24:33,279 --> 00:24:35,430
Okay. And just see what I can do.
Okay. That's that's what we're trying to

1130
00:24:35,430 --> 00:24:35,440
Okay. That's that's what we're trying to
 

1131
00:24:35,440 --> 00:24:36,789
Okay. That's that's what we're trying to
do. That's why this is useful to teach.

1132
00:24:36,789 --> 00:24:36,799
do. That's why this is useful to teach.
 

1133
00:24:36,799 --> 00:24:38,549
do. That's why this is useful to teach.
It's a good way of thinking. Yeah.

1134
00:24:38,549 --> 00:24:38,559
It's a good way of thinking. Yeah.
 

1135
00:24:38,559 --> 00:24:43,909
It's a good way of thinking. Yeah.
>> So step two is doing

1136
00:24:43,909 --> 00:24:43,919

 

1137
00:24:43,919 --> 00:24:46,549

the final. Is that not

1138
00:24:46,549 --> 00:24:46,559
the final. Is that not
 

1139
00:24:46,559 --> 00:24:50,630
the final. Is that not
what we do?

1140
00:24:50,630 --> 00:24:50,640

 

1141
00:24:50,640 --> 00:24:52,950

We get a lot.

1142
00:24:52,950 --> 00:24:52,960
We get a lot.
 

1143
00:24:52,960 --> 00:24:55,350
We get a lot.
>> This is what we do machine learning all

1144
00:24:55,350 --> 00:24:55,360
>> This is what we do machine learning all
 

1145
00:24:55,360 --> 00:24:55,909
>> This is what we do machine learning all
the time.

1146
00:24:55,909 --> 00:24:55,919
the time.
 

1147
00:24:55,919 --> 00:24:56,549
the time.
>> Yeah. Yeah.

1148
00:24:56,549 --> 00:24:56,559
>> Yeah. Yeah.
 

1149
00:24:56,559 --> 00:25:01,830
>> Yeah. Yeah.
>> Okay. But um this is not test time.

1150
00:25:01,830 --> 00:25:01,840
>> Okay. But um this is not test time.
 

1151
00:25:01,840 --> 00:25:02,870
>> Okay. But um this is not test time.
>> Oh, I see.

1152
00:25:02,870 --> 00:25:02,880
>> Oh, I see.
 

1153
00:25:02,880 --> 00:25:04,390
>> Oh, I see.
>> Okay. This is what test time looks like

1154
00:25:04,390 --> 00:25:04,400
>> Okay. This is what test time looks like
 

1155
00:25:04,400 --> 00:25:06,390
>> Okay. This is what test time looks like
for us in machine learning all the time.

1156
00:25:06,390 --> 00:25:06,400
for us in machine learning all the time.
 

1157
00:25:06,400 --> 00:25:09,350
for us in machine learning all the time.
>> So I want to make this look like test

1158
00:25:09,350 --> 00:25:09,360
>> So I want to make this look like test
 

1159
00:25:09,360 --> 00:25:13,669
>> So I want to make this look like test
time. So just map it. Okay. Because this

1160
00:25:13,669 --> 00:25:13,679
time. So just map it. Okay. Because this
 

1161
00:25:13,679 --> 00:25:15,110
time. So just map it. Okay. Because this
is metalarning. What does that mean?

1162
00:25:15,110 --> 00:25:15,120
is metalarning. What does that mean?
 

1163
00:25:15,120 --> 00:25:17,830
is metalarning. What does that mean?
We're doing learning on our learning.

1164
00:25:17,830 --> 00:25:17,840
We're doing learning on our learning.
 

1165
00:25:17,840 --> 00:25:20,310
We're doing learning on our learning.
Okay. Okay. So, how do we do that is the

1166
00:25:20,310 --> 00:25:20,320
Okay. Okay. So, how do we do that is the
 

1167
00:25:20,320 --> 00:25:22,710
Okay. Okay. So, how do we do that is the
is what we're trying to do. Okay.

1168
00:25:22,710 --> 00:25:22,720
is what we're trying to do. Okay.
 

1169
00:25:22,720 --> 00:25:23,909
is what we're trying to do. Okay.
Everyone understand the kind of spirit

1170
00:25:23,909 --> 00:25:23,919
Everyone understand the kind of spirit
 

1171
00:25:23,919 --> 00:25:26,230
Everyone understand the kind of spirit
of what we're trying to do.

1172
00:25:26,230 --> 00:25:26,240
of what we're trying to do.
 

1173
00:25:26,240 --> 00:25:31,430
of what we're trying to do.
So, now I want you to take one minute,

1174
00:25:31,430 --> 00:25:31,440
So, now I want you to take one minute,
 

1175
00:25:31,440 --> 00:25:35,190
So, now I want you to take one minute,
talk to your neighbors and think

1176
00:25:35,190 --> 00:25:35,200
talk to your neighbors and think
 

1177
00:25:35,200 --> 00:25:37,909
talk to your neighbors and think
how do you how do you do this? So, you

1178
00:25:37,909 --> 00:25:37,919
how do you how do you do this? So, you
 

1179
00:25:37,919 --> 00:25:39,990
how do you how do you do this? So, you
know, like you have to figure out what

1180
00:25:39,990 --> 00:25:40,000
know, like you have to figure out what
 

1181
00:25:40,000 --> 00:25:42,470
know, like you have to figure out what
what's the X,

1182
00:25:42,470 --> 00:25:42,480
what's the X,
 

1183
00:25:42,480 --> 00:25:45,990
what's the X,
what's the Y, what's the loss layer, and

1184
00:25:45,990 --> 00:25:46,000
what's the Y, what's the loss layer, and
 

1185
00:25:46,000 --> 00:25:48,710
what's the Y, what's the loss layer, and
what's this box.

1186
00:25:48,710 --> 00:25:48,720
what's this box.
 

1187
00:25:48,720 --> 00:25:52,149
what's this box.
Okay, so take a minute, talk to your

1188
00:25:52,149 --> 00:25:52,159
Okay, so take a minute, talk to your
 

1189
00:25:52,159 --> 00:25:53,669
Okay, so take a minute, talk to your
neighbors, and see how would you force

1190
00:25:53,669 --> 00:25:53,679
neighbors, and see how would you force
 

1191
00:25:53,679 --> 00:26:24,310
neighbors, and see how would you force
it to be this.

1192
00:26:24,310 --> 00:26:24,320

 

1193
00:26:24,320 --> 00:26:27,110

Sorry.

1194
00:26:27,110 --> 00:26:27,120
Sorry.
 

1195
00:26:27,120 --> 00:26:51,669
Sorry.
I didn't realize

1196
00:26:51,669 --> 00:26:51,679

 

1197
00:26:51,679 --> 00:26:55,350

And then what is

1198
00:26:55,350 --> 00:26:55,360
And then what is
 

1199
00:26:55,360 --> 00:26:58,230
And then what is
the hopper in the bottle?

1200
00:26:58,230 --> 00:26:58,240
the hopper in the bottle?
 

1201
00:26:58,240 --> 00:27:17,350
the hopper in the bottle?
Wait,

1202
00:27:17,350 --> 00:27:17,360

 

1203
00:27:17,360 --> 00:27:31,750

that's the last layer.

1204
00:27:31,750 --> 00:27:31,760

 

1205
00:27:31,760 --> 00:27:57,590

That's true.

1206
00:27:57,590 --> 00:27:57,600

 

1207
00:27:57,600 --> 00:27:59,669

Okay. So, in the interest of time, I'm

1208
00:27:59,669 --> 00:27:59,679
Okay. So, in the interest of time, I'm
 

1209
00:27:59,679 --> 00:28:01,510
Okay. So, in the interest of time, I'm
going to stop everyone.

1210
00:28:01,510 --> 00:28:01,520
going to stop everyone.
 

1211
00:28:01,520 --> 00:28:02,630
going to stop everyone.
>> I'll come back together. Hopefully,

1212
00:28:02,630 --> 00:28:02,640
>> I'll come back together. Hopefully,
 

1213
00:28:02,640 --> 00:28:04,389
>> I'll come back together. Hopefully,
you've had a chance to think about it.

1214
00:28:04,389 --> 00:28:04,399
you've had a chance to think about it.
 

1215
00:28:04,399 --> 00:28:09,350
you've had a chance to think about it.
So, now let's like do it together. Um,

1216
00:28:09,350 --> 00:28:09,360
So, now let's like do it together. Um,
 

1217
00:28:09,360 --> 00:28:13,269
So, now let's like do it together. Um,
X has to be everything that runs through

1218
00:28:13,269 --> 00:28:13,279
X has to be everything that runs through
 

1219
00:28:13,279 --> 00:28:15,669
X has to be everything that runs through
your model. Okay, if it's going to run

1220
00:28:15,669 --> 00:28:15,679
your model. Okay, if it's going to run
 

1221
00:28:15,679 --> 00:28:18,710
your model. Okay, if it's going to run
through the operations of things you're

1222
00:28:18,710 --> 00:28:18,720
through the operations of things you're
 

1223
00:28:18,720 --> 00:28:20,470
through the operations of things you're
doing in one, two, and three, it's got

1224
00:28:20,470 --> 00:28:20,480
doing in one, two, and three, it's got
 

1225
00:28:20,480 --> 00:28:22,310
doing in one, two, and three, it's got
to be an X because that's the only thing

1226
00:28:22,310 --> 00:28:22,320
to be an X because that's the only thing
 

1227
00:28:22,320 --> 00:28:24,870
to be an X because that's the only thing
that runs through stuff is stuff in X.

1228
00:28:24,870 --> 00:28:24,880
that runs through stuff is stuff in X.
 

1229
00:28:24,880 --> 00:28:28,950
that runs through stuff is stuff in X.
Okay, so when you're running through the

1230
00:28:28,950 --> 00:28:28,960
Okay, so when you're running through the
 

1231
00:28:28,960 --> 00:28:30,870
Okay, so when you're running through the
model, what are you running? You're

1232
00:28:30,870 --> 00:28:30,880
model, what are you running? You're
 

1233
00:28:30,880 --> 00:28:33,830
model, what are you running? You're
running all of your training data D

1234
00:28:33,830 --> 00:28:33,840
running all of your training data D
 

1235
00:28:33,840 --> 00:28:37,029
running all of your training data D
through step two.

1236
00:28:37,029 --> 00:28:37,039
through step two.
 

1237
00:28:37,039 --> 00:28:39,990
through step two.
So X has to include D. You're also

1238
00:28:39,990 --> 00:28:40,000
So X has to include D. You're also
 

1239
00:28:40,000 --> 00:28:43,830
So X has to include D. You're also
running your held out data in step three

1240
00:28:43,830 --> 00:28:43,840
running your held out data in step three
 

1241
00:28:43,840 --> 00:28:46,070
running your held out data in step three
through your model. So you have to run

1242
00:28:46,070 --> 00:28:46,080
through your model. So you have to run
 

1243
00:28:46,080 --> 00:28:48,149
through your model. So you have to run
that too. So let's just like try to

1244
00:28:48,149 --> 00:28:48,159
that too. So let's just like try to
 

1245
00:28:48,159 --> 00:28:50,230
that too. So let's just like try to
write it out. Just just make make take

1246
00:28:50,230 --> 00:28:50,240
write it out. Just just make make take
 

1247
00:28:50,240 --> 00:28:54,870
write it out. Just just make make take
note of this. So it's all of D and it's

1248
00:28:54,870 --> 00:28:54,880
note of this. So it's all of D and it's
 

1249
00:28:54,880 --> 00:28:59,510
note of this. So it's all of D and it's
at least the X parts of three. X meaning

1250
00:28:59,510 --> 00:28:59,520
at least the X parts of three. X meaning
 

1251
00:28:59,520 --> 00:29:02,149
at least the X parts of three. X meaning
the inputs. We'll put the Y parts into

1252
00:29:02,149 --> 00:29:02,159
the inputs. We'll put the Y parts into
 

1253
00:29:02,159 --> 00:29:04,149
the inputs. We'll put the Y parts into
the loss.

1254
00:29:04,149 --> 00:29:04,159
the loss.
 

1255
00:29:04,159 --> 00:29:08,710
the loss.
Okay. So similarly

1256
00:29:08,710 --> 00:29:08,720

 

1257
00:29:08,720 --> 00:29:11,190

this is what your final evaluation looks

1258
00:29:11,190 --> 00:29:11,200
this is what your final evaluation looks
 

1259
00:29:11,200 --> 00:29:13,510
this is what your final evaluation looks
like. The only thing we use for the

1260
00:29:13,510 --> 00:29:13,520
like. The only thing we use for the
 

1261
00:29:13,520 --> 00:29:16,470
like. The only thing we use for the
final evaluation is the held out data.

1262
00:29:16,470 --> 00:29:16,480
final evaluation is the held out data.
 

1263
00:29:16,480 --> 00:29:19,430
final evaluation is the held out data.
So the only thing that's going into Y is

1264
00:29:19,430 --> 00:29:19,440
So the only thing that's going into Y is
 

1265
00:29:19,440 --> 00:29:22,389
So the only thing that's going into Y is
the label information in the held out

1266
00:29:22,389 --> 00:29:22,399
the label information in the held out
 

1267
00:29:22,399 --> 00:29:24,470
the label information in the held out
data

1268
00:29:24,470 --> 00:29:24,480
data
 

1269
00:29:24,480 --> 00:29:29,110
data
and it also of course includes the loss.

1270
00:29:29,110 --> 00:29:29,120

 

1271
00:29:29,120 --> 00:29:32,630

Okay, so that's the loss layer. So now

1272
00:29:32,630 --> 00:29:32,640
Okay, so that's the loss layer. So now
 

1273
00:29:32,640 --> 00:29:34,870
Okay, so that's the loss layer. So now
what do we know?

1274
00:29:34,870 --> 00:29:34,880
what do we know?
 

1275
00:29:34,880 --> 00:29:37,590
what do we know?
We know this stuff and we know this

1276
00:29:37,590 --> 00:29:37,600
We know this stuff and we know this
 

1277
00:29:37,600 --> 00:29:40,470
We know this stuff and we know this
stuff. However, this one still remains.

1278
00:29:40,470 --> 00:29:40,480
stuff. However, this one still remains.
 

1279
00:29:40,480 --> 00:29:42,149
stuff. However, this one still remains.
What does it mean to operate the model?

1280
00:29:42,149 --> 00:29:42,159
What does it mean to operate the model?
 

1281
00:29:42,159 --> 00:29:44,950
What does it mean to operate the model?
Everyone with me? So we want to match up

1282
00:29:44,950 --> 00:29:44,960
Everyone with me? So we want to match up
 

1283
00:29:44,960 --> 00:29:48,070
Everyone with me? So we want to match up
to 1 2 3. The output of 1 2 3 is the

1284
00:29:48,070 --> 00:29:48,080
to 1 2 3. The output of 1 2 3 is the
 

1285
00:29:48,080 --> 00:29:49,909
to 1 2 3. The output of 1 2 3 is the
output of the loss layer to get a final

1286
00:29:49,909 --> 00:29:49,919
output of the loss layer to get a final
 

1287
00:29:49,919 --> 00:29:52,710
output of the loss layer to get a final
loss. So now that let us get this.

1288
00:29:52,710 --> 00:29:52,720
loss. So now that let us get this.
 

1289
00:29:52,720 --> 00:29:54,070
loss. So now that let us get this.
Everything that's being fed into the

1290
00:29:54,070 --> 00:29:54,080
Everything that's being fed into the
 

1291
00:29:54,080 --> 00:29:56,789
Everything that's being fed into the
model has to be an X. So we've got that.

1292
00:29:56,789 --> 00:29:56,799
model has to be an X. So we've got that.
 

1293
00:29:56,799 --> 00:29:58,389
model has to be an X. So we've got that.
Now it's just the operation of the model

1294
00:29:58,389 --> 00:29:58,399
Now it's just the operation of the model
 

1295
00:29:58,399 --> 00:30:05,190
Now it's just the operation of the model
we have to deal with.

1296
00:30:05,190 --> 00:30:05,200

 

1297
00:30:05,200 --> 00:30:08,470

So for the operation of the model,

1298
00:30:08,470 --> 00:30:08,480
So for the operation of the model,
 

1299
00:30:08,480 --> 00:30:09,750
So for the operation of the model,
what we're going to do is we're just

1300
00:30:09,750 --> 00:30:09,760
what we're going to do is we're just
 

1301
00:30:09,760 --> 00:30:12,630
what we're going to do is we're just
going to look at what do we do step by

1302
00:30:12,630 --> 00:30:12,640
going to look at what do we do step by
 

1303
00:30:12,640 --> 00:30:15,830
going to look at what do we do step by
step. So

1304
00:30:15,830 --> 00:30:15,840
step. So
 

1305
00:30:15,840 --> 00:30:20,389
step. So
let's just what do you do? Okay,

1306
00:30:20,389 --> 00:30:20,399
let's just what do you do? Okay,
 

1307
00:30:20,399 --> 00:30:22,950
let's just what do you do? Okay,
we do training.

1308
00:30:22,950 --> 00:30:22,960
we do training.
 

1309
00:30:22,960 --> 00:30:25,750
we do training.
We start with theta kn.

1310
00:30:25,750 --> 00:30:25,760
We start with theta kn.
 

1311
00:30:25,760 --> 00:30:28,310
We start with theta kn.
We take theta kn and we do a forward and

1312
00:30:28,310 --> 00:30:28,320
We take theta kn and we do a forward and
 

1313
00:30:28,320 --> 00:30:32,389
We take theta kn and we do a forward and
backward pass on some permuted batch of

1314
00:30:32,389 --> 00:30:32,399
backward pass on some permuted batch of
 

1315
00:30:32,399 --> 00:30:36,950
backward pass on some permuted batch of
my training data.

1316
00:30:36,950 --> 00:30:36,960

 

1317
00:30:36,960 --> 00:30:38,549

The result of the forward and backward

1318
00:30:38,549 --> 00:30:38,559
The result of the forward and backward
 

1319
00:30:38,559 --> 00:30:43,909
The result of the forward and backward
pass is a gradient on the parameters

1320
00:30:43,909 --> 00:30:43,919
pass is a gradient on the parameters
 

1321
00:30:43,919 --> 00:30:45,510
pass is a gradient on the parameters
theta.

1322
00:30:45,510 --> 00:30:45,520
theta.
 

1323
00:30:45,520 --> 00:30:48,630
theta.
Everyone with me? We take the gradient,

1324
00:30:48,630 --> 00:30:48,640
Everyone with me? We take the gradient,
 

1325
00:30:48,640 --> 00:30:52,230
Everyone with me? We take the gradient,
we multiply it by a learning rate.

1326
00:30:52,230 --> 00:30:52,240
we multiply it by a learning rate.
 

1327
00:30:52,240 --> 00:30:56,710
we multiply it by a learning rate.
We then take the parameters and we add

1328
00:30:56,710 --> 00:30:56,720
We then take the parameters and we add
 

1329
00:30:56,720 --> 00:30:59,909
We then take the parameters and we add
the update.

1330
00:30:59,909 --> 00:30:59,919
the update.
 

1331
00:30:59,919 --> 00:31:03,029
the update.
This is an SGD step.

1332
00:31:03,029 --> 00:31:03,039
This is an SGD step.
 

1333
00:31:03,039 --> 00:31:08,230
This is an SGD step.
Okay. And then we do this again

1334
00:31:08,230 --> 00:31:08,240
Okay. And then we do this again
 

1335
00:31:08,240 --> 00:31:11,430
Okay. And then we do this again
to get theta 2.

1336
00:31:11,430 --> 00:31:11,440
to get theta 2.
 

1337
00:31:11,440 --> 00:31:14,710
to get theta 2.
And then we do it again

1338
00:31:14,710 --> 00:31:14,720
And then we do it again
 

1339
00:31:14,720 --> 00:31:18,230
And then we do it again
to get theta 3. And we keep going again

1340
00:31:18,230 --> 00:31:18,240
to get theta 3. And we keep going again
 

1341
00:31:18,240 --> 00:31:19,990
to get theta 3. And we keep going again
and again and again and again and again

1342
00:31:19,990 --> 00:31:20,000
and again and again and again and again
 

1343
00:31:20,000 --> 00:31:21,190
and again and again and again and again
through this until we've gone through

1344
00:31:21,190 --> 00:31:21,200
through this until we've gone through
 

1345
00:31:21,200 --> 00:31:24,310
through this until we've gone through
our training data.

1346
00:31:24,310 --> 00:31:24,320
our training data.
 

1347
00:31:24,320 --> 00:31:26,230
our training data.
Having gone through all these steps, we

1348
00:31:26,230 --> 00:31:26,240
Having gone through all these steps, we
 

1349
00:31:26,240 --> 00:31:29,510
Having gone through all these steps, we
have a final data. Does everyone see?

1350
00:31:29,510 --> 00:31:29,520
have a final data. Does everyone see?
 

1351
00:31:29,520 --> 00:31:31,190
have a final data. Does everyone see?
This is what we do to operate the model.

1352
00:31:31,190 --> 00:31:31,200
This is what we do to operate the model.
 

1353
00:31:31,200 --> 00:31:33,909
This is what we do to operate the model.
We do all this. Now, when you look at

1354
00:31:33,909 --> 00:31:33,919
We do all this. Now, when you look at
 

1355
00:31:33,919 --> 00:31:37,350
We do all this. Now, when you look at
this, you go, "Oh, this looks like an

1356
00:31:37,350 --> 00:31:37,360
this, you go, "Oh, this looks like an
 

1357
00:31:37,360 --> 00:31:38,950
this, you go, "Oh, this looks like an
RNN,

1358
00:31:38,950 --> 00:31:38,960
RNN,
 

1359
00:31:38,960 --> 00:31:41,509
RNN,
right? It's recurrent. You do the same

1360
00:31:41,509 --> 00:31:41,519
right? It's recurrent. You do the same
 

1361
00:31:41,519 --> 00:31:44,950
right? It's recurrent. You do the same
thing over and over again

1362
00:31:44,950 --> 00:31:44,960
thing over and over again
 

1363
00:31:44,960 --> 00:31:48,549
thing over and over again
except you have different inputs

1364
00:31:48,549 --> 00:31:48,559
except you have different inputs
 

1365
00:31:48,559 --> 00:31:52,149
except you have different inputs
uh also coming in to do it

1366
00:31:52,149 --> 00:31:52,159
uh also coming in to do it
 

1367
00:31:52,159 --> 00:31:54,630
uh also coming in to do it
and even has like a little residual

1368
00:31:54,630 --> 00:31:54,640
and even has like a little residual
 

1369
00:31:54,640 --> 00:31:57,110
and even has like a little residual
thing

1370
00:31:57,110 --> 00:31:57,120
thing
 

1371
00:31:57,120 --> 00:31:59,269
thing
on it.

1372
00:31:59,269 --> 00:31:59,279
on it.
 

1373
00:31:59,279 --> 00:32:02,549
on it.
Everyone sees this is what you're doing.

1374
00:32:02,549 --> 00:32:02,559
Everyone sees this is what you're doing.
 

1375
00:32:02,559 --> 00:32:04,870
Everyone sees this is what you're doing.
Great. So we do all this but this isn't

1376
00:32:04,870 --> 00:32:04,880
Great. So we do all this but this isn't
 

1377
00:32:04,880 --> 00:32:07,509
Great. So we do all this but this isn't
we're not done yet.

1378
00:32:07,509 --> 00:32:07,519
we're not done yet.
 

1379
00:32:07,519 --> 00:32:10,789
we're not done yet.
We then take the final thing

1380
00:32:10,789 --> 00:32:10,799
We then take the final thing
 

1381
00:32:10,799 --> 00:32:14,389
We then take the final thing
and we feed in

1382
00:32:14,389 --> 00:32:14,399
and we feed in
 

1383
00:32:14,399 --> 00:32:16,230
and we feed in
just do a forward pass up. There's all

1384
00:32:16,230 --> 00:32:16,240
just do a forward pass up. There's all
 

1385
00:32:16,240 --> 00:32:17,990
just do a forward pass up. There's all
forward and backward passes. This is

1386
00:32:17,990 --> 00:32:18,000
forward and backward passes. This is
 

1387
00:32:18,000 --> 00:32:19,350
forward and backward passes. This is
just a straightforward pass through our

1388
00:32:19,350 --> 00:32:19,360
just a straightforward pass through our
 

1389
00:32:19,360 --> 00:32:21,750
just a straightforward pass through our
network using the parameters theta final

1390
00:32:21,750 --> 00:32:21,760
network using the parameters theta final
 

1391
00:32:21,760 --> 00:32:24,950
network using the parameters theta final
and the held out data detailed x and

1392
00:32:24,950 --> 00:32:24,960
and the held out data detailed x and
 

1393
00:32:24,960 --> 00:32:29,669
and the held out data detailed x and
that gives us a final label or output

1394
00:32:29,669 --> 00:32:29,679
that gives us a final label or output
 

1395
00:32:29,679 --> 00:32:31,509
that gives us a final label or output
of our model which we then feed into the

1396
00:32:31,509 --> 00:32:31,519
of our model which we then feed into the
 

1397
00:32:31,519 --> 00:32:33,909
of our model which we then feed into the
loss layer.

1398
00:32:33,909 --> 00:32:33,919
loss layer.
 

1399
00:32:33,919 --> 00:32:36,389
loss layer.
So

1400
00:32:36,389 --> 00:32:36,399
So
 

1401
00:32:36,399 --> 00:32:38,630
So
that box of operating the model theta kn

1402
00:32:38,630 --> 00:32:38,640
that box of operating the model theta kn
 

1403
00:32:38,640 --> 00:32:43,590
that box of operating the model theta kn
means all of this.

1404
00:32:43,590 --> 00:32:43,600

 

1405
00:32:43,600 --> 00:32:46,230

>> Okay. So questions.

1406
00:32:46,230 --> 00:32:46,240
>> Okay. So questions.
 

1407
00:32:46,240 --> 00:32:46,870
>> Okay. So questions.
>> Yeah.

1408
00:32:46,870 --> 00:32:46,880
>> Yeah.
 

1409
00:32:46,880 --> 00:32:49,430
>> Yeah.
>> Are we just learning the initialization

1410
00:32:49,430 --> 00:32:49,440
>> Are we just learning the initialization
 

1411
00:32:49,440 --> 00:32:50,950
>> Are we just learning the initialization
in this setup?

1412
00:32:50,950 --> 00:32:50,960
in this setup?
 

1413
00:32:50,960 --> 00:32:53,909
in this setup?
>> Our only goal in metalarning is to learn

1414
00:32:53,909 --> 00:32:53,919
>> Our only goal in metalarning is to learn
 

1415
00:32:53,919 --> 00:32:56,549
>> Our only goal in metalarning is to learn
the initialization weights because our

1416
00:32:56,549 --> 00:32:56,559
the initialization weights because our
 

1417
00:32:56,559 --> 00:32:58,389
the initialization weights because our
goal is to have initialization weights

1418
00:32:58,389 --> 00:32:58,399
goal is to have initialization weights
 

1419
00:32:58,399 --> 00:33:01,350
goal is to have initialization weights
that will be good for fine-tuning.

1420
00:33:01,350 --> 00:33:01,360
that will be good for fine-tuning.
 

1421
00:33:01,360 --> 00:33:03,350
that will be good for fine-tuning.
We want a fine-tunable model. That's our

1422
00:33:03,350 --> 00:33:03,360
We want a fine-tunable model. That's our
 

1423
00:33:03,360 --> 00:33:06,389
We want a fine-tunable model. That's our
only objective, a fine-tunable model. We

1424
00:33:06,389 --> 00:33:06,399
only objective, a fine-tunable model. We
 

1425
00:33:06,399 --> 00:33:07,669
only objective, a fine-tunable model. We
don't care about what the fine-tunable

1426
00:33:07,669 --> 00:33:07,679
don't care about what the fine-tunable
 

1427
00:33:07,679 --> 00:33:09,350
don't care about what the fine-tunable
model by itself does. We just want to

1428
00:33:09,350 --> 00:33:09,360
model by itself does. We just want to
 

1429
00:33:09,360 --> 00:33:10,630
model by itself does. We just want to
know that it can be fine-tuned to do

1430
00:33:10,630 --> 00:33:10,640
know that it can be fine-tuned to do
 

1431
00:33:10,640 --> 00:33:15,509
know that it can be fine-tuned to do
what we want.

1432
00:33:15,509 --> 00:33:15,519

 

1433
00:33:15,519 --> 00:33:17,990

So

1434
00:33:17,990 --> 00:33:18,000
So
 

1435
00:33:18,000 --> 00:33:19,750
So
the insight was you look at this is a

1436
00:33:19,750 --> 00:33:19,760
the insight was you look at this is a
 

1437
00:33:19,760 --> 00:33:22,070
the insight was you look at this is a
deep model. You can backrop through it.

1438
00:33:22,070 --> 00:33:22,080
deep model. You can backrop through it.
 

1439
00:33:22,080 --> 00:33:25,029
deep model. You can backrop through it.
Now when you look at a model like this

1440
00:33:25,029 --> 00:33:25,039
Now when you look at a model like this
 

1441
00:33:25,039 --> 00:33:26,549
Now when you look at a model like this
and you look at the back prop through

1442
00:33:26,549 --> 00:33:26,559
and you look at the back prop through
 

1443
00:33:26,559 --> 00:33:29,750
and you look at the back prop through
it, you can be concerned

1444
00:33:29,750 --> 00:33:29,760
it, you can be concerned
 

1445
00:33:29,760 --> 00:33:32,070
it, you can be concerned
about this backdrop because you look at

1446
00:33:32,070 --> 00:33:32,080
about this backdrop because you look at
 

1447
00:33:32,080 --> 00:33:34,549
about this backdrop because you look at
this backdrop and you say, well, you

1448
00:33:34,549 --> 00:33:34,559
this backdrop and you say, well, you
 

1449
00:33:34,559 --> 00:33:37,509
this backdrop and you say, well, you
know, the the gradients are definitely

1450
00:33:37,509 --> 00:33:37,519
know, the the gradients are definitely
 

1451
00:33:37,519 --> 00:33:39,669
know, the the gradients are definitely
going to pass back through. Notice how

1452
00:33:39,669 --> 00:33:39,679
going to pass back through. Notice how
 

1453
00:33:39,679 --> 00:33:40,789
going to pass back through. Notice how
they're going to go through these skips.

1454
00:33:40,789 --> 00:33:40,799
they're going to go through these skips.
 

1455
00:33:40,799 --> 00:33:43,669
they're going to go through these skips.
They're going to go through all of the

1456
00:33:43,669 --> 00:33:43,679
They're going to go through all of the
 

1457
00:33:43,679 --> 00:33:45,509
They're going to go through all of the
layers nicely. They'll go back to the

1458
00:33:45,509 --> 00:33:45,519
layers nicely. They'll go back to the
 

1459
00:33:45,519 --> 00:33:47,669
layers nicely. They'll go back to the
beginning for sure.

1460
00:33:47,669 --> 00:33:47,679
beginning for sure.
 

1461
00:33:47,679 --> 00:33:49,190
beginning for sure.
Everyone will send a gradient back to

1462
00:33:49,190 --> 00:33:49,200
Everyone will send a gradient back to
 

1463
00:33:49,200 --> 00:33:51,909
Everyone will send a gradient back to
the beginning. Uh, so there's definitely

1464
00:33:51,909 --> 00:33:51,919
the beginning. Uh, so there's definitely
 

1465
00:33:51,919 --> 00:33:55,350
the beginning. Uh, so there's definitely
gradients flowing through here, but

1466
00:33:55,350 --> 00:33:55,360
gradients flowing through here, but
 

1467
00:33:55,360 --> 00:33:57,190
gradients flowing through here, but
there's nothing in this that's stopping

1468
00:33:57,190 --> 00:33:57,200
there's nothing in this that's stopping
 

1469
00:33:57,200 --> 00:34:01,750
there's nothing in this that's stopping
them from exploding.

1470
00:34:01,750 --> 00:34:01,760

 

1471
00:34:01,760 --> 00:34:04,070

Okay,

1472
00:34:04,070 --> 00:34:04,080
Okay,
 

1473
00:34:04,080 --> 00:34:06,149
Okay,
did you see that? Like there's nothing

1474
00:34:06,149 --> 00:34:06,159
did you see that? Like there's nothing
 

1475
00:34:06,159 --> 00:34:07,830
did you see that? Like there's nothing
there's no normalization type thing

1476
00:34:07,830 --> 00:34:07,840
there's no normalization type thing
 

1477
00:34:07,840 --> 00:34:09,349
there's no normalization type thing
sitting in here that's going to stop

1478
00:34:09,349 --> 00:34:09,359
sitting in here that's going to stop
 

1479
00:34:09,359 --> 00:34:11,349
sitting in here that's going to stop
these from exploding.

1480
00:34:11,349 --> 00:34:11,359
these from exploding.
 

1481
00:34:11,359 --> 00:34:12,950
these from exploding.
So there's a little bit of a risk of

1482
00:34:12,950 --> 00:34:12,960
So there's a little bit of a risk of
 

1483
00:34:12,960 --> 00:34:17,909
So there's a little bit of a risk of
what might happen.

1484
00:34:17,909 --> 00:34:17,919

 

1485
00:34:17,919 --> 00:34:20,550

Second thing you should notice is that

1486
00:34:20,550 --> 00:34:20,560
Second thing you should notice is that
 

1487
00:34:20,560 --> 00:34:23,349
Second thing you should notice is that
like for training an RNN

1488
00:34:23,349 --> 00:34:23,359
like for training an RNN
 

1489
00:34:23,359 --> 00:34:25,589
like for training an RNN
to do this you have to hold all of these

1490
00:34:25,589 --> 00:34:25,599
to do this you have to hold all of these
 

1491
00:34:25,599 --> 00:34:28,950
to do this you have to hold all of these
activations for doing back prop. So you

1492
00:34:28,950 --> 00:34:28,960
activations for doing back prop. So you
 

1493
00:34:28,960 --> 00:34:30,550
activations for doing back prop. So you
have to hold the activations that

1494
00:34:30,550 --> 00:34:30,560
have to hold the activations that
 

1495
00:34:30,560 --> 00:34:32,869
have to hold the activations that
correspond to the forward and backward

1496
00:34:32,869 --> 00:34:32,879
correspond to the forward and backward
 

1497
00:34:32,879 --> 00:34:34,389
correspond to the forward and backward
pass

1498
00:34:34,389 --> 00:34:34,399
pass
 

1499
00:34:34,399 --> 00:34:36,550
pass
here and the forward and backward pass

1500
00:34:36,550 --> 00:34:36,560
here and the forward and backward pass
 

1501
00:34:36,560 --> 00:34:38,950
here and the forward and backward pass
here. So your model you're building a

1502
00:34:38,950 --> 00:34:38,960
here. So your model you're building a
 

1503
00:34:38,960 --> 00:34:41,270
here. So your model you're building a
lot there's a lot of memory as you roll

1504
00:34:41,270 --> 00:34:41,280
lot there's a lot of memory as you roll
 

1505
00:34:41,280 --> 00:34:45,270
lot there's a lot of memory as you roll
out this learning process for a while.

1506
00:34:45,270 --> 00:34:45,280
out this learning process for a while.
 

1507
00:34:45,280 --> 00:34:46,470
out this learning process for a while.
Okay, so that's another thing. You could

1508
00:34:46,470 --> 00:34:46,480
Okay, so that's another thing. You could
 

1509
00:34:46,480 --> 00:34:48,470
Okay, so that's another thing. You could
run out of memory, but these are

1510
00:34:48,470 --> 00:34:48,480
run out of memory, but these are
 

1511
00:34:48,480 --> 00:34:50,230
run out of memory, but these are
details. You can deal with all of them

1512
00:34:50,230 --> 00:34:50,240
details. You can deal with all of them
 

1513
00:34:50,240 --> 00:34:53,109
details. You can deal with all of them
by taking fewer steps.

1514
00:34:53,109 --> 00:34:53,119
by taking fewer steps.
 

1515
00:34:53,119 --> 00:34:55,430
by taking fewer steps.
Okay.

1516
00:34:55,430 --> 00:34:55,440
Okay.
 

1517
00:34:55,440 --> 00:34:56,230
Okay.
>> Yeah.

1518
00:34:56,230 --> 00:34:56,240
>> Yeah.
 

1519
00:34:56,240 --> 00:34:58,870
>> Yeah.
>> What is the pi subscript?

1520
00:34:58,870 --> 00:34:58,880
>> What is the pi subscript?
 

1521
00:34:58,880 --> 00:35:03,430
>> What is the pi subscript?
>> Oh, the pi just mark it for you

1522
00:35:03,430 --> 00:35:03,440
>> Oh, the pi just mark it for you
 

1523
00:35:03,440 --> 00:35:05,589
>> Oh, the pi just mark it for you
by

1524
00:35:05,589 --> 00:35:05,599
by
 

1525
00:35:05,599 --> 00:35:08,710
by
permutation

1526
00:35:08,710 --> 00:35:08,720

 

1527
00:35:08,720 --> 00:35:15,109

of training data.

1528
00:35:15,109 --> 00:35:15,119

 

1529
00:35:15,119 --> 00:35:17,349

usual thing you run through your data

1530
00:35:17,349 --> 00:35:17,359
usual thing you run through your data
 

1531
00:35:17,359 --> 00:35:22,550
usual thing you run through your data
you permute it in some order randomly.

1532
00:35:22,550 --> 00:35:22,560

 

1533
00:35:22,560 --> 00:35:24,630

Okay,

1534
00:35:24,630 --> 00:35:24,640
Okay,
 

1535
00:35:24,640 --> 00:35:27,349
Okay,
so

1536
00:35:27,349 --> 00:35:27,359
so
 

1537
00:35:27,359 --> 00:35:28,870
so
all of this stuff when you take a

1538
00:35:28,870 --> 00:35:28,880
all of this stuff when you take a
 

1539
00:35:28,880 --> 00:35:30,950
all of this stuff when you take a
gradient through all of this what you

1540
00:35:30,950 --> 00:35:30,960
gradient through all of this what you
 

1541
00:35:30,960 --> 00:35:34,630
gradient through all of this what you
end up with is a gradient on this.

1542
00:35:34,630 --> 00:35:34,640
end up with is a gradient on this.
 

1543
00:35:34,640 --> 00:35:36,630
end up with is a gradient on this.
That's the gradient you care about. You

1544
00:35:36,630 --> 00:35:36,640
That's the gradient you care about. You
 

1545
00:35:36,640 --> 00:35:37,829
That's the gradient you care about. You
don't care about the gradients it's

1546
00:35:37,829 --> 00:35:37,839
don't care about the gradients it's
 

1547
00:35:37,839 --> 00:35:39,670
don't care about the gradients it's
generating along the way. There's no

1548
00:35:39,670 --> 00:35:39,680
generating along the way. There's no
 

1549
00:35:39,680 --> 00:35:42,150
generating along the way. There's no
learnable parameters along the way. The

1550
00:35:42,150 --> 00:35:42,160
learnable parameters along the way. The
 

1551
00:35:42,160 --> 00:35:44,710
learnable parameters along the way. The
only learnable parameters are theta kn.

1552
00:35:44,710 --> 00:35:44,720
only learnable parameters are theta kn.
 

1553
00:35:44,720 --> 00:35:46,230
only learnable parameters are theta kn.
So gradients go all the way back to

1554
00:35:46,230 --> 00:35:46,240
So gradients go all the way back to
 

1555
00:35:46,240 --> 00:35:52,710
So gradients go all the way back to
theta kn.

1556
00:35:52,710 --> 00:35:52,720

 

1557
00:35:52,720 --> 00:35:55,030

Everyone fine with this?

1558
00:35:55,030 --> 00:35:55,040
Everyone fine with this?
 

1559
00:35:55,040 --> 00:35:57,430
Everyone fine with this?
So

1560
00:35:57,430 --> 00:35:57,440
So
 

1561
00:35:57,440 --> 00:36:00,069
So
having a gradient on theta kn I can take

1562
00:36:00,069 --> 00:36:00,079
having a gradient on theta kn I can take
 

1563
00:36:00,079 --> 00:36:01,670
having a gradient on theta kn I can take
a step in that direction for a better

1564
00:36:01,670 --> 00:36:01,680
a step in that direction for a better
 

1565
00:36:01,680 --> 00:36:04,870
a step in that direction for a better
initial condition.

1566
00:36:04,870 --> 00:36:04,880
initial condition.
 

1567
00:36:04,880 --> 00:36:06,630
initial condition.
Right? Usual approach in machine

1568
00:36:06,630 --> 00:36:06,640
Right? Usual approach in machine
 

1569
00:36:06,640 --> 00:36:08,630
Right? Usual approach in machine
learning in deep learning. I want to get

1570
00:36:08,630 --> 00:36:08,640
learning in deep learning. I want to get
 

1571
00:36:08,640 --> 00:36:10,069
learning in deep learning. I want to get
better at something. I look at what the

1572
00:36:10,069 --> 00:36:10,079
better at something. I look at what the
 

1573
00:36:10,079 --> 00:36:11,510
better at something. I look at what the
gradient, which direction the gradient

1574
00:36:11,510 --> 00:36:11,520
gradient, which direction the gradient
 

1575
00:36:11,520 --> 00:36:12,870
gradient, which direction the gradient
points me. I take a step in that

1576
00:36:12,870 --> 00:36:12,880
points me. I take a step in that
 

1577
00:36:12,880 --> 00:36:14,870
points me. I take a step in that
direction and I hope this is better. And

1578
00:36:14,870 --> 00:36:14,880
direction and I hope this is better. And
 

1579
00:36:14,880 --> 00:36:18,310
direction and I hope this is better. And
I keep doing this.

1580
00:36:18,310 --> 00:36:18,320
I keep doing this.
 

1581
00:36:18,320 --> 00:36:20,790
I keep doing this.
So I did this for one task. I can repeat

1582
00:36:20,790 --> 00:36:20,800
So I did this for one task. I can repeat
 

1583
00:36:20,800 --> 00:36:23,430
So I did this for one task. I can repeat
it for a new task. So I have a

1584
00:36:23,430 --> 00:36:23,440
it for a new task. So I have a
 

1585
00:36:23,440 --> 00:36:25,430
it for a new task. So I have a
collection of tasks I'm learning from.

1586
00:36:25,430 --> 00:36:25,440
collection of tasks I'm learning from.
 

1587
00:36:25,440 --> 00:36:28,150
collection of tasks I'm learning from.
So I pick one task, I do this, I get a

1588
00:36:28,150 --> 00:36:28,160
So I pick one task, I do this, I get a
 

1589
00:36:28,160 --> 00:36:29,670
So I pick one task, I do this, I get a
gradient. I take a different task, I do

1590
00:36:29,670 --> 00:36:29,680
gradient. I take a different task, I do
 

1591
00:36:29,680 --> 00:36:31,190
gradient. I take a different task, I do
this, I get another gradient. If I want

1592
00:36:31,190 --> 00:36:31,200
this, I get another gradient. If I want
 

1593
00:36:31,200 --> 00:36:33,270
this, I get another gradient. If I want
to, I can take a step with batch size

1594
00:36:33,270 --> 00:36:33,280
to, I can take a step with batch size
 

1595
00:36:33,280 --> 00:36:35,670
to, I can take a step with batch size
one. Or I could average together a bunch

1596
00:36:35,670 --> 00:36:35,680
one. Or I could average together a bunch
 

1597
00:36:35,680 --> 00:36:38,230
one. Or I could average together a bunch
of batches to get a combined average

1598
00:36:38,230 --> 00:36:38,240
of batches to get a combined average
 

1599
00:36:38,240 --> 00:36:39,910
of batches to get a combined average
gradient. take a step on that. All the

1600
00:36:39,910 --> 00:36:39,920
gradient. take a step on that. All the
 

1601
00:36:39,920 --> 00:36:41,829
gradient. take a step on that. All the
usual things.

1602
00:36:41,829 --> 00:36:41,839
usual things.
 

1603
00:36:41,839 --> 00:36:44,470
usual things.
But this is the the point of mammo is to

1604
00:36:44,470 --> 00:36:44,480
But this is the the point of mammo is to
 

1605
00:36:44,480 --> 00:36:48,150
But this is the the point of mammo is to
say that this is your default approach

1606
00:36:48,150 --> 00:36:48,160
say that this is your default approach
 

1607
00:36:48,160 --> 00:36:49,910
say that this is your default approach
based on everything you've learned of

1608
00:36:49,910 --> 00:36:49,920
based on everything you've learned of
 

1609
00:36:49,920 --> 00:36:54,710
based on everything you've learned of
how you would get good at metalarning.

1610
00:36:54,710 --> 00:36:54,720
how you would get good at metalarning.
 

1611
00:36:54,720 --> 00:36:56,470
how you would get good at metalarning.
So just understanding this and unpacking

1612
00:36:56,470 --> 00:36:56,480
So just understanding this and unpacking
 

1613
00:36:56,480 --> 00:37:03,109
So just understanding this and unpacking
this insight was very useful for people.

1614
00:37:03,109 --> 00:37:03,119

 

1615
00:37:03,119 --> 00:37:06,630

Okay. So is this does this work? It

1616
00:37:06,630 --> 00:37:06,640
Okay. So is this does this work? It
 

1617
00:37:06,640 --> 00:37:08,310
Okay. So is this does this work? It
works in the same sense that you hope

1618
00:37:08,310 --> 00:37:08,320
works in the same sense that you hope
 

1619
00:37:08,320 --> 00:37:09,910
works in the same sense that you hope
everything works. You hope gradient

1620
00:37:09,910 --> 00:37:09,920
everything works. You hope gradient
 

1621
00:37:09,920 --> 00:37:12,230
everything works. You hope gradient
descent gets you to a better place. Same

1622
00:37:12,230 --> 00:37:12,240
descent gets you to a better place. Same
 

1623
00:37:12,240 --> 00:37:15,270
descent gets you to a better place. Same
thing applies here.

1624
00:37:15,270 --> 00:37:15,280
thing applies here.
 

1625
00:37:15,280 --> 00:37:19,190
thing applies here.
Okay. So to help you understand this,

1626
00:37:19,190 --> 00:37:19,200
Okay. So to help you understand this,
 

1627
00:37:19,200 --> 00:37:20,870
Okay. So to help you understand this,
I'm going to just do a schematic

1628
00:37:20,870 --> 00:37:20,880
I'm going to just do a schematic
 

1629
00:37:20,880 --> 00:37:23,349
I'm going to just do a schematic
picture. Same picture, but it's kind of

1630
00:37:23,349 --> 00:37:23,359
picture. Same picture, but it's kind of
 

1631
00:37:23,359 --> 00:37:25,750
picture. Same picture, but it's kind of
done in a more schematic style. So the

1632
00:37:25,750 --> 00:37:25,760
done in a more schematic style. So the
 

1633
00:37:25,760 --> 00:37:27,109
done in a more schematic style. So the
way to think about it is that for

1634
00:37:27,109 --> 00:37:27,119
way to think about it is that for
 

1635
00:37:27,119 --> 00:37:30,310
way to think about it is that for
mammal, there's an outer loop that's on

1636
00:37:30,310 --> 00:37:30,320
mammal, there's an outer loop that's on
 

1637
00:37:30,320 --> 00:37:32,390
mammal, there's an outer loop that's on
tasks

1638
00:37:32,390 --> 00:37:32,400
tasks
 

1639
00:37:32,400 --> 00:37:34,069
tasks
and that has associated with some

1640
00:37:34,069 --> 00:37:34,079
and that has associated with some
 

1641
00:37:34,079 --> 00:37:37,190
and that has associated with some
learning rate ADA out. That ADA out

1642
00:37:37,190 --> 00:37:37,200
learning rate ADA out. That ADA out
 

1643
00:37:37,200 --> 00:37:39,829
learning rate ADA out. That ADA out
outer is the learning rate that's

1644
00:37:39,829 --> 00:37:39,839
outer is the learning rate that's
 

1645
00:37:39,839 --> 00:37:41,910
outer is the learning rate that's
modulating how well how much you add of

1646
00:37:41,910 --> 00:37:41,920
modulating how well how much you add of
 

1647
00:37:41,920 --> 00:37:45,510
modulating how well how much you add of
this into your grade into your updates.

1648
00:37:45,510 --> 00:37:45,520
this into your grade into your updates.
 

1649
00:37:45,520 --> 00:37:47,430
this into your grade into your updates.
You have an inner loop on batches and

1650
00:37:47,430 --> 00:37:47,440
You have an inner loop on batches and
 

1651
00:37:47,440 --> 00:37:50,470
You have an inner loop on batches and
examples for a specific task. That inner

1652
00:37:50,470 --> 00:37:50,480
examples for a specific task. That inner
 

1653
00:37:50,480 --> 00:37:52,950
examples for a specific task. That inner
loop has its own learning rate, an inner

1654
00:37:52,950 --> 00:37:52,960
loop has its own learning rate, an inner
 

1655
00:37:52,960 --> 00:37:54,310
loop has its own learning rate, an inner
learning rate. So there's two learning

1656
00:37:54,310 --> 00:37:54,320
learning rate. So there's two learning
 

1657
00:37:54,320 --> 00:37:56,230
learning rate. So there's two learning
rates associated with this. An outer

1658
00:37:56,230 --> 00:37:56,240
rates associated with this. An outer
 

1659
00:37:56,240 --> 00:37:58,150
rates associated with this. An outer
learning rate, which is the one that

1660
00:37:58,150 --> 00:37:58,160
learning rate, which is the one that
 

1661
00:37:58,160 --> 00:37:59,270
learning rate, which is the one that
you're going to use to actually make

1662
00:37:59,270 --> 00:37:59,280
you're going to use to actually make
 

1663
00:37:59,280 --> 00:38:01,750
you're going to use to actually make
updates, and an inner learning rate just

1664
00:38:01,750 --> 00:38:01,760
updates, and an inner learning rate just
 

1665
00:38:01,760 --> 00:38:05,829
updates, and an inner learning rate just
to do tentative updates. So to draw this

1666
00:38:05,829 --> 00:38:05,839
to do tentative updates. So to draw this
 

1667
00:38:05,839 --> 00:38:09,109
to do tentative updates. So to draw this
out in a picture for you all,

1668
00:38:09,109 --> 00:38:09,119
out in a picture for you all,
 

1669
00:38:09,119 --> 00:38:11,349
out in a picture for you all,
I'll just made a little picture. Okay,

1670
00:38:11,349 --> 00:38:11,359
I'll just made a little picture. Okay,
 

1671
00:38:11,359 --> 00:38:14,470
I'll just made a little picture. Okay,
so this is a little picture.

1672
00:38:14,470 --> 00:38:14,480
so this is a little picture.
 

1673
00:38:14,480 --> 00:38:17,349
so this is a little picture.
So

1674
00:38:17,349 --> 00:38:17,359
So
 

1675
00:38:17,359 --> 00:38:19,349
So
in the outer iterations, we're going to

1676
00:38:19,349 --> 00:38:19,359
in the outer iterations, we're going to
 

1677
00:38:19,359 --> 00:38:21,990
in the outer iterations, we're going to
iterate over tasks.

1678
00:38:21,990 --> 00:38:22,000
iterate over tasks.
 

1679
00:38:22,000 --> 00:38:24,790
iterate over tasks.
We'll try task one, two, three. Now this

1680
00:38:24,790 --> 00:38:24,800
We'll try task one, two, three. Now this
 

1681
00:38:24,800 --> 00:38:27,190
We'll try task one, two, three. Now this
is the when I say the third task, it

1682
00:38:27,190 --> 00:38:27,200
is the when I say the third task, it
 

1683
00:38:27,200 --> 00:38:28,550
is the when I say the third task, it
doesn't mean the third task in my

1684
00:38:28,550 --> 00:38:28,560
doesn't mean the third task in my
 

1685
00:38:28,560 --> 00:38:29,990
doesn't mean the third task in my
collection. It means the third task I

1686
00:38:29,990 --> 00:38:30,000
collection. It means the third task I
 

1687
00:38:30,000 --> 00:38:31,829
collection. It means the third task I
chose to iterate on. So this is like

1688
00:38:31,829 --> 00:38:31,839
chose to iterate on. So this is like
 

1689
00:38:31,839 --> 00:38:34,550
chose to iterate on. So this is like
draw sampling with replacement

1690
00:38:34,550 --> 00:38:34,560
draw sampling with replacement
 

1691
00:38:34,560 --> 00:38:36,630
draw sampling with replacement
tasks, right? I just keep sampling them

1692
00:38:36,630 --> 00:38:36,640
tasks, right? I just keep sampling them
 

1693
00:38:36,640 --> 00:38:39,190
tasks, right? I just keep sampling them
with replacement

1694
00:38:39,190 --> 00:38:39,200
with replacement
 

1695
00:38:39,200 --> 00:38:42,150
with replacement
and I up I start at a initial condition

1696
00:38:42,150 --> 00:38:42,160
and I up I start at a initial condition
 

1697
00:38:42,160 --> 00:38:44,069
and I up I start at a initial condition
for my initial condition which could be

1698
00:38:44,069 --> 00:38:44,079
for my initial condition which could be
 

1699
00:38:44,079 --> 00:38:47,829
for my initial condition which could be
like Xavier initialized for my model or

1700
00:38:47,829 --> 00:38:47,839
like Xavier initialized for my model or
 

1701
00:38:47,839 --> 00:38:50,470
like Xavier initialized for my model or
it could be some pre-trained model based

1702
00:38:50,470 --> 00:38:50,480
it could be some pre-trained model based
 

1703
00:38:50,480 --> 00:38:52,150
it could be some pre-trained model based
on some other set of data that I'm

1704
00:38:52,150 --> 00:38:52,160
on some other set of data that I'm
 

1705
00:38:52,160 --> 00:38:54,390
on some other set of data that I'm
fine-tuning to be good at metalarning

1706
00:38:54,390 --> 00:38:54,400
fine-tuning to be good at metalarning
 

1707
00:38:54,400 --> 00:38:56,710
fine-tuning to be good at metalarning
for a family of tasks. Whatever it is, I

1708
00:38:56,710 --> 00:38:56,720
for a family of tasks. Whatever it is, I
 

1709
00:38:56,720 --> 00:38:58,230
for a family of tasks. Whatever it is, I
start with some initial condition. Then

1710
00:38:58,230 --> 00:38:58,240
start with some initial condition. Then
 

1711
00:38:58,240 --> 00:38:59,750
start with some initial condition. Then
I take gradient steps on the initial

1712
00:38:59,750 --> 00:38:59,760
I take gradient steps on the initial
 

1713
00:38:59,760 --> 00:39:02,470
I take gradient steps on the initial
condition down this way

1714
00:39:02,470 --> 00:39:02,480
condition down this way
 

1715
00:39:02,480 --> 00:39:05,430
condition down this way
until I get to my final initial

1716
00:39:05,430 --> 00:39:05,440
until I get to my final initial
 

1717
00:39:05,440 --> 00:39:07,750
until I get to my final initial
condition.

1718
00:39:07,750 --> 00:39:07,760
condition.
 

1719
00:39:07,760 --> 00:39:10,069
condition.
However, to get these gradient steps,

1720
00:39:10,069 --> 00:39:10,079
However, to get these gradient steps,
 

1721
00:39:10,079 --> 00:39:12,390
However, to get these gradient steps,
what I do is I do inner iterations where

1722
00:39:12,390 --> 00:39:12,400
what I do is I do inner iterations where
 

1723
00:39:12,400 --> 00:39:15,910
what I do is I do inner iterations where
I take for task one, I do I make an

1724
00:39:15,910 --> 00:39:15,920
I take for task one, I do I make an
 

1725
00:39:15,920 --> 00:39:19,109
I take for task one, I do I make an
initialization based on theta 0. So this

1726
00:39:19,109 --> 00:39:19,119
initialization based on theta 0. So this
 

1727
00:39:19,119 --> 00:39:20,710
initialization based on theta 0. So this
could be for example initializing the

1728
00:39:20,710 --> 00:39:20,720
could be for example initializing the
 

1729
00:39:20,720 --> 00:39:21,829
could be for example initializing the
head to zero. It could be initializing

1730
00:39:21,829 --> 00:39:21,839
head to zero. It could be initializing
 

1731
00:39:21,839 --> 00:39:23,190
head to zero. It could be initializing
the head for the overlapping segments

1732
00:39:23,190 --> 00:39:23,200
the head for the overlapping segments
 

1733
00:39:23,200 --> 00:39:26,069
the head for the overlapping segments
based on whatever the specific case is.

1734
00:39:26,069 --> 00:39:26,079
based on whatever the specific case is.
 

1735
00:39:26,079 --> 00:39:29,670
based on whatever the specific case is.
And then I take gradient steps based on

1736
00:39:29,670 --> 00:39:29,680
And then I take gradient steps based on
 

1737
00:39:29,680 --> 00:39:32,710
And then I take gradient steps based on
the training examples for that task.

1738
00:39:32,710 --> 00:39:32,720
the training examples for that task.
 

1739
00:39:32,720 --> 00:39:36,230
the training examples for that task.
I get a loss at the end on held out data

1740
00:39:36,230 --> 00:39:36,240
I get a loss at the end on held out data
 

1741
00:39:36,240 --> 00:39:38,230
I get a loss at the end on held out data
for that task

1742
00:39:38,230 --> 00:39:38,240
for that task
 

1743
00:39:38,240 --> 00:39:40,550
for that task
that gives me a real number. This allows

1744
00:39:40,550 --> 00:39:40,560
that gives me a real number. This allows
 

1745
00:39:40,560 --> 00:39:44,150
that gives me a real number. This allows
me to comput a gradient back onto this

1746
00:39:44,150 --> 00:39:44,160
me to comput a gradient back onto this
 

1747
00:39:44,160 --> 00:39:47,030
me to comput a gradient back onto this
initial condition. That gradient applies

1748
00:39:47,030 --> 00:39:47,040
initial condition. That gradient applies
 

1749
00:39:47,040 --> 00:39:49,510
initial condition. That gradient applies
then here. Okay, there's like a picture

1750
00:39:49,510 --> 00:39:49,520
then here. Okay, there's like a picture
 

1751
00:39:49,520 --> 00:39:52,470
then here. Okay, there's like a picture
of what's going on in mammal.

1752
00:39:52,470 --> 00:39:52,480
of what's going on in mammal.
 

1753
00:39:52,480 --> 00:40:01,829
of what's going on in mammal.
Any questions on this picture?

1754
00:40:01,829 --> 00:40:01,839

 

1755
00:40:01,839 --> 00:40:04,870

It's not a complicated idea

1756
00:40:04,870 --> 00:40:04,880
It's not a complicated idea
 

1757
00:40:04,880 --> 00:40:07,910
It's not a complicated idea
um by itself. It's just taking ideas

1758
00:40:07,910 --> 00:40:07,920
um by itself. It's just taking ideas
 

1759
00:40:07,920 --> 00:40:10,150
um by itself. It's just taking ideas
that you've seen before and connecting

1760
00:40:10,150 --> 00:40:10,160
that you've seen before and connecting
 

1761
00:40:10,160 --> 00:40:14,710
that you've seen before and connecting
them except one level lifted up.

1762
00:40:14,710 --> 00:40:14,720
them except one level lifted up.
 

1763
00:40:14,720 --> 00:40:17,030
them except one level lifted up.
And so, as I mentioned before, being

1764
00:40:17,030 --> 00:40:17,040
And so, as I mentioned before, being
 

1765
00:40:17,040 --> 00:40:21,190
And so, as I mentioned before, being
able to implement this uh really mucked

1766
00:40:21,190 --> 00:40:21,200
able to implement this uh really mucked
 

1767
00:40:21,200 --> 00:40:25,670
able to implement this uh really mucked
up uh old style PyTorch and TensorFlow

1768
00:40:25,670 --> 00:40:25,680
up uh old style PyTorch and TensorFlow
 

1769
00:40:25,680 --> 00:40:27,829
up uh old style PyTorch and TensorFlow
because old style PyTorch and TensorFlow

1770
00:40:27,829 --> 00:40:27,839
because old style PyTorch and TensorFlow
 

1771
00:40:27,839 --> 00:40:30,310
because old style PyTorch and TensorFlow
assumed that this thing would sort of be

1772
00:40:30,310 --> 00:40:30,320
assumed that this thing would sort of be
 

1773
00:40:30,320 --> 00:40:33,349
assumed that this thing would sort of be
frozen. But now we want to like take

1774
00:40:33,349 --> 00:40:33,359
frozen. But now we want to like take
 

1775
00:40:33,359 --> 00:40:36,069
frozen. But now we want to like take
gradients, hold those activations, take

1776
00:40:36,069 --> 00:40:36,079
gradients, hold those activations, take
 

1777
00:40:36,079 --> 00:40:37,430
gradients, hold those activations, take
the gradients through it, and then apply

1778
00:40:37,430 --> 00:40:37,440
the gradients through it, and then apply
 

1779
00:40:37,440 --> 00:40:39,750
the gradients through it, and then apply
another loop. And so that prompted the

1780
00:40:39,750 --> 00:40:39,760
another loop. And so that prompted the
 

1781
00:40:39,760 --> 00:40:42,950
another loop. And so that prompted the
development of lots of tools and and re

1782
00:40:42,950 --> 00:40:42,960
development of lots of tools and and re
 

1783
00:40:42,960 --> 00:40:45,190
development of lots of tools and and re
refactoring of how PyTorch and

1784
00:40:45,190 --> 00:40:45,200
refactoring of how PyTorch and
 

1785
00:40:45,200 --> 00:40:47,829
refactoring of how PyTorch and
TensorFlow worked caused the creation of

1786
00:40:47,829 --> 00:40:47,839
TensorFlow worked caused the creation of
 

1787
00:40:47,839 --> 00:40:50,870
TensorFlow worked caused the creation of
jacks uh to make this kind of stuff

1788
00:40:50,870 --> 00:40:50,880
jacks uh to make this kind of stuff
 

1789
00:40:50,880 --> 00:40:52,550
jacks uh to make this kind of stuff
easier to do. Then those ideas were

1790
00:40:52,550 --> 00:40:52,560
easier to do. Then those ideas were
 

1791
00:40:52,560 --> 00:40:54,390
easier to do. Then those ideas were
backported basically into PyTorch and

1792
00:40:54,390 --> 00:40:54,400
backported basically into PyTorch and
 

1793
00:40:54,400 --> 00:40:58,309
backported basically into PyTorch and
TensorFlow.

1794
00:40:58,309 --> 00:40:58,319

 

1795
00:40:58,319 --> 00:41:00,150

Okay.

1796
00:41:00,150 --> 00:41:00,160
Okay.
 

1797
00:41:00,160 --> 00:41:04,390
Okay.
So what I want to now you understand u

1798
00:41:04,390 --> 00:41:04,400
So what I want to now you understand u
 

1799
00:41:04,400 --> 00:41:07,670
So what I want to now you understand u
basic mammal I want to

1800
00:41:07,670 --> 00:41:07,680
basic mammal I want to
 

1801
00:41:07,680 --> 00:41:12,950
basic mammal I want to
talk about two variants of this idea and

1802
00:41:12,950 --> 00:41:12,960
talk about two variants of this idea and
 

1803
00:41:12,960 --> 00:41:15,270
talk about two variants of this idea and
I want to encourage yeah go ahead

1804
00:41:15,270 --> 00:41:15,280
I want to encourage yeah go ahead
 

1805
00:41:15,280 --> 00:41:16,790
I want to encourage yeah go ahead
>> question

1806
00:41:16,790 --> 00:41:16,800
>> question
 

1807
00:41:16,800 --> 00:41:20,230
>> question
like the loss you take on the parameters

1808
00:41:20,230 --> 00:41:20,240
like the loss you take on the parameters
 

1809
00:41:20,240 --> 00:41:24,069
like the loss you take on the parameters
it's only depend on the held out data

1810
00:41:24,069 --> 00:41:24,079
it's only depend on the held out data
 

1811
00:41:24,079 --> 00:41:27,190
it's only depend on the held out data
>> great question so

1812
00:41:27,190 --> 00:41:27,200
>> great question so
 

1813
00:41:27,200 --> 00:41:30,630
>> great question so
this loss that we take is on the heldout

1814
00:41:30,630 --> 00:41:30,640
this loss that we take is on the heldout
 

1815
00:41:30,640 --> 00:41:32,390
this loss that we take is on the heldout
data for this task. How do I get the

1816
00:41:32,390 --> 00:41:32,400
data for this task. How do I get the
 

1817
00:41:32,400 --> 00:41:34,390
data for this task. How do I get the
held out data for the task? As I

1818
00:41:34,390 --> 00:41:34,400
held out data for the task? As I
 

1819
00:41:34,400 --> 00:41:37,750
held out data for the task? As I
mentioned before, in many cases, there's

1820
00:41:37,750 --> 00:41:37,760
mentioned before, in many cases, there's
 

1821
00:41:37,760 --> 00:41:40,069
mentioned before, in many cases, there's
a limit to how far out we can roll this

1822
00:41:40,069 --> 00:41:40,079
a limit to how far out we can roll this
 

1823
00:41:40,079 --> 00:41:43,030
a limit to how far out we can roll this
thing out based on memory restrictions

1824
00:41:43,030 --> 00:41:43,040
thing out based on memory restrictions
 

1825
00:41:43,040 --> 00:41:45,270
thing out based on memory restrictions
on we can train.

1826
00:41:45,270 --> 00:41:45,280
on we can train.
 

1827
00:41:45,280 --> 00:41:47,829
on we can train.
And so we might have a lot of training

1828
00:41:47,829 --> 00:41:47,839
And so we might have a lot of training
 

1829
00:41:47,839 --> 00:41:50,309
And so we might have a lot of training
data for that task actually. And so

1830
00:41:50,309 --> 00:41:50,319
data for that task actually. And so
 

1831
00:41:50,319 --> 00:41:52,150
data for that task actually. And so
there's only a limited amount we can use

1832
00:41:52,150 --> 00:41:52,160
there's only a limited amount we can use
 

1833
00:41:52,160 --> 00:41:54,630
there's only a limited amount we can use
in this batch. And so we can choose to

1834
00:41:54,630 --> 00:41:54,640
in this batch. And so we can choose to
 

1835
00:41:54,640 --> 00:41:57,030
in this batch. And so we can choose to
generate our own heldout set from that

1836
00:41:57,030 --> 00:41:57,040
generate our own heldout set from that
 

1837
00:41:57,040 --> 00:41:59,510
generate our own heldout set from that
batch of training data. as an example.

1838
00:41:59,510 --> 00:41:59,520
batch of training data. as an example.
 

1839
00:41:59,520 --> 00:42:01,829
batch of training data. as an example.
And when that we revisit that same task

1840
00:42:01,829 --> 00:42:01,839
And when that we revisit that same task
 

1841
00:42:01,839 --> 00:42:04,550
And when that we revisit that same task
again, we might draw a new batch of

1842
00:42:04,550 --> 00:42:04,560
again, we might draw a new batch of
 

1843
00:42:04,560 --> 00:42:06,069
again, we might draw a new batch of
training data and a new batch of held

1844
00:42:06,069 --> 00:42:06,079
training data and a new batch of held
 

1845
00:42:06,079 --> 00:42:08,390
training data and a new batch of held
out data. When we revisit that same task

1846
00:42:08,390 --> 00:42:08,400
out data. When we revisit that same task
 

1847
00:42:08,400 --> 00:42:10,630
out data. When we revisit that same task
later on this these sets of iterations,

1848
00:42:10,630 --> 00:42:10,640
later on this these sets of iterations,
 

1849
00:42:10,640 --> 00:42:12,309
later on this these sets of iterations,
the memory is restricting us in this

1850
00:42:12,309 --> 00:42:12,319
the memory is restricting us in this
 

1851
00:42:12,319 --> 00:42:14,710
the memory is restricting us in this
direction, right? There's no memory

1852
00:42:14,710 --> 00:42:14,720
direction, right? There's no memory
 

1853
00:42:14,720 --> 00:42:16,230
direction, right? There's no memory
restriction on this direction because

1854
00:42:16,230 --> 00:42:16,240
restriction on this direction because
 

1855
00:42:16,240 --> 00:42:18,710
restriction on this direction because
that's just iterations.

1856
00:42:18,710 --> 00:42:18,720
that's just iterations.
 

1857
00:42:18,720 --> 00:42:20,230
that's just iterations.
So this should remind you of some

1858
00:42:20,230 --> 00:42:20,240
So this should remind you of some
 

1859
00:42:20,240 --> 00:42:23,589
So this should remind you of some
things. This should remind you of

1860
00:42:23,589 --> 00:42:23,599
things. This should remind you of
 

1861
00:42:23,599 --> 00:42:26,710
things. This should remind you of
this direction is like context length in

1862
00:42:26,710 --> 00:42:26,720
this direction is like context length in
 

1863
00:42:26,720 --> 00:42:30,230
this direction is like context length in
a language model where you have to train

1864
00:42:30,230 --> 00:42:30,240
a language model where you have to train
 

1865
00:42:30,240 --> 00:42:32,470
a language model where you have to train
on you have to hold the entire context

1866
00:42:32,470 --> 00:42:32,480
on you have to hold the entire context
 

1867
00:42:32,480 --> 00:42:34,230
on you have to hold the entire context
length and do all this stuff on the

1868
00:42:34,230 --> 00:42:34,240
length and do all this stuff on the
 

1869
00:42:34,240 --> 00:42:38,870
length and do all this stuff on the
context. Okay. So if you reflect on what

1870
00:42:38,870 --> 00:42:38,880
context. Okay. So if you reflect on what
 

1871
00:42:38,880 --> 00:42:40,630
context. Okay. So if you reflect on what
we're talking about in mamml, it

1872
00:42:40,630 --> 00:42:40,640
we're talking about in mamml, it
 

1873
00:42:40,640 --> 00:42:42,710
we're talking about in mamml, it
provides a very useful lens to help you

1874
00:42:42,710 --> 00:42:42,720
provides a very useful lens to help you
 

1875
00:42:42,720 --> 00:42:45,990
provides a very useful lens to help you
also think about

1876
00:42:45,990 --> 00:42:46,000
also think about
 

1877
00:42:46,000 --> 00:42:49,589
also think about
what is going on in the training of a

1878
00:42:49,589 --> 00:42:49,599
what is going on in the training of a
 

1879
00:42:49,599 --> 00:42:51,430
what is going on in the training of a
language model that you know is going to

1880
00:42:51,430 --> 00:42:51,440
language model that you know is going to
 

1881
00:42:51,440 --> 00:42:54,390
language model that you know is going to
do ICL. What's ICL? In context learning

1882
00:42:54,390 --> 00:42:54,400
do ICL. What's ICL? In context learning
 

1883
00:42:54,400 --> 00:42:57,030
do ICL. What's ICL? In context learning
is some kind of learning.

1884
00:42:57,030 --> 00:42:57,040
is some kind of learning.
 

1885
00:42:57,040 --> 00:43:00,550
is some kind of learning.
It's like gradient steps.

1886
00:43:00,550 --> 00:43:00,560
It's like gradient steps.
 

1887
00:43:00,560 --> 00:43:03,270
It's like gradient steps.
So the development of incontext learning

1888
00:43:03,270 --> 00:43:03,280
So the development of incontext learning
 

1889
00:43:03,280 --> 00:43:06,630
So the development of incontext learning
capabilities in language models has a

1890
00:43:06,630 --> 00:43:06,640
capabilities in language models has a
 

1891
00:43:06,640 --> 00:43:10,230
capabilities in language models has a
vague parallel to what's happening here.

1892
00:43:10,230 --> 00:43:10,240
vague parallel to what's happening here.
 

1893
00:43:10,240 --> 00:43:13,109
vague parallel to what's happening here.
Okay. But I want to make sure you

1894
00:43:13,109 --> 00:43:13,119
Okay. But I want to make sure you
 

1895
00:43:13,119 --> 00:43:15,109
Okay. But I want to make sure you
understand this. That way you can use it

1896
00:43:15,109 --> 00:43:15,119
understand this. That way you can use it
 

1897
00:43:15,119 --> 00:43:18,710
understand this. That way you can use it
to understand other things too.

1898
00:43:18,710 --> 00:43:18,720
to understand other things too.
 

1899
00:43:18,720 --> 00:43:21,190
to understand other things too.
So comments everything here has to be

1900
00:43:21,190 --> 00:43:21,200
So comments everything here has to be
 

1901
00:43:21,200 --> 00:43:23,270
So comments everything here has to be
differentiable. So if your task comes

1902
00:43:23,270 --> 00:43:23,280
differentiable. So if your task comes
 

1903
00:43:23,280 --> 00:43:24,870
differentiable. So if your task comes
with a held out loss that's not

1904
00:43:24,870 --> 00:43:24,880
with a held out loss that's not
 

1905
00:43:24,880 --> 00:43:26,710
with a held out loss that's not
differentiable, too bad. Switch to one

1906
00:43:26,710 --> 00:43:26,720
differentiable, too bad. Switch to one
 

1907
00:43:26,720 --> 00:43:29,510
differentiable, too bad. Switch to one
that is um for that job. It doesn't have

1908
00:43:29,510 --> 00:43:29,520
that is um for that job. It doesn't have
 

1909
00:43:29,520 --> 00:43:30,630
that is um for that job. It doesn't have
to be the same as your training loss,

1910
00:43:30,630 --> 00:43:30,640
to be the same as your training loss,
 

1911
00:43:30,640 --> 00:43:31,430
to be the same as your training loss,
but it has to be something

1912
00:43:31,430 --> 00:43:31,440
but it has to be something
 

1913
00:43:31,440 --> 00:43:32,470
but it has to be something
differentiable. So you can take a

1914
00:43:32,470 --> 00:43:32,480
differentiable. So you can take a
 

1915
00:43:32,480 --> 00:43:34,550
differentiable. So you can take a
gradient through it. If you can't take a

1916
00:43:34,550 --> 00:43:34,560
gradient through it. If you can't take a
 

1917
00:43:34,560 --> 00:43:36,870
gradient through it. If you can't take a
gradient, if you really want to say, I

1918
00:43:36,870 --> 00:43:36,880
gradient, if you really want to say, I
 

1919
00:43:36,880 --> 00:43:38,550
gradient, if you really want to say, I
want this loss and I can't differentiate

1920
00:43:38,550 --> 00:43:38,560
want this loss and I can't differentiate
 

1921
00:43:38,560 --> 00:43:41,670
want this loss and I can't differentiate
it, then you can do the RL type tricks

1922
00:43:41,670 --> 00:43:41,680
it, then you can do the RL type tricks
 

1923
00:43:41,680 --> 00:43:44,150
it, then you can do the RL type tricks
that we showed you in the homework,

1924
00:43:44,150 --> 00:43:44,160
that we showed you in the homework,
 

1925
00:43:44,160 --> 00:43:45,990
that we showed you in the homework,
right? If I have a non-ifferiable loss,

1926
00:43:45,990 --> 00:43:46,000
right? If I have a non-ifferiable loss,
 

1927
00:43:46,000 --> 00:43:48,230
right? If I have a non-ifferiable loss,
what do you do to make it still have a

1928
00:43:48,230 --> 00:43:48,240
what do you do to make it still have a
 

1929
00:43:48,240 --> 00:43:49,990
what do you do to make it still have a
gradient to train on? You can do those

1930
00:43:49,990 --> 00:43:50,000
gradient to train on? You can do those
 

1931
00:43:50,000 --> 00:43:52,309
gradient to train on? You can do those
tricks. In fact, the first development

1932
00:43:52,309 --> 00:43:52,319
tricks. In fact, the first development
 

1933
00:43:52,319 --> 00:43:54,150
tricks. In fact, the first development
of mammal was in the context of being

1934
00:43:54,150 --> 00:43:54,160
of mammal was in the context of being
 

1935
00:43:54,160 --> 00:43:56,470
of mammal was in the context of being
able to do those RL tricks. But later

1936
00:43:56,470 --> 00:43:56,480
able to do those RL tricks. But later
 

1937
00:43:56,480 --> 00:43:58,390
able to do those RL tricks. But later
people understood that those are not a

1938
00:43:58,390 --> 00:43:58,400
people understood that those are not a
 

1939
00:43:58,400 --> 00:44:00,069
people understood that those are not a
core part of what's required here.

1940
00:44:00,069 --> 00:44:00,079
core part of what's required here.
 

1941
00:44:00,079 --> 00:44:02,630
core part of what's required here.
>> Yeah.

1942
00:44:02,630 --> 00:44:02,640
>> Yeah.
 

1943
00:44:02,640 --> 00:44:05,829
>> Yeah.
>> Like the meta learning the main idea

1944
00:44:05,829 --> 00:44:05,839
>> Like the meta learning the main idea
 

1945
00:44:05,839 --> 00:44:09,270
>> Like the meta learning the main idea
here is that the visualization is a set

1946
00:44:09,270 --> 00:44:09,280
here is that the visualization is a set
 

1947
00:44:09,280 --> 00:44:12,309
here is that the visualization is a set
of parameters that can make us like

1948
00:44:12,309 --> 00:44:12,319
of parameters that can make us like
 

1949
00:44:12,319 --> 00:44:14,069
of parameters that can make us like
learn something really useful in the

1950
00:44:14,069 --> 00:44:14,079
learn something really useful in the
 

1951
00:44:14,079 --> 00:44:14,630
learn something really useful in the
end.

1952
00:44:14,630 --> 00:44:14,640
end.
 

1953
00:44:14,640 --> 00:44:18,390
end.
>> Yes. But um so what about like

1954
00:44:18,390 --> 00:44:18,400
>> Yes. But um so what about like
 

1955
00:44:18,400 --> 00:44:20,150
>> Yes. But um so what about like
computerization

1956
00:44:20,150 --> 00:44:20,160
computerization
 

1957
00:44:20,160 --> 00:44:22,069
computerization
is also like a set of parameters that

1958
00:44:22,069 --> 00:44:22,079
is also like a set of parameters that
 

1959
00:44:22,079 --> 00:44:24,870
is also like a set of parameters that
can make make things like learn learn

1960
00:44:24,870 --> 00:44:24,880
can make make things like learn learn
 

1961
00:44:24,880 --> 00:44:25,910
can make make things like learn learn
faster.

1962
00:44:25,910 --> 00:44:25,920
faster.
 

1963
00:44:25,920 --> 00:44:28,309
faster.
>> Great question. So the question was

1964
00:44:28,309 --> 00:44:28,319
>> Great question. So the question was
 

1965
00:44:28,319 --> 00:44:30,390
>> Great question. So the question was
repeating for the recording the question

1966
00:44:30,390 --> 00:44:30,400
repeating for the recording the question
 

1967
00:44:30,400 --> 00:44:33,990
repeating for the recording the question
was the goal of mammal we said was to

1968
00:44:33,990 --> 00:44:34,000
was the goal of mammal we said was to
 

1969
00:44:34,000 --> 00:44:35,910
was the goal of mammal we said was to
have a initialization that helps us

1970
00:44:35,910 --> 00:44:35,920
have a initialization that helps us
 

1971
00:44:35,920 --> 00:44:39,910
have a initialization that helps us
learn well. Um however

1972
00:44:39,910 --> 00:44:39,920
learn well. Um however
 

1973
00:44:39,920 --> 00:44:41,829
learn well. Um however
is this also initialization that helps

1974
00:44:41,829 --> 00:44:41,839
is this also initialization that helps
 

1975
00:44:41,839 --> 00:44:44,790
is this also initialization that helps
us learn fast right what does it mean to

1976
00:44:44,790 --> 00:44:44,800
us learn fast right what does it mean to
 

1977
00:44:44,800 --> 00:44:46,630
us learn fast right what does it mean to
learn well learning well could be I

1978
00:44:46,630 --> 00:44:46,640
learn well learning well could be I
 

1979
00:44:46,640 --> 00:44:48,470
learn well learning well could be I
converge to a good place at the end

1980
00:44:48,470 --> 00:44:48,480
converge to a good place at the end
 

1981
00:44:48,480 --> 00:44:50,150
converge to a good place at the end
could also mean I converge to a good

1982
00:44:50,150 --> 00:44:50,160
could also mean I converge to a good
 

1983
00:44:50,160 --> 00:44:54,710
could also mean I converge to a good
place faster so definitely in mammal

1984
00:44:54,710 --> 00:44:54,720
place faster so definitely in mammal
 

1985
00:44:54,720 --> 00:44:56,790
place faster so definitely in mammal
because you might have a limit on how

1986
00:44:56,790 --> 00:44:56,800
because you might have a limit on how
 

1987
00:44:56,800 --> 00:45:01,030
because you might have a limit on how
far out you can go based on your memory

1988
00:45:01,030 --> 00:45:01,040
far out you can go based on your memory
 

1989
00:45:01,040 --> 00:45:03,990
far out you can go based on your memory
uh in a sense you are encouraging it to

1990
00:45:03,990 --> 00:45:04,000
uh in a sense you are encouraging it to
 

1991
00:45:04,000 --> 00:45:06,470
uh in a sense you are encouraging it to
learn fast

1992
00:45:06,470 --> 00:45:06,480
learn fast
 

1993
00:45:06,480 --> 00:45:08,870
learn fast
um because the you're only taking steps

1994
00:45:08,870 --> 00:45:08,880
um because the you're only taking steps
 

1995
00:45:08,880 --> 00:45:11,670
um because the you're only taking steps
on fast learnings on basically few shot

1996
00:45:11,670 --> 00:45:11,680
on fast learnings on basically few shot
 

1997
00:45:11,680 --> 00:45:14,069
on fast learnings on basically few shot
learnings. So you're making it even

1998
00:45:14,069 --> 00:45:14,079
learnings. So you're making it even
 

1999
00:45:14,079 --> 00:45:15,270
learnings. So you're making it even
though you say you're making something

2000
00:45:15,270 --> 00:45:15,280
though you say you're making something
 

2001
00:45:15,280 --> 00:45:17,270
though you say you're making something
that's a good metalarner you're actually

2002
00:45:17,270 --> 00:45:17,280
that's a good metalarner you're actually
 

2003
00:45:17,280 --> 00:45:22,230
that's a good metalarner you're actually
training it to be a good fot learner

2004
00:45:22,230 --> 00:45:22,240

 

2005
00:45:22,240 --> 00:45:23,349

which means you're training it to

2006
00:45:23,349 --> 00:45:23,359
which means you're training it to
 

2007
00:45:23,359 --> 00:45:25,829
which means you're training it to
converge fast initialization that will

2008
00:45:25,829 --> 00:45:25,839
converge fast initialization that will
 

2009
00:45:25,839 --> 00:45:29,829
converge fast initialization that will
converge fast to a good place.

2010
00:45:29,829 --> 00:45:29,839
converge fast to a good place.
 

2011
00:45:29,839 --> 00:45:31,670
converge fast to a good place.
So I'll talk about I'll I'll discuss a

2012
00:45:31,670 --> 00:45:31,680
So I'll talk about I'll I'll discuss a
 

2013
00:45:31,680 --> 00:45:33,270
So I'll talk about I'll I'll discuss a
variant in a bit that will try to move

2014
00:45:33,270 --> 00:45:33,280
variant in a bit that will try to move
 

2015
00:45:33,280 --> 00:45:35,270
variant in a bit that will try to move
away from that. Yeah,

2016
00:45:35,270 --> 00:45:35,280
away from that. Yeah,
 

2017
00:45:35,280 --> 00:45:37,510
away from that. Yeah,
>> I'm like a bit confused. This like inner

2018
00:45:37,510 --> 00:45:37,520
>> I'm like a bit confused. This like inner
 

2019
00:45:37,520 --> 00:45:40,470
>> I'm like a bit confused. This like inner
iteration is this like fine tuning or in

2020
00:45:40,470 --> 00:45:40,480
iteration is this like fine tuning or in
 

2021
00:45:40,480 --> 00:45:41,910
iteration is this like fine tuning or in
context learning like how would you

2022
00:45:41,910 --> 00:45:41,920
context learning like how would you
 

2023
00:45:41,920 --> 00:45:45,430
context learning like how would you
descri

2024
00:45:45,430 --> 00:45:45,440

 

2025
00:45:45,440 --> 00:45:48,069

>> this is a fine tune that's running. Um

2026
00:45:48,069 --> 00:45:48,079
>> this is a fine tune that's running. Um
 

2027
00:45:48,079 --> 00:45:49,990
>> this is a fine tune that's running. Um
it's just we have to hold the whole

2028
00:45:49,990 --> 00:45:50,000
it's just we have to hold the whole
 

2029
00:45:50,000 --> 00:45:52,150
it's just we have to hold the whole
thing in memory.

2030
00:45:52,150 --> 00:45:52,160
thing in memory.
 

2031
00:45:52,160 --> 00:45:53,829
thing in memory.
Uh so we're doing a fine tune where

2032
00:45:53,829 --> 00:45:53,839
Uh so we're doing a fine tune where
 

2033
00:45:53,839 --> 00:45:55,349
Uh so we're doing a fine tune where
we're holding everything in memory and

2034
00:45:55,349 --> 00:45:55,359
we're holding everything in memory and
 

2035
00:45:55,359 --> 00:45:57,910
we're holding everything in memory and
we're passing a gradient then at the end

2036
00:45:57,910 --> 00:45:57,920
we're passing a gradient then at the end
 

2037
00:45:57,920 --> 00:46:02,790
we're passing a gradient then at the end
back to update the weights.

2038
00:46:02,790 --> 00:46:02,800
back to update the weights.
 

2039
00:46:02,800 --> 00:46:05,430
back to update the weights.
Okay. So I was making the connection for

2040
00:46:05,430 --> 00:46:05,440
Okay. So I was making the connection for
 

2041
00:46:05,440 --> 00:46:07,990
Okay. So I was making the connection for
you that this is

2042
00:46:07,990 --> 00:46:08,000
you that this is
 

2043
00:46:08,000 --> 00:46:10,309
you that this is
this is also few shot limited in how

2044
00:46:10,309 --> 00:46:10,319
this is also few shot limited in how
 

2045
00:46:10,319 --> 00:46:12,870
this is also few shot limited in how
much stuff you can put. So it feels at a

2046
00:46:12,870 --> 00:46:12,880
much stuff you can put. So it feels at a
 

2047
00:46:12,880 --> 00:46:15,589
much stuff you can put. So it feels at a
high level related to what you're doing

2048
00:46:15,589 --> 00:46:15,599
high level related to what you're doing
 

2049
00:46:15,599 --> 00:46:17,589
high level related to what you're doing
in a language model where you're

2050
00:46:17,589 --> 00:46:17,599
in a language model where you're
 

2051
00:46:17,599 --> 00:46:21,109
in a language model where you're
learning from using context.

2052
00:46:21,109 --> 00:46:21,119
learning from using context.
 

2053
00:46:21,119 --> 00:46:24,870
learning from using context.
>> So this is like context

2054
00:46:24,870 --> 00:46:24,880
>> So this is like context
 

2055
00:46:24,880 --> 00:46:27,109
>> So this is like context
in in many ways.

2056
00:46:27,109 --> 00:46:27,119
in in many ways.
 

2057
00:46:27,119 --> 00:46:28,550
in in many ways.
This helps you understand what might be

2058
00:46:28,550 --> 00:46:28,560
This helps you understand what might be
 

2059
00:46:28,560 --> 00:46:31,510
This helps you understand what might be
happening.

2060
00:46:31,510 --> 00:46:31,520
happening.
 

2061
00:46:31,520 --> 00:46:33,750
happening.
Okay. So a comment was just brought up

2062
00:46:33,750 --> 00:46:33,760
Okay. So a comment was just brought up
 

2063
00:46:33,760 --> 00:46:35,589
Okay. So a comment was just brought up
is this is actually because there might

2064
00:46:35,589 --> 00:46:35,599
is this is actually because there might
 

2065
00:46:35,599 --> 00:46:37,430
is this is actually because there might
be memory limitations or exploiting

2066
00:46:37,430 --> 00:46:37,440
be memory limitations or exploiting
 

2067
00:46:37,440 --> 00:46:39,510
be memory limitations or exploiting
gradient problems that makes you not be

2068
00:46:39,510 --> 00:46:39,520
gradient problems that makes you not be
 

2069
00:46:39,520 --> 00:46:42,069
gradient problems that makes you not be
able to take too many steps. Uh you're

2070
00:46:42,069 --> 00:46:42,079
able to take too many steps. Uh you're
 

2071
00:46:42,079 --> 00:46:43,990
able to take too many steps. Uh you're
actually implicitly saying hey learn

2072
00:46:43,990 --> 00:46:44,000
actually implicitly saying hey learn
 

2073
00:46:44,000 --> 00:46:46,069
actually implicitly saying hey learn
fast. So is there anything we can do

2074
00:46:46,069 --> 00:46:46,079
fast. So is there anything we can do
 

2075
00:46:46,079 --> 00:46:48,309
fast. So is there anything we can do
about that?

2076
00:46:48,309 --> 00:46:48,319
about that?
 

2077
00:46:48,319 --> 00:46:50,950
about that?
And so the first variant I want to talk

2078
00:46:50,950 --> 00:46:50,960
And so the first variant I want to talk
 

2079
00:46:50,960 --> 00:46:53,829
And so the first variant I want to talk
to is the humorously named reptile which

2080
00:46:53,829 --> 00:46:53,839
to is the humorously named reptile which
 

2081
00:46:53,839 --> 00:46:56,870
to is the humorously named reptile which
is literally named because it's more

2082
00:46:56,870 --> 00:46:56,880
is literally named because it's more
 

2083
00:46:56,880 --> 00:46:59,190
is literally named because it's more
primitive than mammal.

2084
00:46:59,190 --> 00:46:59,200
primitive than mammal.
 

2085
00:46:59,200 --> 00:47:03,910
primitive than mammal.
um which says can I avoid having to do

2086
00:47:03,910 --> 00:47:03,920
um which says can I avoid having to do
 

2087
00:47:03,920 --> 00:47:06,630
um which says can I avoid having to do
back prop through back prop because

2088
00:47:06,630 --> 00:47:06,640
back prop through back prop because
 

2089
00:47:06,640 --> 00:47:08,950
back prop through back prop because
remember each of these steps is back

2090
00:47:08,950 --> 00:47:08,960
remember each of these steps is back
 

2091
00:47:08,960 --> 00:47:10,710
remember each of these steps is back
prop and we're doing back prop through

2092
00:47:10,710 --> 00:47:10,720
prop and we're doing back prop through
 

2093
00:47:10,720 --> 00:47:13,109
prop and we're doing back prop through
backdrop so that's what had that's why

2094
00:47:13,109 --> 00:47:13,119
backdrop so that's what had that's why
 

2095
00:47:13,119 --> 00:47:14,550
backdrop so that's what had that's why
we had to hold all this activation all

2096
00:47:14,550 --> 00:47:14,560
we had to hold all this activation all
 

2097
00:47:14,560 --> 00:47:17,670
we had to hold all this activation all
this state around

2098
00:47:17,670 --> 00:47:17,680
this state around
 

2099
00:47:17,680 --> 00:47:21,430
this state around
and the answer is yes you can and the

2100
00:47:21,430 --> 00:47:21,440
and the answer is yes you can and the
 

2101
00:47:21,440 --> 00:47:23,349
and the answer is yes you can and the
approach in mammal in reptile is very

2102
00:47:23,349 --> 00:47:23,359
approach in mammal in reptile is very
 

2103
00:47:23,359 --> 00:47:27,270
approach in mammal in reptile is very
simple what reptile says is to

2104
00:47:27,270 --> 00:47:27,280
simple what reptile says is to
 

2105
00:47:27,280 --> 00:47:29,430
simple what reptile says is to
approximate the gradient So what it does

2106
00:47:29,430 --> 00:47:29,440
approximate the gradient So what it does
 

2107
00:47:29,440 --> 00:47:41,430
approximate the gradient So what it does
is it approximates the gradient.

2108
00:47:41,430 --> 00:47:41,440

 

2109
00:47:41,440 --> 00:47:43,510

Right? So the gradient is computed by

2110
00:47:43,510 --> 00:47:43,520
Right? So the gradient is computed by
 

2111
00:47:43,520 --> 00:47:44,950
Right? So the gradient is computed by
taking back prop through the whole

2112
00:47:44,950 --> 00:47:44,960
taking back prop through the whole
 

2113
00:47:44,960 --> 00:47:47,430
taking back prop through the whole
thing. But one approximation for the

2114
00:47:47,430 --> 00:47:47,440
thing. But one approximation for the
 

2115
00:47:47,440 --> 00:47:50,230
thing. But one approximation for the
gradient is to say look remember let me

2116
00:47:50,230 --> 00:47:50,240
gradient is to say look remember let me
 

2117
00:47:50,240 --> 00:47:52,309
gradient is to say look remember let me
go back the picture. I wish I could hold

2118
00:47:52,309 --> 00:47:52,319
go back the picture. I wish I could hold
 

2119
00:47:52,319 --> 00:47:53,829
go back the picture. I wish I could hold
everything on the this is where I'm

2120
00:47:53,829 --> 00:47:53,839
everything on the this is where I'm
 

2121
00:47:53,839 --> 00:47:55,589
everything on the this is where I'm
missing having a bigger board and using

2122
00:47:55,589 --> 00:47:55,599
missing having a bigger board and using
 

2123
00:47:55,599 --> 00:47:57,910
missing having a bigger board and using
it. So remember this is what we're

2124
00:47:57,910 --> 00:47:57,920
it. So remember this is what we're
 

2125
00:47:57,920 --> 00:48:01,030
it. So remember this is what we're
backing through is this thing. Notice

2126
00:48:01,030 --> 00:48:01,040
backing through is this thing. Notice
 

2127
00:48:01,040 --> 00:48:04,069
backing through is this thing. Notice
all the residual connections.

2128
00:48:04,069 --> 00:48:04,079
all the residual connections.
 

2129
00:48:04,079 --> 00:48:06,069
all the residual connections.
Okay.

2130
00:48:06,069 --> 00:48:06,079
Okay.
 

2131
00:48:06,079 --> 00:48:08,710
Okay.
So at some level if I want to

2132
00:48:08,710 --> 00:48:08,720
So at some level if I want to
 

2133
00:48:08,720 --> 00:48:11,349
So at some level if I want to
approximate the gradient

2134
00:48:11,349 --> 00:48:11,359
approximate the gradient
 

2135
00:48:11,359 --> 00:48:13,910
approximate the gradient
one approximation for the gradient is

2136
00:48:13,910 --> 00:48:13,920
one approximation for the gradient is
 

2137
00:48:13,920 --> 00:48:16,470
one approximation for the gradient is
just the gradient is saying what

2138
00:48:16,470 --> 00:48:16,480
just the gradient is saying what
 

2139
00:48:16,480 --> 00:48:19,750
just the gradient is saying what
direction do I wish theta knot moves.

2140
00:48:19,750 --> 00:48:19,760
direction do I wish theta knot moves.
 

2141
00:48:19,760 --> 00:48:21,430
direction do I wish theta knot moves.
Okay that's what a gradient is telling

2142
00:48:21,430 --> 00:48:21,440
Okay that's what a gradient is telling
 

2143
00:48:21,440 --> 00:48:24,950
Okay that's what a gradient is telling
me. Well, one approximation is where did

2144
00:48:24,950 --> 00:48:24,960
me. Well, one approximation is where did
 

2145
00:48:24,960 --> 00:48:29,670
me. Well, one approximation is where did
I actually move?

2146
00:48:29,670 --> 00:48:29,680

 

2147
00:48:29,680 --> 00:48:31,829

I moved to theta final. That was

2148
00:48:31,829 --> 00:48:31,839
I moved to theta final. That was
 

2149
00:48:31,839 --> 00:48:34,390
I moved to theta final. That was
presumably what I wanted to do.

2150
00:48:34,390 --> 00:48:34,400
presumably what I wanted to do.
 

2151
00:48:34,400 --> 00:48:37,910
presumably what I wanted to do.
And so one approximation is just theta

2152
00:48:37,910 --> 00:48:37,920
And so one approximation is just theta
 

2153
00:48:37,920 --> 00:48:40,870
And so one approximation is just theta
final minus theta kn.

2154
00:48:40,870 --> 00:48:40,880
final minus theta kn.
 

2155
00:48:40,880 --> 00:48:42,710
final minus theta kn.
Does everyone see that? Like what

2156
00:48:42,710 --> 00:48:42,720
Does everyone see that? Like what
 

2157
00:48:42,720 --> 00:48:44,150
Does everyone see that? Like what
direction do I want to move? Where did

2158
00:48:44,150 --> 00:48:44,160
direction do I want to move? Where did
 

2159
00:48:44,160 --> 00:48:46,950
direction do I want to move? Where did
you move?

2160
00:48:46,950 --> 00:48:46,960
you move?
 

2161
00:48:46,960 --> 00:48:50,230
you move?
Right? They're related, right?

2162
00:48:50,230 --> 00:48:50,240
Right? They're related, right?
 

2163
00:48:50,240 --> 00:48:53,030
Right? They're related, right?
And so I can approximate the gradient

2164
00:48:53,030 --> 00:48:53,040
And so I can approximate the gradient
 

2165
00:48:53,040 --> 00:49:08,069
And so I can approximate the gradient
with theta final minus theta.

2166
00:49:08,069 --> 00:49:08,079

 

2167
00:49:08,079 --> 00:49:09,670

This is within this is within one of

2168
00:49:09,670 --> 00:49:09,680
This is within this is within one of
 

2169
00:49:09,680 --> 00:49:12,549
This is within this is within one of
these horizontal segments.

2170
00:49:12,549 --> 00:49:12,559
these horizontal segments.
 

2171
00:49:12,559 --> 00:49:15,109
these horizontal segments.
So because with this approximation I

2172
00:49:15,109 --> 00:49:15,119
So because with this approximation I
 

2173
00:49:15,119 --> 00:49:18,710
So because with this approximation I
have the direction of the gradient.

2174
00:49:18,710 --> 00:49:18,720
have the direction of the gradient.
 

2175
00:49:18,720 --> 00:49:20,390
have the direction of the gradient.
Okay,

2176
00:49:20,390 --> 00:49:20,400
Okay,
 

2177
00:49:20,400 --> 00:49:22,150
Okay,
approximately

2178
00:49:22,150 --> 00:49:22,160
approximately
 

2179
00:49:22,160 --> 00:49:24,870
approximately
I'm not going to take a step in that

2180
00:49:24,870 --> 00:49:24,880
I'm not going to take a step in that
 

2181
00:49:24,880 --> 00:49:26,150
I'm not going to take a step in that
with that side. That would just be going

2182
00:49:26,150 --> 00:49:26,160
with that side. That would just be going
 

2183
00:49:26,160 --> 00:49:28,230
with that side. That would just be going
to theta final. I'm not going to do that

2184
00:49:28,230 --> 00:49:28,240
to theta final. I'm not going to do that
 

2185
00:49:28,240 --> 00:49:29,589
to theta final. I'm not going to do that
because I don't do that in gradient

2186
00:49:29,589 --> 00:49:29,599
because I don't do that in gradient
 

2187
00:49:29,599 --> 00:49:32,549
because I don't do that in gradient
descent. How big of a step do I take? I

2188
00:49:32,549 --> 00:49:32,559
descent. How big of a step do I take? I
 

2189
00:49:32,559 --> 00:49:34,790
descent. How big of a step do I take? I
take a step that's small in the

2190
00:49:34,790 --> 00:49:34,800
take a step that's small in the
 

2191
00:49:34,800 --> 00:49:37,670
take a step that's small in the
direction of the gradient. So I take a a

2192
00:49:37,670 --> 00:49:37,680
direction of the gradient. So I take a a
 

2193
00:49:37,680 --> 00:49:41,589
direction of the gradient. So I take a a
direction that's this thing small. I

2194
00:49:41,589 --> 00:49:41,599
direction that's this thing small. I
 

2195
00:49:41,599 --> 00:49:42,950
direction that's this thing small. I
have a learning rate. I don't go

2196
00:49:42,950 --> 00:49:42,960
have a learning rate. I don't go
 

2197
00:49:42,960 --> 00:49:44,710
have a learning rate. I don't go
straight. So this is different than

2198
00:49:44,710 --> 00:49:44,720
straight. So this is different than
 

2199
00:49:44,720 --> 00:49:46,950
straight. So this is different than
simply training the model on all the

2200
00:49:46,950 --> 00:49:46,960
simply training the model on all the
 

2201
00:49:46,960 --> 00:49:49,349
simply training the model on all the
tasks one after the other. If I train

2202
00:49:49,349 --> 00:49:49,359
tasks one after the other. If I train
 

2203
00:49:49,359 --> 00:49:50,870
tasks one after the other. If I train
the model on all the tasks one at the

2204
00:49:50,870 --> 00:49:50,880
the model on all the tasks one at the
 

2205
00:49:50,880 --> 00:49:52,630
the model on all the tasks one at the
other one after the other I would just

2206
00:49:52,630 --> 00:49:52,640
other one after the other I would just
 

2207
00:49:52,640 --> 00:49:55,990
other one after the other I would just
take this final thing

2208
00:49:55,990 --> 00:49:56,000
take this final thing
 

2209
00:49:56,000 --> 00:49:58,790
take this final thing
and move it to here.

2210
00:49:58,790 --> 00:49:58,800
and move it to here.
 

2211
00:49:58,800 --> 00:50:00,230
and move it to here.
Does everyone see that? That would be

2212
00:50:00,230 --> 00:50:00,240
Does everyone see that? That would be
 

2213
00:50:00,240 --> 00:50:01,670
Does everyone see that? That would be
training the model on all the tasks one

2214
00:50:01,670 --> 00:50:01,680
training the model on all the tasks one
 

2215
00:50:01,680 --> 00:50:03,910
training the model on all the tasks one
after the other. We don't do that.

2216
00:50:03,910 --> 00:50:03,920
after the other. We don't do that.
 

2217
00:50:03,920 --> 00:50:07,990
after the other. We don't do that.
Instead we just say from here I would go

2218
00:50:07,990 --> 00:50:08,000
Instead we just say from here I would go
 

2219
00:50:08,000 --> 00:50:10,390
Instead we just say from here I would go
to here in this direction. I'll take a

2220
00:50:10,390 --> 00:50:10,400
to here in this direction. I'll take a
 

2221
00:50:10,400 --> 00:50:11,990
to here in this direction. I'll take a
small step in that direction and that'll

2222
00:50:11,990 --> 00:50:12,000
small step in that direction and that'll
 

2223
00:50:12,000 --> 00:50:14,069
small step in that direction and that'll
be my update.

2224
00:50:14,069 --> 00:50:14,079
be my update.
 

2225
00:50:14,079 --> 00:50:16,630
be my update.
And what's the idea? The idea in a

2226
00:50:16,630 --> 00:50:16,640
And what's the idea? The idea in a
 

2227
00:50:16,640 --> 00:50:19,829
And what's the idea? The idea in a
reptile is to say that this approximates

2228
00:50:19,829 --> 00:50:19,839
reptile is to say that this approximates
 

2229
00:50:19,839 --> 00:50:21,510
reptile is to say that this approximates
the gradient but has a huge

2230
00:50:21,510 --> 00:50:21,520
the gradient but has a huge
 

2231
00:50:21,520 --> 00:50:23,030
the gradient but has a huge
computational savings because now I

2232
00:50:23,030 --> 00:50:23,040
computational savings because now I
 

2233
00:50:23,040 --> 00:50:25,349
computational savings because now I
don't have to save all the activations.

2234
00:50:25,349 --> 00:50:25,359
don't have to save all the activations.
 

2235
00:50:25,359 --> 00:50:29,109
don't have to save all the activations.
I can just run the RNN forward as many

2236
00:50:29,109 --> 00:50:29,119
I can just run the RNN forward as many
 

2237
00:50:29,119 --> 00:50:31,910
I can just run the RNN forward as many
steps as I want.

2238
00:50:31,910 --> 00:50:31,920
steps as I want.
 

2239
00:50:31,920 --> 00:50:33,349
steps as I want.
It also makes it easier to use other

2240
00:50:33,349 --> 00:50:33,359
It also makes it easier to use other
 

2241
00:50:33,359 --> 00:50:35,430
It also makes it easier to use other
optimizers

2242
00:50:35,430 --> 00:50:35,440
optimizers
 

2243
00:50:35,440 --> 00:50:36,870
optimizers
because I can, you know, it's easier to

2244
00:50:36,870 --> 00:50:36,880
because I can, you know, it's easier to
 

2245
00:50:36,880 --> 00:50:39,829
because I can, you know, it's easier to
take the gradients through anything now.

2246
00:50:39,829 --> 00:50:39,839
take the gradients through anything now.
 

2247
00:50:39,839 --> 00:50:42,150
take the gradients through anything now.
I can do whatever I want. roll out as

2248
00:50:42,150 --> 00:50:42,160
I can do whatever I want. roll out as
 

2249
00:50:42,160 --> 00:50:43,829
I can do whatever I want. roll out as
far as I want, do whatever I want, find

2250
00:50:43,829 --> 00:50:43,839
far as I want, do whatever I want, find
 

2251
00:50:43,839 --> 00:50:45,829
far as I want, do whatever I want, find
a final spot, and look at what direction

2252
00:50:45,829 --> 00:50:45,839
a final spot, and look at what direction
 

2253
00:50:45,839 --> 00:50:47,670
a final spot, and look at what direction
that was pointing me in and just move a

2254
00:50:47,670 --> 00:50:47,680
that was pointing me in and just move a
 

2255
00:50:47,680 --> 00:50:49,670
that was pointing me in and just move a
little bit in that direction. And the

2256
00:50:49,670 --> 00:50:49,680
little bit in that direction. And the
 

2257
00:50:49,680 --> 00:50:52,950
little bit in that direction. And the
idea is that every task,

2258
00:50:52,950 --> 00:50:52,960
idea is that every task,
 

2259
00:50:52,960 --> 00:50:54,950
idea is that every task,
the intuition is every task has two

2260
00:50:54,950 --> 00:50:54,960
the intuition is every task has two
 

2261
00:50:54,960 --> 00:50:56,710
the intuition is every task has two
things it might want. It might want to

2262
00:50:56,710 --> 00:50:56,720
things it might want. It might want to
 

2263
00:50:56,720 --> 00:50:59,030
things it might want. It might want to
move to a better thing for this task,

2264
00:50:59,030 --> 00:50:59,040
move to a better thing for this task,
 

2265
00:50:59,040 --> 00:51:01,910
move to a better thing for this task,
but also a better general position. So

2266
00:51:01,910 --> 00:51:01,920
but also a better general position. So
 

2267
00:51:01,920 --> 00:51:03,589
but also a better general position. So
by taking little steps, I'm going to

2268
00:51:03,589 --> 00:51:03,599
by taking little steps, I'm going to
 

2269
00:51:03,599 --> 00:51:05,190
by taking little steps, I'm going to
average out the things that are task

2270
00:51:05,190 --> 00:51:05,200
average out the things that are task
 

2271
00:51:05,200 --> 00:51:07,030
average out the things that are task
specific and just get a drift in the

2272
00:51:07,030 --> 00:51:07,040
specific and just get a drift in the
 

2273
00:51:07,040 --> 00:51:10,630
specific and just get a drift in the
direction of generally better.

2274
00:51:10,630 --> 00:51:10,640
direction of generally better.
 

2275
00:51:10,640 --> 00:51:12,069
direction of generally better.
and that'll move me to a better initial

2276
00:51:12,069 --> 00:51:12,079
and that'll move me to a better initial
 

2277
00:51:12,079 --> 00:51:15,589
and that'll move me to a better initial
condition. So, reptile

2278
00:51:15,589 --> 00:51:15,599
condition. So, reptile
 

2279
00:51:15,599 --> 00:51:18,390
condition. So, reptile
worked pretty well and in many cases it

2280
00:51:18,390 --> 00:51:18,400
worked pretty well and in many cases it
 

2281
00:51:18,400 --> 00:51:21,030
worked pretty well and in many cases it
would work as well as mammal um even

2282
00:51:21,030 --> 00:51:21,040
would work as well as mammal um even
 

2283
00:51:21,040 --> 00:51:22,870
would work as well as mammal um even
with limited rollouts but with extended

2284
00:51:22,870 --> 00:51:22,880
with limited rollouts but with extended
 

2285
00:51:22,880 --> 00:51:25,829
with limited rollouts but with extended
rollouts would work even better. So,

2286
00:51:25,829 --> 00:51:25,839
rollouts would work even better. So,
 

2287
00:51:25,839 --> 00:51:27,510
rollouts would work even better. So,
that's an approach that one can do.

2288
00:51:27,510 --> 00:51:27,520
that's an approach that one can do.
 

2289
00:51:27,520 --> 00:51:28,309
that's an approach that one can do.
Yeah,

2290
00:51:28,309 --> 00:51:28,319
Yeah,
 

2291
00:51:28,319 --> 00:51:30,870
Yeah,
>> I don't see how in reptile how the final

2292
00:51:30,870 --> 00:51:30,880
>> I don't see how in reptile how the final
 

2293
00:51:30,880 --> 00:51:33,349
>> I don't see how in reptile how the final
loss factors into that or does it at

2294
00:51:33,349 --> 00:51:33,359
loss factors into that or does it at
 

2295
00:51:33,359 --> 00:51:33,750
loss factors into that or does it at
all?

2296
00:51:33,750 --> 00:51:33,760
all?
 

2297
00:51:33,760 --> 00:51:35,910
all?
>> Ah, great question. Yes, in reptile you

2298
00:51:35,910 --> 00:51:35,920
>> Ah, great question. Yes, in reptile you
 

2299
00:51:35,920 --> 00:51:40,150
>> Ah, great question. Yes, in reptile you
ignore the final loss.

2300
00:51:40,150 --> 00:51:40,160

 

2301
00:51:40,160 --> 00:51:42,150

So then in that sense why not just like

2302
00:51:42,150 --> 00:51:42,160
So then in that sense why not just like
 

2303
00:51:42,160 --> 00:51:43,750
So then in that sense why not just like
why are you looking at the final why not

2304
00:51:43,750 --> 00:51:43,760
why are you looking at the final why not
 

2305
00:51:43,760 --> 00:51:46,549
why are you looking at the final why not
look at any of the intermediate like and

2306
00:51:46,549 --> 00:51:46,559
look at any of the intermediate like and
 

2307
00:51:46,559 --> 00:51:48,870
look at any of the intermediate like and
any of the intermediate steps also.

2308
00:51:48,870 --> 00:51:48,880
any of the intermediate steps also.
 

2309
00:51:48,880 --> 00:51:50,790
any of the intermediate steps also.
>> Uh great question. So the question was

2310
00:51:50,790 --> 00:51:50,800
>> Uh great question. So the question was
 

2311
00:51:50,800 --> 00:51:52,230
>> Uh great question. So the question was
why not look at any of the intermediate

2312
00:51:52,230 --> 00:51:52,240
why not look at any of the intermediate
 

2313
00:51:52,240 --> 00:51:54,069
why not look at any of the intermediate
steps and the idea is you look at the

2314
00:51:54,069 --> 00:51:54,079
steps and the idea is you look at the
 

2315
00:51:54,079 --> 00:51:55,589
steps and the idea is you look at the
final one because you expect there to be

2316
00:51:55,589 --> 00:51:55,599
final one because you expect there to be
 

2317
00:51:55,599 --> 00:51:57,510
final one because you expect there to be
some meandering from the randomness in

2318
00:51:57,510 --> 00:51:57,520
some meandering from the randomness in
 

2319
00:51:57,520 --> 00:51:59,190
some meandering from the randomness in
the batch and it's kind of all averaging

2320
00:51:59,190 --> 00:51:59,200
the batch and it's kind of all averaging
 

2321
00:51:59,200 --> 00:52:00,630
the batch and it's kind of all averaging
together. So you might as well look at

2322
00:52:00,630 --> 00:52:00,640
together. So you might as well look at
 

2323
00:52:00,640 --> 00:52:02,870
together. So you might as well look at
the final one as the net result of all

2324
00:52:02,870 --> 00:52:02,880
the final one as the net result of all
 

2325
00:52:02,880 --> 00:52:04,870
the final one as the net result of all
of that. And if you wanted to do

2326
00:52:04,870 --> 00:52:04,880
of that. And if you wanted to do
 

2327
00:52:04,880 --> 00:52:05,990
of that. And if you wanted to do
something different about that

2328
00:52:05,990 --> 00:52:06,000
something different about that
 

2329
00:52:06,000 --> 00:52:08,230
something different about that
averaging, the reptile approach would be

2330
00:52:08,230 --> 00:52:08,240
averaging, the reptile approach would be
 

2331
00:52:08,240 --> 00:52:09,829
averaging, the reptile approach would be
build it into your optimizer for inner

2332
00:52:09,829 --> 00:52:09,839
build it into your optimizer for inner
 

2333
00:52:09,839 --> 00:52:11,910
build it into your optimizer for inner
task.

2334
00:52:11,910 --> 00:52:11,920
task.
 

2335
00:52:11,920 --> 00:52:14,710
task.
See where you want to end up and just

2336
00:52:14,710 --> 00:52:14,720
See where you want to end up and just
 

2337
00:52:14,720 --> 00:52:16,390
See where you want to end up and just
use a small step in that direction to

2338
00:52:16,390 --> 00:52:16,400
use a small step in that direction to
 

2339
00:52:16,400 --> 00:52:21,990
use a small step in that direction to
approximate a better initial condition.

2340
00:52:21,990 --> 00:52:22,000

 

2341
00:52:22,000 --> 00:52:24,790

Okay. Yeah, great great points.

2342
00:52:24,790 --> 00:52:24,800
Okay. Yeah, great great points.
 

2343
00:52:24,800 --> 00:52:28,470
Okay. Yeah, great great points.
So this is very interesting. And now

2344
00:52:28,470 --> 00:52:28,480
So this is very interesting. And now
 

2345
00:52:28,480 --> 00:52:30,950
So this is very interesting. And now
another approach all everything I'm

2346
00:52:30,950 --> 00:52:30,960
another approach all everything I'm
 

2347
00:52:30,960 --> 00:52:32,790
another approach all everything I'm
doing with reptile and mammal was all

2348
00:52:32,790 --> 00:52:32,800
doing with reptile and mammal was all
 

2349
00:52:32,800 --> 00:52:35,030
doing with reptile and mammal was all
about uh the case where we're going to

2350
00:52:35,030 --> 00:52:35,040
about uh the case where we're going to
 

2351
00:52:35,040 --> 00:52:36,950
about uh the case where we're going to
do a full fine-tune.

2352
00:52:36,950 --> 00:52:36,960
do a full fine-tune.
 

2353
00:52:36,960 --> 00:52:39,109
do a full fine-tune.
So the natural question is if you wanted

2354
00:52:39,109 --> 00:52:39,119
So the natural question is if you wanted
 

2355
00:52:39,119 --> 00:52:42,950
So the natural question is if you wanted
to optimize a model using this kind of

2356
00:52:42,950 --> 00:52:42,960
to optimize a model using this kind of
 

2357
00:52:42,960 --> 00:52:46,470
to optimize a model using this kind of
thinking except not for a full fine tune

2358
00:52:46,470 --> 00:52:46,480
thinking except not for a full fine tune
 

2359
00:52:46,480 --> 00:52:50,150
thinking except not for a full fine tune
but just for being a good embedder.

2360
00:52:50,150 --> 00:52:50,160
but just for being a good embedder.
 

2361
00:52:50,160 --> 00:52:52,470
but just for being a good embedder.
How would you do it? Does everyone see

2362
00:52:52,470 --> 00:52:52,480
How would you do it? Does everyone see
 

2363
00:52:52,480 --> 00:52:54,549
How would you do it? Does everyone see
this? It's like just I want the same

2364
00:52:54,549 --> 00:52:54,559
this? It's like just I want the same
 

2365
00:52:54,559 --> 00:52:56,870
this? It's like just I want the same
approach of metalarning except not for

2366
00:52:56,870 --> 00:52:56,880
approach of metalarning except not for
 

2367
00:52:56,880 --> 00:53:00,790
approach of metalarning except not for
full fine tunes but for linear probe

2368
00:53:00,790 --> 00:53:00,800
full fine tunes but for linear probe
 

2369
00:53:00,800 --> 00:53:03,829
full fine tunes but for linear probe
and this is a a family of papers called

2370
00:53:03,829 --> 00:53:03,839
and this is a a family of papers called
 

2371
00:53:03,839 --> 00:53:07,910
and this is a a family of papers called
u anil meta optnet and r2d2 I don't I

2372
00:53:07,910 --> 00:53:07,920
u anil meta optnet and r2d2 I don't I
 

2373
00:53:07,920 --> 00:53:09,510
u anil meta optnet and r2d2 I don't I
forgot what these things stand for which

2374
00:53:09,510 --> 00:53:09,520
forgot what these things stand for which
 

2375
00:53:09,520 --> 00:53:11,910
forgot what these things stand for which
is basically optimizing for this thing

2376
00:53:11,910 --> 00:53:11,920
is basically optimizing for this thing
 

2377
00:53:11,920 --> 00:53:14,069
is basically optimizing for this thing
but here's the core idea and the papers

2378
00:53:14,069 --> 00:53:14,079
but here's the core idea and the papers
 

2379
00:53:14,079 --> 00:53:15,430
but here's the core idea and the papers
are more complicated but the core idea

2380
00:53:15,430 --> 00:53:15,440
are more complicated but the core idea
 

2381
00:53:15,440 --> 00:53:17,990
are more complicated but the core idea
is pretty simple

2382
00:53:17,990 --> 00:53:18,000
is pretty simple
 

2383
00:53:18,000 --> 00:53:21,109
is pretty simple
core idea says look

2384
00:53:21,109 --> 00:53:21,119
core idea says look
 

2385
00:53:21,119 --> 00:53:23,190
core idea says look
this is the thing that we're going to be

2386
00:53:23,190 --> 00:53:23,200
this is the thing that we're going to be
 

2387
00:53:23,200 --> 00:53:26,069
this is the thing that we're going to be
doing. Okay,

2388
00:53:26,069 --> 00:53:26,079
doing. Okay,
 

2389
00:53:26,079 --> 00:53:29,030
doing. Okay,
I'm putting both in green because we're

2390
00:53:29,030 --> 00:53:29,040
I'm putting both in green because we're
 

2391
00:53:29,040 --> 00:53:31,589
I'm putting both in green because we're
we want to learn new weights for this in

2392
00:53:31,589 --> 00:53:31,599
we want to learn new weights for this in
 

2393
00:53:31,599 --> 00:53:33,829
we want to learn new weights for this in
our metalarning. So metalarning, we want

2394
00:53:33,829 --> 00:53:33,839
our metalarning. So metalarning, we want
 

2395
00:53:33,839 --> 00:53:35,030
our metalarning. So metalarning, we want
to learn new weights for this is our

2396
00:53:35,030 --> 00:53:35,040
to learn new weights for this is our
 

2397
00:53:35,040 --> 00:53:36,710
to learn new weights for this is our
goal.

2398
00:53:36,710 --> 00:53:36,720
goal.
 

2399
00:53:36,720 --> 00:53:39,030
goal.
But how do we do it? So to understand

2400
00:53:39,030 --> 00:53:39,040
But how do we do it? So to understand
 

2401
00:53:39,040 --> 00:53:40,390
But how do we do it? So to understand
it, it's easiest to think about

2402
00:53:40,390 --> 00:53:40,400
it, it's easiest to think about
 

2403
00:53:40,400 --> 00:53:42,470
it, it's easiest to think about
regression problems. Okay, so let's

2404
00:53:42,470 --> 00:53:42,480
regression problems. Okay, so let's
 

2405
00:53:42,480 --> 00:53:44,549
regression problems. Okay, so let's
think about regression problems first.

2406
00:53:44,549 --> 00:53:44,559
think about regression problems first.
 

2407
00:53:44,559 --> 00:53:47,670
think about regression problems first.
So if we're doing a regression problem

2408
00:53:47,670 --> 00:53:47,680
So if we're doing a regression problem
 

2409
00:53:47,680 --> 00:53:51,349
So if we're doing a regression problem
with le squares or ridge regression, we

2410
00:53:51,349 --> 00:53:51,359
with le squares or ridge regression, we
 

2411
00:53:51,359 --> 00:53:53,670
with le squares or ridge regression, we
actually have a closed form formula for

2412
00:53:53,670 --> 00:53:53,680
actually have a closed form formula for
 

2413
00:53:53,680 --> 00:53:57,190
actually have a closed form formula for
the head.

2414
00:53:57,190 --> 00:53:57,200

 

2415
00:53:57,200 --> 00:53:59,750

Everyone remember we just like this is a

2416
00:53:59,750 --> 00:53:59,760
Everyone remember we just like this is a
 

2417
00:53:59,760 --> 00:54:02,390
Everyone remember we just like this is a
frozen feature extractor. It's just

2418
00:54:02,390 --> 00:54:02,400
frozen feature extractor. It's just
 

2419
00:54:02,400 --> 00:54:04,549
frozen feature extractor. It's just
vectors. We can then run those vectors

2420
00:54:04,549 --> 00:54:04,559
vectors. We can then run those vectors
 

2421
00:54:04,559 --> 00:54:06,390
vectors. We can then run those vectors
into a closed form formula to get the

2422
00:54:06,390 --> 00:54:06,400
into a closed form formula to get the
 

2423
00:54:06,400 --> 00:54:09,349
into a closed form formula to get the
head.

2424
00:54:09,349 --> 00:54:09,359
head.
 

2425
00:54:09,359 --> 00:54:12,230
head.
Any questions on that?

2426
00:54:12,230 --> 00:54:12,240
Any questions on that?
 

2427
00:54:12,240 --> 00:54:15,190
Any questions on that?
Great. that closed form formula is

2428
00:54:15,190 --> 00:54:15,200
Great. that closed form formula is
 

2429
00:54:15,200 --> 00:54:19,990
Great. that closed form formula is
differentiable,

2430
00:54:19,990 --> 00:54:20,000

 

2431
00:54:20,000 --> 00:54:21,030

right? There's nothing in that closed

2432
00:54:21,030 --> 00:54:21,040
right? There's nothing in that closed
 

2433
00:54:21,040 --> 00:54:23,270
right? There's nothing in that closed
form formula that's not differentiable.

2434
00:54:23,270 --> 00:54:23,280
form formula that's not differentiable.
 

2435
00:54:23,280 --> 00:54:24,790
form formula that's not differentiable.
So

2436
00:54:24,790 --> 00:54:24,800
So
 

2437
00:54:24,800 --> 00:54:27,990
So
you can just take a step, you take the

2438
00:54:27,990 --> 00:54:28,000
you can just take a step, you take the
 

2439
00:54:28,000 --> 00:54:29,510
you can just take a step, you take the
task, but instead of doing a whole bunch

2440
00:54:29,510 --> 00:54:29,520
task, but instead of doing a whole bunch
 

2441
00:54:29,520 --> 00:54:33,349
task, but instead of doing a whole bunch
of gradient steps, you just

2442
00:54:33,349 --> 00:54:33,359
of gradient steps, you just
 

2443
00:54:33,359 --> 00:54:40,470
of gradient steps, you just
do one formula. So everything up here,

2444
00:54:40,470 --> 00:54:40,480

 

2445
00:54:40,480 --> 00:54:42,790

you don't have to do gradient descent.

2446
00:54:42,790 --> 00:54:42,800
you don't have to do gradient descent.
 

2447
00:54:42,800 --> 00:54:46,870
you don't have to do gradient descent.
You just collect the data, do a formula.

2448
00:54:46,870 --> 00:54:46,880
You just collect the data, do a formula.
 

2449
00:54:46,880 --> 00:54:49,190
You just collect the data, do a formula.
That formula will then push gradients

2450
00:54:49,190 --> 00:54:49,200
That formula will then push gradients
 

2451
00:54:49,200 --> 00:54:53,670
That formula will then push gradients
back on its inputs.

2452
00:54:53,670 --> 00:54:53,680

 

2453
00:54:53,680 --> 00:54:57,349

Okay, having gradients on its inputs

2454
00:54:57,349 --> 00:54:57,359
Okay, having gradients on its inputs
 

2455
00:54:57,359 --> 00:55:02,630
Okay, having gradients on its inputs
means you get gradients on this

2456
00:55:02,630 --> 00:55:02,640

 

2457
00:55:02,640 --> 00:55:08,630

and you can take a step through that.

2458
00:55:08,630 --> 00:55:08,640

 

2459
00:55:08,640 --> 00:55:11,829

So

2460
00:55:11,829 --> 00:55:11,839

 

2461
00:55:11,839 --> 00:55:13,190

this works for regression problems

2462
00:55:13,190 --> 00:55:13,200
this works for regression problems
 

2463
00:55:13,200 --> 00:55:15,589
this works for regression problems
because you have a closed form formula.

2464
00:55:15,589 --> 00:55:15,599
because you have a closed form formula.
 

2465
00:55:15,599 --> 00:55:17,990
because you have a closed form formula.
Everyone good with that?

2466
00:55:17,990 --> 00:55:18,000
Everyone good with that?
 

2467
00:55:18,000 --> 00:55:21,349
Everyone good with that?
What about classification problems?

2468
00:55:21,349 --> 00:55:21,359
What about classification problems?
 

2469
00:55:21,359 --> 00:55:24,230
What about classification problems?
The key insight behind this approach is

2470
00:55:24,230 --> 00:55:24,240
The key insight behind this approach is
 

2471
00:55:24,240 --> 00:55:27,670
The key insight behind this approach is
to say that anytimes anytime your

2472
00:55:27,670 --> 00:55:27,680
to say that anytimes anytime your
 

2473
00:55:27,680 --> 00:55:29,829
to say that anytimes anytime your
problem is such that you can use a

2474
00:55:29,829 --> 00:55:29,839
problem is such that you can use a
 

2475
00:55:29,839 --> 00:55:32,390
problem is such that you can use a
convex optimizer,

2476
00:55:32,390 --> 00:55:32,400
convex optimizer,
 

2477
00:55:32,400 --> 00:55:35,109
convex optimizer,
you can do all kinds of tricks with that

2478
00:55:35,109 --> 00:55:35,119
you can do all kinds of tricks with that
 

2479
00:55:35,119 --> 00:55:37,430
you can do all kinds of tricks with that
convex optimizer to make it run pretty

2480
00:55:37,430 --> 00:55:37,440
convex optimizer to make it run pretty
 

2481
00:55:37,440 --> 00:55:39,510
convex optimizer to make it run pretty
fast. For example, you can do things

2482
00:55:39,510 --> 00:55:39,520
fast. For example, you can do things
 

2483
00:55:39,520 --> 00:55:42,950
fast. For example, you can do things
like Newton's method and so on on it. By

2484
00:55:42,950 --> 00:55:42,960
like Newton's method and so on on it. By
 

2485
00:55:42,960 --> 00:55:45,589
like Newton's method and so on on it. By
doing that, you can take a few steps in

2486
00:55:45,589 --> 00:55:45,599
doing that, you can take a few steps in
 

2487
00:55:45,599 --> 00:55:47,190
doing that, you can take a few steps in
your convex optimizer to get the

2488
00:55:47,190 --> 00:55:47,200
your convex optimizer to get the
 

2489
00:55:47,200 --> 00:55:48,309
your convex optimizer to get the
solution. That's why context problems

2490
00:55:48,309 --> 00:55:48,319
solution. That's why context problems
 

2491
00:55:48,319 --> 00:55:50,069
solution. That's why context problems
are so much faster to you do practically

2492
00:55:50,069 --> 00:55:50,079
are so much faster to you do practically
 

2493
00:55:50,079 --> 00:55:52,470
are so much faster to you do practically
than training a deep network, right? You

2494
00:55:52,470 --> 00:55:52,480
than training a deep network, right? You
 

2495
00:55:52,480 --> 00:55:56,870
than training a deep network, right? You
just have more stuff you can do and many

2496
00:55:56,870 --> 00:55:56,880
just have more stuff you can do and many
 

2497
00:55:56,880 --> 00:55:59,670
just have more stuff you can do and many
classification problems like SPMs,

2498
00:55:59,670 --> 00:55:59,680
classification problems like SPMs,
 

2499
00:55:59,680 --> 00:56:02,870
classification problems like SPMs,
right? In 127 you teach about SPMs,

2500
00:56:02,870 --> 00:56:02,880
right? In 127 you teach about SPMs,
 

2501
00:56:02,880 --> 00:56:04,710
right? In 127 you teach about SPMs,
right? So in 127 you saw how you can

2502
00:56:04,710 --> 00:56:04,720
right? So in 127 you saw how you can
 

2503
00:56:04,720 --> 00:56:07,829
right? So in 127 you saw how you can
cast an SPM as a a convex problem and

2504
00:56:07,829 --> 00:56:07,839
cast an SPM as a a convex problem and
 

2505
00:56:07,839 --> 00:56:10,150
cast an SPM as a a convex problem and
can solve it. So it turns out those

2506
00:56:10,150 --> 00:56:10,160
can solve it. So it turns out those
 

2507
00:56:10,160 --> 00:56:12,710
can solve it. So it turns out those
steps are also differentiable and you

2508
00:56:12,710 --> 00:56:12,720
steps are also differentiable and you
 

2509
00:56:12,720 --> 00:56:16,069
steps are also differentiable and you
can get for classification type problems

2510
00:56:16,069 --> 00:56:16,079
can get for classification type problems
 

2511
00:56:16,079 --> 00:56:17,910
can get for classification type problems
a counterpart to this differentiating

2512
00:56:17,910 --> 00:56:17,920
a counterpart to this differentiating
 

2513
00:56:17,920 --> 00:56:19,670
a counterpart to this differentiating
through this closed form formula that's

2514
00:56:19,670 --> 00:56:19,680
through this closed form formula that's
 

2515
00:56:19,680 --> 00:56:22,950
through this closed form formula that's
fast

2516
00:56:22,950 --> 00:56:22,960

 

2517
00:56:22,960 --> 00:56:24,390

and doesn't have this instability

2518
00:56:24,390 --> 00:56:24,400
and doesn't have this instability
 

2519
00:56:24,400 --> 00:56:25,430
and doesn't have this instability
problem. Yeah,

2520
00:56:25,430 --> 00:56:25,440
problem. Yeah,
 

2521
00:56:25,440 --> 00:56:26,950
problem. Yeah,
>> I think I'm a little confused because I

2522
00:56:26,950 --> 00:56:26,960
>> I think I'm a little confused because I
 

2523
00:56:26,960 --> 00:56:29,670
>> I think I'm a little confused because I
thought for linear probing like the the

2524
00:56:29,670 --> 00:56:29,680
thought for linear probing like the the
 

2525
00:56:29,680 --> 00:56:32,390
thought for linear probing like the the
parameters of the model constant.

2526
00:56:32,390 --> 00:56:32,400
parameters of the model constant.
 

2527
00:56:32,400 --> 00:56:35,510
parameters of the model constant.
>> Okay, excellent. So in linear probing

2528
00:56:35,510 --> 00:56:35,520
>> Okay, excellent. So in linear probing
 

2529
00:56:35,520 --> 00:56:37,990
>> Okay, excellent. So in linear probing
the parameters of this thing stay

2530
00:56:37,990 --> 00:56:38,000
the parameters of this thing stay
 

2531
00:56:38,000 --> 00:56:41,510
the parameters of this thing stay
constant. Our job is to make a good

2532
00:56:41,510 --> 00:56:41,520
constant. Our job is to make a good
 

2533
00:56:41,520 --> 00:56:45,109
constant. Our job is to make a good
model for linear probing. So now we want

2534
00:56:45,109 --> 00:56:45,119
model for linear probing. So now we want
 

2535
00:56:45,119 --> 00:56:47,910
model for linear probing. So now we want
to learn what to do here such that it is

2536
00:56:47,910 --> 00:56:47,920
to learn what to do here such that it is
 

2537
00:56:47,920 --> 00:56:50,069
to learn what to do here such that it is
good for linear probing on a variety of

2538
00:56:50,069 --> 00:56:50,079
good for linear probing on a variety of
 

2539
00:56:50,079 --> 00:56:53,030
good for linear probing on a variety of
tasks.

2540
00:56:53,030 --> 00:56:53,040
tasks.
 

2541
00:56:53,040 --> 00:56:54,789
tasks.
And because we're doing linear probing,

2542
00:56:54,789 --> 00:56:54,799
And because we're doing linear probing,
 

2543
00:56:54,799 --> 00:56:56,870
And because we're doing linear probing,
because we're doing linear probing,

2544
00:56:56,870 --> 00:56:56,880
because we're doing linear probing,
 

2545
00:56:56,880 --> 00:56:59,430
because we're doing linear probing,
those linear things can be optimized

2546
00:56:59,430 --> 00:56:59,440
those linear things can be optimized
 

2547
00:56:59,440 --> 00:57:01,510
those linear things can be optimized
taking advantage of their convexity of

2548
00:57:01,510 --> 00:57:01,520
taking advantage of their convexity of
 

2549
00:57:01,520 --> 00:57:02,710
taking advantage of their convexity of
the classical machine learning

2550
00:57:02,710 --> 00:57:02,720
the classical machine learning
 

2551
00:57:02,720 --> 00:57:05,430
the classical machine learning
algorithms.

2552
00:57:05,430 --> 00:57:05,440
algorithms.
 

2553
00:57:05,440 --> 00:57:07,190
algorithms.
Okay? And specifically doing things

2554
00:57:07,190 --> 00:57:07,200
Okay? And specifically doing things
 

2555
00:57:07,200 --> 00:57:09,829
Okay? And specifically doing things
like, oh, I'll take a few Newton steps

2556
00:57:09,829 --> 00:57:09,839
like, oh, I'll take a few Newton steps
 

2557
00:57:09,839 --> 00:57:11,990
like, oh, I'll take a few Newton steps
to solve it. Has have any of you played

2558
00:57:11,990 --> 00:57:12,000
to solve it. Has have any of you played
 

2559
00:57:12,000 --> 00:57:13,910
to solve it. Has have any of you played
with this where you try to do like

2560
00:57:13,910 --> 00:57:13,920
with this where you try to do like
 

2561
00:57:13,920 --> 00:57:15,829
with this where you try to do like
logistic regression, solving logistic

2562
00:57:15,829 --> 00:57:15,839
logistic regression, solving logistic
 

2563
00:57:15,839 --> 00:57:18,470
logistic regression, solving logistic
regression with a few Newton steps? Did

2564
00:57:18,470 --> 00:57:18,480
regression with a few Newton steps? Did
 

2565
00:57:18,480 --> 00:57:20,230
regression with a few Newton steps? Did
you guys have to do this in 189? I don't

2566
00:57:20,230 --> 00:57:20,240
you guys have to do this in 189? I don't
 

2567
00:57:20,240 --> 00:57:21,670
you guys have to do this in 189? I don't
know if you were made to do it. do a do

2568
00:57:21,670 --> 00:57:21,680
know if you were made to do it. do a do
 

2569
00:57:21,680 --> 00:57:23,030
know if you were made to do it. do a do
a few Newton steps for a logistic

2570
00:57:23,030 --> 00:57:23,040
a few Newton steps for a logistic
 

2571
00:57:23,040 --> 00:57:24,870
a few Newton steps for a logistic
regression. See how fast it converges

2572
00:57:24,870 --> 00:57:24,880
regression. See how fast it converges
 

2573
00:57:24,880 --> 00:57:26,710
regression. See how fast it converges
relative to gradient descent. Have you

2574
00:57:26,710 --> 00:57:26,720
relative to gradient descent. Have you
 

2575
00:57:26,720 --> 00:57:27,910
relative to gradient descent. Have you
done any of these comparisons of

2576
00:57:27,910 --> 00:57:27,920
done any of these comparisons of
 

2577
00:57:27,920 --> 00:57:29,670
done any of these comparisons of
something smarter relative to gradient

2578
00:57:29,670 --> 00:57:29,680
something smarter relative to gradient
 

2579
00:57:29,680 --> 00:57:31,270
something smarter relative to gradient
descent and it works much faster in a

2580
00:57:31,270 --> 00:57:31,280
descent and it works much faster in a
 

2581
00:57:31,280 --> 00:57:35,510
descent and it works much faster in a
convex problem? That okay that's what we

2582
00:57:35,510 --> 00:57:35,520
convex problem? That okay that's what we
 

2583
00:57:35,520 --> 00:57:37,990
convex problem? That okay that's what we
can exploit here. So this is another

2584
00:57:37,990 --> 00:57:38,000
can exploit here. So this is another
 

2585
00:57:38,000 --> 00:57:40,870
can exploit here. So this is another
approach uh to metalarning

2586
00:57:40,870 --> 00:57:40,880
approach uh to metalarning
 

2587
00:57:40,880 --> 00:57:42,710
approach uh to metalarning
that is for linear probes. So there's

2588
00:57:42,710 --> 00:57:42,720
that is for linear probes. So there's
 

2589
00:57:42,720 --> 00:57:44,870
that is for linear probes. So there's
lots of these things that people have

2590
00:57:44,870 --> 00:57:44,880
lots of these things that people have
 

2591
00:57:44,880 --> 00:57:46,549
lots of these things that people have
done

2592
00:57:46,549 --> 00:57:46,559
done
 

2593
00:57:46,559 --> 00:57:49,430
done
is the important thing about this is

2594
00:57:49,430 --> 00:57:49,440
is the important thing about this is
 

2595
00:57:49,440 --> 00:57:50,950
is the important thing about this is
more for your learning and

2596
00:57:50,950 --> 00:57:50,960
more for your learning and
 

2597
00:57:50,960 --> 00:57:52,390
more for your learning and
understanding. We'll give you a homework

2598
00:57:52,390 --> 00:57:52,400
understanding. We'll give you a homework
 

2599
00:57:52,400 --> 00:57:53,829
understanding. We'll give you a homework
problem on metalarning so you can watch

2600
00:57:53,829 --> 00:57:53,839
problem on metalarning so you can watch
 

2601
00:57:53,839 --> 00:57:57,430
problem on metalarning so you can watch
it work uh for a handcrafted example um

2602
00:57:57,430 --> 00:57:57,440
it work uh for a handcrafted example um
 

2603
00:57:57,440 --> 00:57:59,109
it work uh for a handcrafted example um
and you can see how it learns features

2604
00:57:59,109 --> 00:57:59,119
and you can see how it learns features
 

2605
00:57:59,119 --> 00:58:03,430
and you can see how it learns features
this way that are useful.

2606
00:58:03,430 --> 00:58:03,440

 

2607
00:58:03,440 --> 00:58:06,069

Okay. So yes.

2608
00:58:06,069 --> 00:58:06,079
Okay. So yes.
 

2609
00:58:06,079 --> 00:58:09,109
Okay. So yes.
So I guess so for the second item is

2610
00:58:09,109 --> 00:58:09,119
So I guess so for the second item is
 

2611
00:58:09,119 --> 00:58:11,270
So I guess so for the second item is
we're is that the idea that we're we're

2612
00:58:11,270 --> 00:58:11,280
we're is that the idea that we're we're
 

2613
00:58:11,280 --> 00:58:13,910
we're is that the idea that we're we're
just sort of

2614
00:58:13,910 --> 00:58:13,920
just sort of
 

2615
00:58:13,920 --> 00:58:15,910
just sort of
like are we just learning the head in

2616
00:58:15,910 --> 00:58:15,920
like are we just learning the head in
 

2617
00:58:15,920 --> 00:58:17,990
like are we just learning the head in
like a closed form fashion like more

2618
00:58:17,990 --> 00:58:18,000
like a closed form fashion like more
 

2619
00:58:18,000 --> 00:58:19,589
like a closed form fashion like more
efficiently

2620
00:58:19,589 --> 00:58:19,599
efficiently
 

2621
00:58:19,599 --> 00:58:21,190
efficiently
given that it's like a specific like

2622
00:58:21,190 --> 00:58:21,200
given that it's like a specific like
 

2623
00:58:21,200 --> 00:58:24,630
given that it's like a specific like
linear or like classifier or something

2624
00:58:24,630 --> 00:58:24,640
linear or like classifier or something
 

2625
00:58:24,640 --> 00:58:26,710
linear or like classifier or something
is what I what I'm missing is maybe like

2626
00:58:26,710 --> 00:58:26,720
is what I what I'm missing is maybe like
 

2627
00:58:26,720 --> 00:58:28,710
is what I what I'm missing is maybe like
the

2628
00:58:28,710 --> 00:58:28,720
the
 

2629
00:58:28,720 --> 00:58:30,150
the
where the the benefit is like coming

2630
00:58:30,150 --> 00:58:30,160
where the the benefit is like coming
 

2631
00:58:30,160 --> 00:58:32,150
where the the benefit is like coming
from over like is it just because the

2632
00:58:32,150 --> 00:58:32,160
from over like is it just because the
 

2633
00:58:32,160 --> 00:58:33,670
from over like is it just because the
gradients are kind of messy if you have

2634
00:58:33,670 --> 00:58:33,680
gradients are kind of messy if you have
 

2635
00:58:33,680 --> 00:58:35,910
gradients are kind of messy if you have
to pass through the whole model

2636
00:58:35,910 --> 00:58:35,920
to pass through the whole model
 

2637
00:58:35,920 --> 00:58:37,349
to pass through the whole model
Okay, so it's very important to

2638
00:58:37,349 --> 00:58:37,359
Okay, so it's very important to
 

2639
00:58:37,359 --> 00:58:38,870
Okay, so it's very important to
understand what we're trying to do.

2640
00:58:38,870 --> 00:58:38,880
understand what we're trying to do.
 

2641
00:58:38,880 --> 00:58:42,069
understand what we're trying to do.
Okay,

2642
00:58:42,069 --> 00:58:42,079

 

2643
00:58:42,079 --> 00:58:44,710

we want to make a model. Here's the

2644
00:58:44,710 --> 00:58:44,720
we want to make a model. Here's the
 

2645
00:58:44,720 --> 00:58:45,829
we want to make a model. Here's the
question. We're trying to make a model

2646
00:58:45,829 --> 00:58:45,839
question. We're trying to make a model
 

2647
00:58:45,839 --> 00:58:48,309
question. We're trying to make a model
that's good for linear probing.

2648
00:58:48,309 --> 00:58:48,319
that's good for linear probing.
 

2649
00:58:48,319 --> 00:58:49,510
that's good for linear probing.
So, we're going to have to adjust the

2650
00:58:49,510 --> 00:58:49,520
So, we're going to have to adjust the
 

2651
00:58:49,520 --> 00:58:52,390
So, we're going to have to adjust the
weights in here. That's a given. The

2652
00:58:52,390 --> 00:58:52,400
weights in here. That's a given. The
 

2653
00:58:52,400 --> 00:58:53,430
weights in here. That's a given. The
question is, how are we going to get

2654
00:58:53,430 --> 00:58:53,440
question is, how are we going to get
 

2655
00:58:53,440 --> 00:58:57,510
question is, how are we going to get
gradients to adjust this weight?

2656
00:58:57,510 --> 00:58:57,520

 

2657
00:58:57,520 --> 00:58:59,910

In the mammal approach, the way you got

2658
00:58:59,910 --> 00:58:59,920
In the mammal approach, the way you got
 

2659
00:58:59,920 --> 00:59:01,829
In the mammal approach, the way you got
those gradients was that you did full

2660
00:59:01,829 --> 00:59:01,839
those gradients was that you did full
 

2661
00:59:01,839 --> 00:59:04,069
those gradients was that you did full
fine-tunes on that, looked at that as an

2662
00:59:04,069 --> 00:59:04,079
fine-tunes on that, looked at that as an
 

2663
00:59:04,079 --> 00:59:05,910
fine-tunes on that, looked at that as an
RNN, and then passed gradients back

2664
00:59:05,910 --> 00:59:05,920
RNN, and then passed gradients back
 

2665
00:59:05,920 --> 00:59:08,309
RNN, and then passed gradients back
through that. In reptile, you did a full

2666
00:59:08,309 --> 00:59:08,319
through that. In reptile, you did a full
 

2667
00:59:08,319 --> 00:59:10,470
through that. In reptile, you did a full
fine-tune, you took gradient steps, and

2668
00:59:10,470 --> 00:59:10,480
fine-tune, you took gradient steps, and
 

2669
00:59:10,480 --> 00:59:13,510
fine-tune, you took gradient steps, and
you looked at the last point as the

2670
00:59:13,510 --> 00:59:13,520
you looked at the last point as the
 

2671
00:59:13,520 --> 00:59:15,910
you looked at the last point as the
default for where you would end up and

2672
00:59:15,910 --> 00:59:15,920
default for where you would end up and
 

2673
00:59:15,920 --> 00:59:17,349
default for where you would end up and
computed an approximate gradient and

2674
00:59:17,349 --> 00:59:17,359
computed an approximate gradient and
 

2675
00:59:17,359 --> 00:59:18,710
computed an approximate gradient and
then took a small step in that

2676
00:59:18,710 --> 00:59:18,720
then took a small step in that
 

2677
00:59:18,720 --> 00:59:21,750
then took a small step in that
direction. Here, we're not doing a full

2678
00:59:21,750 --> 00:59:21,760
direction. Here, we're not doing a full
 

2679
00:59:21,760 --> 00:59:24,069
direction. Here, we're not doing a full
fine tune. We're just the way we're

2680
00:59:24,069 --> 00:59:24,079
fine tune. We're just the way we're
 

2681
00:59:24,079 --> 00:59:25,829
fine tune. We're just the way we're
going to use it at test time is a linear

2682
00:59:25,829 --> 00:59:25,839
going to use it at test time is a linear
 

2683
00:59:25,839 --> 00:59:27,430
going to use it at test time is a linear
probe.

2684
00:59:27,430 --> 00:59:27,440
probe.
 

2685
00:59:27,440 --> 00:59:28,789
probe.
So because we're going to doing a doing

2686
00:59:28,789 --> 00:59:28,799
So because we're going to doing a doing
 

2687
00:59:28,799 --> 00:59:31,589
So because we're going to doing a doing
a linear probe at test time, we can just

2688
00:59:31,589 --> 00:59:31,599
a linear probe at test time, we can just
 

2689
00:59:31,599 --> 00:59:34,230
a linear probe at test time, we can just
solve the linear probe problem using

2690
00:59:34,230 --> 00:59:34,240
solve the linear probe problem using
 

2691
00:59:34,240 --> 00:59:37,270
solve the linear probe problem using
closed form at fake test time, which is

2692
00:59:37,270 --> 00:59:37,280
closed form at fake test time, which is
 

2693
00:59:37,280 --> 00:59:42,470
closed form at fake test time, which is
training time. So instead of doing

2694
00:59:42,470 --> 00:59:42,480

 

2695
00:59:42,480 --> 00:59:45,589

these steps here,

2696
00:59:45,589 --> 00:59:45,599
these steps here,
 

2697
00:59:45,599 --> 00:59:48,390
these steps here,
we just take the training data, run it

2698
00:59:48,390 --> 00:59:48,400
we just take the training data, run it
 

2699
00:59:48,400 --> 00:59:50,069
we just take the training data, run it
through our feature extractor in

2700
00:59:50,069 --> 00:59:50,079
through our feature extractor in
 

2701
00:59:50,079 --> 00:59:52,870
through our feature extractor in
parallel

2702
00:59:52,870 --> 00:59:52,880
parallel
 

2703
00:59:52,880 --> 00:59:59,349
parallel
and apply le squares.

2704
00:59:59,349 --> 00:59:59,359

 

2705
00:59:59,359 --> 01:00:01,270

So we don't have to do all of this.

2706
01:00:01,270 --> 01:00:01,280
So we don't have to do all of this.
 

2707
01:00:01,280 --> 01:00:03,190
So we don't have to do all of this.
There's no step by stepping. There's

2708
01:00:03,190 --> 01:00:03,200
There's no step by stepping. There's
 

2709
01:00:03,200 --> 01:00:05,510
There's no step by stepping. There's
just in parallel computation of features

2710
01:00:05,510 --> 01:00:05,520
just in parallel computation of features
 

2711
01:00:05,520 --> 01:00:07,030
just in parallel computation of features
and then the application of the le

2712
01:00:07,030 --> 01:00:07,040
and then the application of the le
 

2713
01:00:07,040 --> 01:00:09,030
and then the application of the le
squares formula.

2714
01:00:09,030 --> 01:00:09,040
squares formula.
 

2715
01:00:09,040 --> 01:00:11,109
squares formula.
>> So it's it's just a way to extract

2716
01:00:11,109 --> 01:00:11,119
>> So it's it's just a way to extract
 

2717
01:00:11,119 --> 01:00:13,109
>> So it's it's just a way to extract
gradients without having to actually

2718
01:00:13,109 --> 01:00:13,119
gradients without having to actually
 

2719
01:00:13,119 --> 01:00:16,950
gradients without having to actually
forward and backass the model itself.

2720
01:00:16,950 --> 01:00:16,960
forward and backass the model itself.
 

2721
01:00:16,960 --> 01:00:18,549
forward and backass the model itself.
>> We're going to have to forward and back

2722
01:00:18,549 --> 01:00:18,559
>> We're going to have to forward and back
 

2723
01:00:18,559 --> 01:00:23,109
>> We're going to have to forward and back
pass the model. We can get around that.

2724
01:00:23,109 --> 01:00:23,119
pass the model. We can get around that.
 

2725
01:00:23,119 --> 01:00:24,789
pass the model. We can get around that.
But what we're going to do is we're not

2726
01:00:24,789 --> 01:00:24,799
But what we're going to do is we're not
 

2727
01:00:24,799 --> 01:00:27,109
But what we're going to do is we're not
going to do a bunch of iterative updates

2728
01:00:27,109 --> 01:00:27,119
going to do a bunch of iterative updates
 

2729
01:00:27,119 --> 01:00:28,390
going to do a bunch of iterative updates
like this. We're going to get a gradient

2730
01:00:28,390 --> 01:00:28,400
like this. We're going to get a gradient
 

2731
01:00:28,400 --> 01:00:32,069
like this. We're going to get a gradient
for the entire task in one pump

2732
01:00:32,069 --> 01:00:32,079
for the entire task in one pump
 

2733
01:00:32,079 --> 01:00:34,549
for the entire task in one pump
because we'll we'll forward pass through

2734
01:00:34,549 --> 01:00:34,559
because we'll we'll forward pass through
 

2735
01:00:34,559 --> 01:00:36,470
because we'll we'll forward pass through
the model lots of forward passes through

2736
01:00:36,470 --> 01:00:36,480
the model lots of forward passes through
 

2737
01:00:36,480 --> 01:00:38,789
the model lots of forward passes through
the model in parallel. Now we have a

2738
01:00:38,789 --> 01:00:38,799
the model in parallel. Now we have a
 

2739
01:00:38,799 --> 01:00:40,549
the model in parallel. Now we have a
bunch of

2740
01:00:40,549 --> 01:00:40,559
bunch of
 

2741
01:00:40,559 --> 01:00:42,870
bunch of
numbers you know a bunch of vectors we

2742
01:00:42,870 --> 01:00:42,880
numbers you know a bunch of vectors we
 

2743
01:00:42,880 --> 01:00:45,349
numbers you know a bunch of vectors we
run them through le squares as a box. Le

2744
01:00:45,349 --> 01:00:45,359
run them through le squares as a box. Le
 

2745
01:00:45,359 --> 01:00:48,150
run them through le squares as a box. Le
squares is a box. Le squares will give a

2746
01:00:48,150 --> 01:00:48,160
squares is a box. Le squares will give a
 

2747
01:00:48,160 --> 01:00:50,390
squares is a box. Le squares will give a
prediction. That prediction we can you

2748
01:00:50,390 --> 01:00:50,400
prediction. That prediction we can you
 

2749
01:00:50,400 --> 01:00:53,430
prediction. That prediction we can you
run on held out data.

2750
01:00:53,430 --> 01:00:53,440
run on held out data.
 

2751
01:00:53,440 --> 01:00:55,109
run on held out data.
We can now run gradients back through

2752
01:00:55,109 --> 01:00:55,119
We can now run gradients back through
 

2753
01:00:55,119 --> 01:00:58,630
We can now run gradients back through
this model. So they go back through the

2754
01:00:58,630 --> 01:00:58,640
this model. So they go back through the
 

2755
01:00:58,640 --> 01:01:01,270
this model. So they go back through the
le squares flow to all of these parallel

2756
01:01:01,270 --> 01:01:01,280
le squares flow to all of these parallel
 

2757
01:01:01,280 --> 01:01:04,069
le squares flow to all of these parallel
instantiations of the model.

2758
01:01:04,069 --> 01:01:04,079
instantiations of the model.
 

2759
01:01:04,079 --> 01:01:05,510
instantiations of the model.
The key point is they can pass through

2760
01:01:05,510 --> 01:01:05,520
The key point is they can pass through
 

2761
01:01:05,520 --> 01:01:07,349
The key point is they can pass through
le squares

2762
01:01:07,349 --> 01:01:07,359
le squares
 

2763
01:01:07,359 --> 01:01:09,670
le squares
instead of having to go down an RNN. Le

2764
01:01:09,670 --> 01:01:09,680
instead of having to go down an RNN. Le
 

2765
01:01:09,680 --> 01:01:11,030
instead of having to go down an RNN. Le
squares is very stable with these

2766
01:01:11,030 --> 01:01:11,040
squares is very stable with these
 

2767
01:01:11,040 --> 01:01:13,109
squares is very stable with these
gradients. You can do it lot in

2768
01:01:13,109 --> 01:01:13,119
gradients. You can do it lot in
 

2769
01:01:13,119 --> 01:01:15,270
gradients. You can do it lot in
parallel. There's no recurrence.

2770
01:01:15,270 --> 01:01:15,280
parallel. There's no recurrence.
 

2771
01:01:15,280 --> 01:01:17,270
parallel. There's no recurrence.
So you can roll out as much as you want.

2772
01:01:17,270 --> 01:01:17,280
So you can roll out as much as you want.
 

2773
01:01:17,280 --> 01:01:19,510
So you can roll out as much as you want.
Run it through le squares pop out get

2774
01:01:19,510 --> 01:01:19,520
Run it through le squares pop out get
 

2775
01:01:19,520 --> 01:01:21,510
Run it through le squares pop out get
gradients to everything. compute now

2776
01:01:21,510 --> 01:01:21,520
gradients to everything. compute now
 

2777
01:01:21,520 --> 01:01:22,789
gradients to everything. compute now
gradients pass passing through these

2778
01:01:22,789 --> 01:01:22,799
gradients pass passing through these
 

2779
01:01:22,799 --> 01:01:25,349
gradients pass passing through these
weight the weight shared model and do an

2780
01:01:25,349 --> 01:01:25,359
weight the weight shared model and do an
 

2781
01:01:25,359 --> 01:01:27,030
weight the weight shared model and do an
update.

2782
01:01:27,030 --> 01:01:27,040
update.
 

2783
01:01:27,040 --> 01:01:30,150
update.
So it's just replacing

2784
01:01:30,150 --> 01:01:30,160
So it's just replacing
 

2785
01:01:30,160 --> 01:01:32,950
So it's just replacing
this step with something else. Instead

2786
01:01:32,950 --> 01:01:32,960
this step with something else. Instead
 

2787
01:01:32,960 --> 01:01:36,470
this step with something else. Instead
of doing this, you do le squares.

2788
01:01:36,470 --> 01:01:36,480
of doing this, you do le squares.
 

2789
01:01:36,480 --> 01:01:37,829
of doing this, you do le squares.
You still have to run gradients through

2790
01:01:37,829 --> 01:01:37,839
You still have to run gradients through
 

2791
01:01:37,839 --> 01:01:42,789
You still have to run gradients through
the model.

2792
01:01:42,789 --> 01:01:42,799

 

2793
01:01:42,799 --> 01:01:43,109

>> Yeah.

2794
01:01:43,109 --> 01:01:43,119
>> Yeah.
 

2795
01:01:43,119 --> 01:01:45,030
>> Yeah.
>> Yeah. So the main idea here is just to

2796
01:01:45,030 --> 01:01:45,040
>> Yeah. So the main idea here is just to
 

2797
01:01:45,040 --> 01:01:46,789
>> Yeah. So the main idea here is just to
make that inner learning loop more

2798
01:01:46,789 --> 01:01:46,799
make that inner learning loop more
 

2799
01:01:46,799 --> 01:01:49,109
make that inner learning loop more
compact and more efficient.

2800
01:01:49,109 --> 01:01:49,119
compact and more efficient.
 

2801
01:01:49,119 --> 01:01:50,630
compact and more efficient.
>> Yes. The inner learning loop is more

2802
01:01:50,630 --> 01:01:50,640
>> Yes. The inner learning loop is more
 

2803
01:01:50,640 --> 01:01:54,789
>> Yes. The inner learning loop is more
compact and efficient and stable.

2804
01:01:54,789 --> 01:01:54,799
compact and efficient and stable.
 

2805
01:01:54,799 --> 01:01:56,870
compact and efficient and stable.
It inherits all the nice very very nice

2806
01:01:56,870 --> 01:01:56,880
It inherits all the nice very very nice
 

2807
01:01:56,880 --> 01:02:00,309
It inherits all the nice very very nice
stability properties. So uh and again

2808
01:02:00,309 --> 01:02:00,319
stability properties. So uh and again
 

2809
01:02:00,319 --> 01:02:02,470
stability properties. So uh and again
you don't have a memory restriction. So

2810
01:02:02,470 --> 01:02:02,480
you don't have a memory restriction. So
 

2811
01:02:02,480 --> 01:02:04,710
you don't have a memory restriction. So
from a from a memory point of view uh

2812
01:02:04,710 --> 01:02:04,720
from a from a memory point of view uh
 

2813
01:02:04,720 --> 01:02:07,109
from a from a memory point of view uh
you have to be able to hold the features

2814
01:02:07,109 --> 01:02:07,119
you have to be able to hold the features
 

2815
01:02:07,119 --> 01:02:08,870
you have to be able to hold the features
but you can do remember we gave you this

2816
01:02:08,870 --> 01:02:08,880
but you can do remember we gave you this
 

2817
01:02:08,880 --> 01:02:12,230
but you can do remember we gave you this
problem on uh gradient checkpointing and

2818
01:02:12,230 --> 01:02:12,240
problem on uh gradient checkpointing and
 

2819
01:02:12,240 --> 01:02:13,589
problem on uh gradient checkpointing and
recomputation of gradients. Remember

2820
01:02:13,589 --> 01:02:13,599
recomputation of gradients. Remember
 

2821
01:02:13,599 --> 01:02:18,710
recomputation of gradients. Remember
that homework problem everyone?

2822
01:02:18,710 --> 01:02:18,720

 

2823
01:02:18,720 --> 01:02:20,549

So wait, do you remember this homework

2824
01:02:20,549 --> 01:02:20,559
So wait, do you remember this homework
 

2825
01:02:20,559 --> 01:02:21,670
So wait, do you remember this homework
problem? We gave you this homework

2826
01:02:21,670 --> 01:02:21,680
problem? We gave you this homework
 

2827
01:02:21,680 --> 01:02:24,230
problem? We gave you this homework
problem, right?

2828
01:02:24,230 --> 01:02:24,240
problem, right?
 

2829
01:02:24,240 --> 01:02:25,910
problem, right?
Where we said suppose you couldn't

2830
01:02:25,910 --> 01:02:25,920
Where we said suppose you couldn't
 

2831
01:02:25,920 --> 01:02:27,510
Where we said suppose you couldn't
actually store all the activations and

2832
01:02:27,510 --> 01:02:27,520
actually store all the activations and
 

2833
01:02:27,520 --> 01:02:29,430
actually store all the activations and
the gradients. What if you just

2834
01:02:29,430 --> 01:02:29,440
the gradients. What if you just
 

2835
01:02:29,440 --> 01:02:31,109
the gradients. What if you just
checkpointed the activations at every

2836
01:02:31,109 --> 01:02:31,119
checkpointed the activations at every
 

2837
01:02:31,119 --> 01:02:33,670
checkpointed the activations at every
fifth layer and then you had the

2838
01:02:33,670 --> 01:02:33,680
fifth layer and then you had the
 

2839
01:02:33,680 --> 01:02:35,430
fifth layer and then you had the
gradients at the end and then to backrop

2840
01:02:35,430 --> 01:02:35,440
gradients at the end and then to backrop
 

2841
01:02:35,440 --> 01:02:38,069
gradients at the end and then to backrop
it, you recomputee the activations that

2842
01:02:38,069 --> 01:02:38,079
it, you recomputee the activations that
 

2843
01:02:38,079 --> 01:02:40,230
it, you recomputee the activations that
you need and then you can backrop and

2844
01:02:40,230 --> 01:02:40,240
you need and then you can backrop and
 

2845
01:02:40,240 --> 01:02:41,910
you need and then you can backrop and
then you recomputee the activations.

2846
01:02:41,910 --> 01:02:41,920
then you recomputee the activations.
 

2847
01:02:41,920 --> 01:02:43,829
then you recomputee the activations.
Then you backrop then you recomputee the

2848
01:02:43,829 --> 01:02:43,839
Then you backrop then you recomputee the
 

2849
01:02:43,839 --> 01:02:45,270
Then you backrop then you recomputee the
activations and back prop. Remember that

2850
01:02:45,270 --> 01:02:45,280
activations and back prop. Remember that
 

2851
01:02:45,280 --> 01:02:50,950
activations and back prop. Remember that
homework problem that okay so here

2852
01:02:50,950 --> 01:02:50,960
homework problem that okay so here
 

2853
01:02:50,960 --> 01:02:53,430
homework problem that okay so here
you don't have to hold everything in

2854
01:02:53,430 --> 01:02:53,440
you don't have to hold everything in
 

2855
01:02:53,440 --> 01:02:56,470
you don't have to hold everything in
memory you can just

2856
01:02:56,470 --> 01:02:56,480
memory you can just
 

2857
01:02:56,480 --> 01:02:58,549
memory you can just
compute run forward one pass through

2858
01:02:58,549 --> 01:02:58,559
compute run forward one pass through
 

2859
01:02:58,559 --> 01:03:01,270
compute run forward one pass through
forward pass through your model

2860
01:03:01,270 --> 01:03:01,280
forward pass through your model
 

2861
01:03:01,280 --> 01:03:04,230
forward pass through your model
compute a bunch of features now run le

2862
01:03:04,230 --> 01:03:04,240
compute a bunch of features now run le
 

2863
01:03:04,240 --> 01:03:07,030
compute a bunch of features now run le
squares now back prop through that back

2864
01:03:07,030 --> 01:03:07,040
squares now back prop through that back
 

2865
01:03:07,040 --> 01:03:08,630
squares now back prop through that back
prop through le squares now you have

2866
01:03:08,630 --> 01:03:08,640
prop through le squares now you have
 

2867
01:03:08,640 --> 01:03:11,190
prop through le squares now you have
gradients on all of these

2868
01:03:11,190 --> 01:03:11,200
gradients on all of these
 

2869
01:03:11,200 --> 01:03:13,589
gradients on all of these
outputs of these models now rerun

2870
01:03:13,589 --> 01:03:13,599
outputs of these models now rerun
 

2871
01:03:13,599 --> 01:03:15,589
outputs of these models now rerun
forward passes back props individually

2872
01:03:15,589 --> 01:03:15,599
forward passes back props individually
 

2873
01:03:15,599 --> 01:03:19,270
forward passes back props individually
on the on that.

2874
01:03:19,270 --> 01:03:19,280

 

2875
01:03:19,280 --> 01:03:21,270

Okay, so it's very efficient and you

2876
01:03:21,270 --> 01:03:21,280
Okay, so it's very efficient and you
 

2877
01:03:21,280 --> 01:03:23,750
Okay, so it's very efficient and you
very nicely paralyzable. It just works.

2878
01:03:23,750 --> 01:03:23,760
very nicely paralyzable. It just works.
 

2879
01:03:23,760 --> 01:03:25,190
very nicely paralyzable. It just works.
You know, there's no exploding gradient

2880
01:03:25,190 --> 01:03:25,200
You know, there's no exploding gradient
 

2881
01:03:25,200 --> 01:03:27,510
You know, there's no exploding gradient
problems. It's just much nicer uh to

2882
01:03:27,510 --> 01:03:27,520
problems. It's just much nicer uh to
 

2883
01:03:27,520 --> 01:03:31,430
problems. It's just much nicer uh to
work with.

2884
01:03:31,430 --> 01:03:31,440

 

2885
01:03:31,440 --> 01:03:32,950

So for linear probes, you can do this

2886
01:03:32,950 --> 01:03:32,960
So for linear probes, you can do this
 

2887
01:03:32,960 --> 01:03:35,109
So for linear probes, you can do this
kind of stuff. And it's useful again to

2888
01:03:35,109 --> 01:03:35,119
kind of stuff. And it's useful again to
 

2889
01:03:35,119 --> 01:03:36,390
kind of stuff. And it's useful again to
keep these in your mind as you're

2890
01:03:36,390 --> 01:03:36,400
keep these in your mind as you're
 

2891
01:03:36,400 --> 01:03:38,069
keep these in your mind as you're
understanding things like in context

2892
01:03:38,069 --> 01:03:38,079
understanding things like in context
 

2893
01:03:38,079 --> 01:03:40,470
understanding things like in context
learning and stuff.

2894
01:03:40,470 --> 01:03:40,480
learning and stuff.
 

2895
01:03:40,480 --> 01:03:42,069
learning and stuff.
Okay, so that's all I wanted to say

2896
01:03:42,069 --> 01:03:42,079
Okay, so that's all I wanted to say
 

2897
01:03:42,079 --> 01:03:44,950
Okay, so that's all I wanted to say
about metalarning. I want to like move

2898
01:03:44,950 --> 01:03:44,960
about metalarning. I want to like move
 

2899
01:03:44,960 --> 01:03:47,910
about metalarning. I want to like move
on ahead to talk about a very important

2900
01:03:47,910 --> 01:03:47,920
on ahead to talk about a very important
 

2901
01:03:47,920 --> 01:03:49,829
on ahead to talk about a very important
concept. This is going to be talked

2902
01:03:49,829 --> 01:03:49,839
concept. This is going to be talked
 

2903
01:03:49,839 --> 01:03:51,910
concept. This is going to be talked
about more in discussion. So, but I want

2904
01:03:51,910 --> 01:03:51,920
about more in discussion. So, but I want
 

2905
01:03:51,920 --> 01:03:54,309
about more in discussion. So, but I want
to make sure you know about the concept

2906
01:03:54,309 --> 01:03:54,319
to make sure you know about the concept
 

2907
01:03:54,319 --> 01:03:56,309
to make sure you know about the concept
and then you'll see it more in

2908
01:03:56,309 --> 01:03:56,319
and then you'll see it more in
 

2909
01:03:56,319 --> 01:03:58,470
and then you'll see it more in
discussion. I think many of your

2910
01:03:58,470 --> 01:03:58,480
discussion. I think many of your
 

2911
01:03:58,480 --> 01:03:59,910
discussion. I think many of your
projects are engaging with this stuff

2912
01:03:59,910 --> 01:03:59,920
projects are engaging with this stuff
 

2913
01:03:59,920 --> 01:04:01,270
projects are engaging with this stuff
anyway. Lots of people's projects are

2914
01:04:01,270 --> 01:04:01,280
anyway. Lots of people's projects are
 

2915
01:04:01,280 --> 01:04:05,990
anyway. Lots of people's projects are
touching this problem. But so people

2916
01:04:05,990 --> 01:04:06,000
touching this problem. But so people
 

2917
01:04:06,000 --> 01:04:08,549
touching this problem. But so people
know observed the following phenomena.

2918
01:04:08,549 --> 01:04:08,559
know observed the following phenomena.
 

2919
01:04:08,559 --> 01:04:11,589
know observed the following phenomena.
This was first uh observed by people

2920
01:04:11,589 --> 01:04:11,599
This was first uh observed by people
 

2921
01:04:11,599 --> 01:04:13,510
This was first uh observed by people
when they were taking models and

2922
01:04:13,510 --> 01:04:13,520
when they were taking models and
 

2923
01:04:13,520 --> 01:04:17,109
when they were taking models and
continuously retraining them

2924
01:04:17,109 --> 01:04:17,119
continuously retraining them
 

2925
01:04:17,119 --> 01:04:18,870
continuously retraining them
uh for tasks as they saw them in the

2926
01:04:18,870 --> 01:04:18,880
uh for tasks as they saw them in the
 

2927
01:04:18,880 --> 01:04:20,789
uh for tasks as they saw them in the
field. What they experienced was that

2928
01:04:20,789 --> 01:04:20,799
field. What they experienced was that
 

2929
01:04:20,799 --> 01:04:24,069
field. What they experienced was that
when fine-tuning a model,

2930
01:04:24,069 --> 01:04:24,079
when fine-tuning a model,
 

2931
01:04:24,079 --> 01:04:25,510
when fine-tuning a model,
the model used to know how to do

2932
01:04:25,510 --> 01:04:25,520
the model used to know how to do
 

2933
01:04:25,520 --> 01:04:27,349
the model used to know how to do
something

2934
01:04:27,349 --> 01:04:27,359
something
 

2935
01:04:27,359 --> 01:04:30,549
something
after fine-tuning on other stuff, it

2936
01:04:30,549 --> 01:04:30,559
after fine-tuning on other stuff, it
 

2937
01:04:30,559 --> 01:04:33,109
after fine-tuning on other stuff, it
forgets how to do what it knew how to

2938
01:04:33,109 --> 01:04:33,119
forgets how to do what it knew how to
 

2939
01:04:33,119 --> 01:04:35,270
forgets how to do what it knew how to
do.

2940
01:04:35,270 --> 01:04:35,280
do.
 

2941
01:04:35,280 --> 01:04:37,190
do.
It got good at the new task but it

2942
01:04:37,190 --> 01:04:37,200
It got good at the new task but it
 

2943
01:04:37,200 --> 01:04:38,710
It got good at the new task but it
forgot how to do old tasks it knew how

2944
01:04:38,710 --> 01:04:38,720
forgot how to do old tasks it knew how
 

2945
01:04:38,720 --> 01:04:42,069
forgot how to do old tasks it knew how
to do. This is an empirical observation

2946
01:04:42,069 --> 01:04:42,079
to do. This is an empirical observation
 

2947
01:04:42,079 --> 01:04:45,349
to do. This is an empirical observation
that people had. This happens

2948
01:04:45,349 --> 01:04:45,359
that people had. This happens
 

2949
01:04:45,359 --> 01:04:47,109
that people had. This happens
and it was confusing to people because

2950
01:04:47,109 --> 01:04:47,119
and it was confusing to people because
 

2951
01:04:47,119 --> 01:04:48,950
and it was confusing to people because
they were like this other task there was

2952
01:04:48,950 --> 01:04:48,960
they were like this other task there was
 

2953
01:04:48,960 --> 01:04:51,829
they were like this other task there was
no interference. This this the task is

2954
01:04:51,829 --> 01:04:51,839
no interference. This this the task is
 

2955
01:04:51,839 --> 01:04:54,549
no interference. This this the task is
very different. Why is it not

2956
01:04:54,549 --> 01:04:54,559
very different. Why is it not
 

2957
01:04:54,559 --> 01:04:56,470
very different. Why is it not
remembering how to do this other task it

2958
01:04:56,470 --> 01:04:56,480
remembering how to do this other task it
 

2959
01:04:56,480 --> 01:05:01,109
remembering how to do this other task it
used to know how to do? And so people

2960
01:05:01,109 --> 01:05:01,119
used to know how to do? And so people
 

2961
01:05:01,119 --> 01:05:03,990
used to know how to do? And so people
ask the question of okay why is this

2962
01:05:03,990 --> 01:05:04,000
ask the question of okay why is this
 

2963
01:05:04,000 --> 01:05:07,430
ask the question of okay why is this
happening and what can we do about it?

2964
01:05:07,430 --> 01:05:07,440
happening and what can we do about it?
 

2965
01:05:07,440 --> 01:05:09,829
happening and what can we do about it?
Okay so everyone kind of understand the

2966
01:05:09,829 --> 01:05:09,839
Okay so everyone kind of understand the
 

2967
01:05:09,839 --> 01:05:13,029
Okay so everyone kind of understand the
idea of what this is. So one perspective

2968
01:05:13,029 --> 01:05:13,039
idea of what this is. So one perspective
 

2969
01:05:13,039 --> 01:05:14,870
idea of what this is. So one perspective
on this for finetuning is to say this is

2970
01:05:14,870 --> 01:05:14,880
on this for finetuning is to say this is
 

2971
01:05:14,880 --> 01:05:17,430
on this for finetuning is to say this is
not a problem. So let's we have to

2972
01:05:17,430 --> 01:05:17,440
not a problem. So let's we have to
 

2973
01:05:17,440 --> 01:05:18,950
not a problem. So let's we have to
acknowledge that this is not a problem

2974
01:05:18,950 --> 01:05:18,960
acknowledge that this is not a problem
 

2975
01:05:18,960 --> 01:05:20,549
acknowledge that this is not a problem
perspective before talking about the why

2976
01:05:20,549 --> 01:05:20,559
perspective before talking about the why
 

2977
01:05:20,559 --> 01:05:22,789
perspective before talking about the why
is it a problem. The not a problem

2978
01:05:22,789 --> 01:05:22,799
is it a problem. The not a problem
 

2979
01:05:22,799 --> 01:05:25,430
is it a problem. The not a problem
perspective says well I'm fine-tuning my

2980
01:05:25,430 --> 01:05:25,440
perspective says well I'm fine-tuning my
 

2981
01:05:25,440 --> 01:05:28,230
perspective says well I'm fine-tuning my
model for a new task. I only care about

2982
01:05:28,230 --> 01:05:28,240
model for a new task. I only care about
 

2983
01:05:28,240 --> 01:05:30,470
model for a new task. I only care about
the new task. If it can't do something

2984
01:05:30,470 --> 01:05:30,480
the new task. If it can't do something
 

2985
01:05:30,480 --> 01:05:33,829
the new task. If it can't do something
it used to be able to do, I don't care.

2986
01:05:33,829 --> 01:05:33,839
it used to be able to do, I don't care.
 

2987
01:05:33,839 --> 01:05:35,190
it used to be able to do, I don't care.
I'm not interested in that task. I'm

2988
01:05:35,190 --> 01:05:35,200
I'm not interested in that task. I'm
 

2989
01:05:35,200 --> 01:05:37,270
I'm not interested in that task. I'm
interested in this task. All I care is

2990
01:05:37,270 --> 01:05:37,280
interested in this task. All I care is
 

2991
01:05:37,280 --> 01:05:38,789
interested in this task. All I care is
how good is it at this task. If it gets

2992
01:05:38,789 --> 01:05:38,799
how good is it at this task. If it gets
 

2993
01:05:38,799 --> 01:05:40,789
how good is it at this task. If it gets
other things, fine, let it. Does

2994
01:05:40,789 --> 01:05:40,799
other things, fine, let it. Does
 

2995
01:05:40,799 --> 01:05:43,990
other things, fine, let it. Does
everyone see this?

2996
01:05:43,990 --> 01:05:44,000
everyone see this?
 

2997
01:05:44,000 --> 01:05:47,510
everyone see this?
Very reasonable point of view. Um,

2998
01:05:47,510 --> 01:05:47,520
Very reasonable point of view. Um,
 

2999
01:05:47,520 --> 01:05:49,589
Very reasonable point of view. Um,
and

3000
01:05:49,589 --> 01:05:49,599
and
 

3001
01:05:49,599 --> 01:05:52,710
and
there's a problem. Uh the problem is

3002
01:05:52,710 --> 01:05:52,720
there's a problem. Uh the problem is
 

3003
01:05:52,720 --> 01:05:54,309
there's a problem. Uh the problem is
that

3004
01:05:54,309 --> 01:05:54,319
that
 

3005
01:05:54,319 --> 01:05:56,470
that
sometimes the kind of thing you're

3006
01:05:56,470 --> 01:05:56,480
sometimes the kind of thing you're
 

3007
01:05:56,480 --> 01:06:00,950
sometimes the kind of thing you're
fine-tuning for is pretty broad

3008
01:06:00,950 --> 01:06:00,960
fine-tuning for is pretty broad
 

3009
01:06:00,960 --> 01:06:02,710
fine-tuning for is pretty broad
and

3010
01:06:02,710 --> 01:06:02,720
and
 

3011
01:06:02,720 --> 01:06:04,230
and
this is particularly the case with

3012
01:06:04,230 --> 01:06:04,240
this is particularly the case with
 

3013
01:06:04,240 --> 01:06:07,270
this is particularly the case with
modern language models. Um so you have a

3014
01:06:07,270 --> 01:06:07,280
modern language models. Um so you have a
 

3015
01:06:07,280 --> 01:06:09,589
modern language models. Um so you have a
language model that you've pre-trained

3016
01:06:09,589 --> 01:06:09,599
language model that you've pre-trained
 

3017
01:06:09,599 --> 01:06:11,510
language model that you've pre-trained
on lots of data so it can do all kinds

3018
01:06:11,510 --> 01:06:11,520
on lots of data so it can do all kinds
 

3019
01:06:11,520 --> 01:06:13,349
on lots of data so it can do all kinds
of things.

3020
01:06:13,349 --> 01:06:13,359
of things.
 

3021
01:06:13,359 --> 01:06:15,589
of things.
And then in real language models people

3022
01:06:15,589 --> 01:06:15,599
And then in real language models people
 

3023
01:06:15,599 --> 01:06:17,829
And then in real language models people
will run post-training. We'll talk about

3024
01:06:17,829 --> 01:06:17,839
will run post-training. We'll talk about
 

3025
01:06:17,839 --> 01:06:19,829
will run post-training. We'll talk about
post- training. I told you one example

3026
01:06:19,829 --> 01:06:19,839
post- training. I told you one example
 

3027
01:06:19,839 --> 01:06:22,390
post- training. I told you one example
of it which is I said hey we want to

3028
01:06:22,390 --> 01:06:22,400
of it which is I said hey we want to
 

3029
01:06:22,400 --> 01:06:24,470
of it which is I said hey we want to
make it follow instructions.

3030
01:06:24,470 --> 01:06:24,480
make it follow instructions.
 

3031
01:06:24,480 --> 01:06:27,029
make it follow instructions.
So we postrain it. We find fine-tune it

3032
01:06:27,029 --> 01:06:27,039
So we postrain it. We find fine-tune it
 

3033
01:06:27,039 --> 01:06:29,270
So we postrain it. We find fine-tune it
by saying

3034
01:06:29,270 --> 01:06:29,280
by saying
 

3035
01:06:29,280 --> 01:06:30,710
by saying
here are some examples of instruction

3036
01:06:30,710 --> 01:06:30,720
here are some examples of instruction
 

3037
01:06:30,720 --> 01:06:33,510
here are some examples of instruction
following. Okay. So what we wanted to

3038
01:06:33,510 --> 01:06:33,520
following. Okay. So what we wanted to
 

3039
01:06:33,520 --> 01:06:34,870
following. Okay. So what we wanted to
pick up from this is how to follow

3040
01:06:34,870 --> 01:06:34,880
pick up from this is how to follow
 

3041
01:06:34,880 --> 01:06:37,109
pick up from this is how to follow
instructions.

3042
01:06:37,109 --> 01:06:37,119
instructions.
 

3043
01:06:37,119 --> 01:06:41,349
instructions.
Everyone with me? So before it could for

3044
01:06:41,349 --> 01:06:41,359
Everyone with me? So before it could for
 

3045
01:06:41,359 --> 01:06:45,109
Everyone with me? So before it could for
example complete a poem about unicorns,

3046
01:06:45,109 --> 01:06:45,119
example complete a poem about unicorns,
 

3047
01:06:45,119 --> 01:06:46,870
example complete a poem about unicorns,
right? It could complete a poem about

3048
01:06:46,870 --> 01:06:46,880
right? It could complete a poem about
 

3049
01:06:46,880 --> 01:06:49,670
right? It could complete a poem about
nuclear fusion, also complete a poem

3050
01:06:49,670 --> 01:06:49,680
nuclear fusion, also complete a poem
 

3051
01:06:49,680 --> 01:06:52,630
nuclear fusion, also complete a poem
about making meth. Okay, then we

3052
01:06:52,630 --> 01:06:52,640
about making meth. Okay, then we
 

3053
01:06:52,640 --> 01:06:56,710
about making meth. Okay, then we
postrain it. Okay, and the examples we

3054
01:06:56,710 --> 01:06:56,720
postrain it. Okay, and the examples we
 

3055
01:06:56,720 --> 01:06:59,190
postrain it. Okay, and the examples we
give it aren't going to be examples of

3056
01:06:59,190 --> 01:06:59,200
give it aren't going to be examples of
 

3057
01:06:59,200 --> 01:07:01,029
give it aren't going to be examples of
making poems about everything. We just

3058
01:07:01,029 --> 01:07:01,039
making poems about everything. We just
 

3059
01:07:01,039 --> 01:07:03,750
making poems about everything. We just
want it to know that when I tell you to

3060
01:07:03,750 --> 01:07:03,760
want it to know that when I tell you to
 

3061
01:07:03,760 --> 01:07:06,309
want it to know that when I tell you to
make a poem, make a poem. Don't make me

3062
01:07:06,309 --> 01:07:06,319
make a poem, make a poem. Don't make me
 

3063
01:07:06,319 --> 01:07:07,910
make a poem, make a poem. Don't make me
give you the beginning of the poem and

3064
01:07:07,910 --> 01:07:07,920
give you the beginning of the poem and
 

3065
01:07:07,920 --> 01:07:11,029
give you the beginning of the poem and
complete it. Okay? So, I give it

3066
01:07:11,029 --> 01:07:11,039
complete it. Okay? So, I give it
 

3067
01:07:11,039 --> 01:07:12,630
complete it. Okay? So, I give it
instruction following examples. a

3068
01:07:12,630 --> 01:07:12,640
instruction following examples. a
 

3069
01:07:12,640 --> 01:07:15,109
instruction following examples. a
thousand or 10,000 examples is enough.

3070
01:07:15,109 --> 01:07:15,119
thousand or 10,000 examples is enough.
 

3071
01:07:15,119 --> 01:07:18,069
thousand or 10,000 examples is enough.
Empirically, people found

3072
01:07:18,069 --> 01:07:18,079
Empirically, people found
 

3073
01:07:18,079 --> 01:07:21,270
Empirically, people found
now I want it now to be able to

3074
01:07:21,270 --> 01:07:21,280
now I want it now to be able to
 

3075
01:07:21,280 --> 01:07:23,829
now I want it now to be able to
instruction follow all the things it

3076
01:07:23,829 --> 01:07:23,839
instruction follow all the things it
 

3077
01:07:23,839 --> 01:07:26,309
instruction follow all the things it
used to know how to do. Everyone with

3078
01:07:26,309 --> 01:07:26,319
used to know how to do. Everyone with
 

3079
01:07:26,319 --> 01:07:28,309
used to know how to do. Everyone with
me?

3080
01:07:28,309 --> 01:07:28,319
me?
 

3081
01:07:28,319 --> 01:07:32,069
me?
But this can happen

3082
01:07:32,069 --> 01:07:32,079
But this can happen
 

3083
01:07:32,079 --> 01:07:34,630
But this can happen
which means that now because I fine-tune

3084
01:07:34,630 --> 01:07:34,640
which means that now because I fine-tune
 

3085
01:07:34,640 --> 01:07:36,870
which means that now because I fine-tune
it to follow instructions, the simple

3086
01:07:36,870 --> 01:07:36,880
it to follow instructions, the simple
 

3087
01:07:36,880 --> 01:07:38,789
it to follow instructions, the simple
act of doing this fine-tuning made it

3088
01:07:38,789 --> 01:07:38,799
act of doing this fine-tuning made it
 

3089
01:07:38,799 --> 01:07:40,470
act of doing this fine-tuning made it
forget a bunch of other things it used

3090
01:07:40,470 --> 01:07:40,480
forget a bunch of other things it used
 

3091
01:07:40,480 --> 01:07:43,750
forget a bunch of other things it used
to know. That's not good.

3092
01:07:43,750 --> 01:07:43,760
to know. That's not good.
 

3093
01:07:43,760 --> 01:07:45,510
to know. That's not good.
Okay, so that's an example of it being

3094
01:07:45,510 --> 01:07:45,520
Okay, so that's an example of it being
 

3095
01:07:45,520 --> 01:07:47,430
Okay, so that's an example of it being
bad. The other example which is very

3096
01:07:47,430 --> 01:07:47,440
bad. The other example which is very
 

3097
01:07:47,440 --> 01:07:50,549
bad. The other example which is very
practical is you want the model to not

3098
01:07:50,549 --> 01:07:50,559
practical is you want the model to not
 

3099
01:07:50,559 --> 01:07:52,390
practical is you want the model to not
be offensive or dangerous. So you don't

3100
01:07:52,390 --> 01:07:52,400
be offensive or dangerous. So you don't
 

3101
01:07:52,400 --> 01:07:54,309
be offensive or dangerous. So you don't
want to give a formula for making meth

3102
01:07:54,309 --> 01:07:54,319
want to give a formula for making meth
 

3103
01:07:54,319 --> 01:07:57,510
want to give a formula for making meth
or a poem about making me math. Okay. So

3104
01:07:57,510 --> 01:07:57,520
or a poem about making me math. Okay. So
 

3105
01:07:57,520 --> 01:08:00,150
or a poem about making me math. Okay. So
you do alignment tuning and what people

3106
01:08:00,150 --> 01:08:00,160
you do alignment tuning and what people
 

3107
01:08:00,160 --> 01:08:02,470
you do alignment tuning and what people
observed is that when you did alignment

3108
01:08:02,470 --> 01:08:02,480
observed is that when you did alignment
 

3109
01:08:02,480 --> 01:08:04,069
observed is that when you did alignment
tuning on the models with kind of fine

3110
01:08:04,069 --> 01:08:04,079
tuning on the models with kind of fine
 

3111
01:08:04,079 --> 01:08:08,069
tuning on the models with kind of fine
tuning, the models became dumber

3112
01:08:08,069 --> 01:08:08,079
tuning, the models became dumber
 

3113
01:08:08,079 --> 01:08:09,990
tuning, the models became dumber
on things that had nothing to do with

3114
01:08:09,990 --> 01:08:10,000
on things that had nothing to do with
 

3115
01:08:10,000 --> 01:08:12,630
on things that had nothing to do with
unsafe behavior.

3116
01:08:12,630 --> 01:08:12,640
unsafe behavior.
 

3117
01:08:12,640 --> 01:08:14,230
unsafe behavior.
Okay, so it's not like it became dumber

3118
01:08:14,230 --> 01:08:14,240
Okay, so it's not like it became dumber
 

3119
01:08:14,240 --> 01:08:15,510
Okay, so it's not like it became dumber
because it couldn't do this unsafe

3120
01:08:15,510 --> 01:08:15,520
because it couldn't do this unsafe
 

3121
01:08:15,520 --> 01:08:17,189
because it couldn't do this unsafe
things are completely unrelated,

3122
01:08:17,189 --> 01:08:17,199
things are completely unrelated,
 

3123
01:08:17,199 --> 01:08:19,829
things are completely unrelated,
degraded.

3124
01:08:19,829 --> 01:08:19,839
degraded.
 

3125
01:08:19,839 --> 01:08:22,149
degraded.
So catastrophic forgetting is a real

3126
01:08:22,149 --> 01:08:22,159
So catastrophic forgetting is a real
 

3127
01:08:22,159 --> 01:08:26,550
So catastrophic forgetting is a real
problem. And uh when you have this idea

3128
01:08:26,550 --> 01:08:26,560
problem. And uh when you have this idea
 

3129
01:08:26,560 --> 01:08:28,229
problem. And uh when you have this idea
that the task you want to actually

3130
01:08:28,229 --> 01:08:28,239
that the task you want to actually
 

3131
01:08:28,239 --> 01:08:30,870
that the task you want to actually
fine-tune for should potentially be able

3132
01:08:30,870 --> 01:08:30,880
fine-tune for should potentially be able
 

3133
01:08:30,880 --> 01:08:33,030
fine-tune for should potentially be able
to access all kinds of other abilities

3134
01:08:33,030 --> 01:08:33,040
to access all kinds of other abilities
 

3135
01:08:33,040 --> 01:08:35,189
to access all kinds of other abilities
that might be required, but I don't want

3136
01:08:35,189 --> 01:08:35,199
that might be required, but I don't want
 

3137
01:08:35,199 --> 01:08:38,789
that might be required, but I don't want
to give all those as examples.

3138
01:08:38,789 --> 01:08:38,799
to give all those as examples.
 

3139
01:08:38,799 --> 01:08:41,990
to give all those as examples.
Everyone good with that? Okay, so I'm

3140
01:08:41,990 --> 01:08:42,000
Everyone good with that? Okay, so I'm
 

3141
01:08:42,000 --> 01:08:44,070
Everyone good with that? Okay, so I'm
watching the time.

3142
01:08:44,070 --> 01:08:44,080
watching the time.
 

3143
01:08:44,080 --> 01:08:46,229
watching the time.
So I will just jump and make sure you

3144
01:08:46,229 --> 01:08:46,239
So I will just jump and make sure you
 

3145
01:08:46,239 --> 01:08:49,349
So I will just jump and make sure you
understand the key practical solution.

3146
01:08:49,349 --> 01:08:49,359
understand the key practical solution.
 

3147
01:08:49,359 --> 01:08:51,189
understand the key practical solution.
What is the standard solution to this

3148
01:08:51,189 --> 01:08:51,199
What is the standard solution to this
 

3149
01:08:51,199 --> 01:08:52,950
What is the standard solution to this
problem? The default, if you can do it,

3150
01:08:52,950 --> 01:08:52,960
problem? The default, if you can do it,
 

3151
01:08:52,960 --> 01:08:54,870
problem? The default, if you can do it,
do this.

3152
01:08:54,870 --> 01:08:54,880
do this.
 

3153
01:08:54,880 --> 01:08:58,070
do this.
The default is that while you're

3154
01:08:58,070 --> 01:08:58,080
The default is that while you're
 

3155
01:08:58,080 --> 01:09:02,149
The default is that while you're
fine-tuning, don't just fine-tune.

3156
01:09:02,149 --> 01:09:02,159
fine-tuning, don't just fine-tune.
 

3157
01:09:02,159 --> 01:09:06,309
fine-tuning, don't just fine-tune.
Add a mix 10% some amount of

3158
01:09:06,309 --> 01:09:06,319
Add a mix 10% some amount of
 

3159
01:09:06,319 --> 01:09:09,430
Add a mix 10% some amount of
pre-training style data.

3160
01:09:09,430 --> 01:09:09,440
pre-training style data.
 

3161
01:09:09,440 --> 01:09:11,269
pre-training style data.
It could be actual data from the same

3162
01:09:11,269 --> 01:09:11,279
It could be actual data from the same
 

3163
01:09:11,279 --> 01:09:12,309
It could be actual data from the same
distribution that you did for

3164
01:09:12,309 --> 01:09:12,319
distribution that you did for
 

3165
01:09:12,319 --> 01:09:13,590
distribution that you did for
pre-training. Whatever it is that gave

3166
01:09:13,590 --> 01:09:13,600
pre-training. Whatever it is that gave
 

3167
01:09:13,600 --> 01:09:15,110
pre-training. Whatever it is that gave
the model the capabilities it had in the

3168
01:09:15,110 --> 01:09:15,120
the model the capabilities it had in the
 

3169
01:09:15,120 --> 01:09:16,789
the model the capabilities it had in the
first place.

3170
01:09:16,789 --> 01:09:16,799
first place.
 

3171
01:09:16,799 --> 01:09:19,110
first place.
Mix in some of that

3172
01:09:19,110 --> 01:09:19,120
Mix in some of that
 

3173
01:09:19,120 --> 01:09:21,030
Mix in some of that
while you're training. So as you adapt

3174
01:09:21,030 --> 01:09:21,040
while you're training. So as you adapt
 

3175
01:09:21,040 --> 01:09:23,590
while you're training. So as you adapt
the weights, make sure you don't lose

3176
01:09:23,590 --> 01:09:23,600
the weights, make sure you don't lose
 

3177
01:09:23,600 --> 01:09:27,110
the weights, make sure you don't lose
the capabilities it already had.

3178
01:09:27,110 --> 01:09:27,120
the capabilities it already had.
 

3179
01:09:27,120 --> 01:09:30,070
the capabilities it already had.
Everyone clear with this as an idea? So

3180
01:09:30,070 --> 01:09:30,080
Everyone clear with this as an idea? So
 

3181
01:09:30,080 --> 01:09:33,269
Everyone clear with this as an idea? So
this should be done if you can do it.

3182
01:09:33,269 --> 01:09:33,279
this should be done if you can do it.
 

3183
01:09:33,279 --> 01:09:35,110
this should be done if you can do it.
When can't you do it? You can't do it

3184
01:09:35,110 --> 01:09:35,120
When can't you do it? You can't do it
 

3185
01:09:35,120 --> 01:09:36,149
When can't you do it? You can't do it
when you don't have access to

3186
01:09:36,149 --> 01:09:36,159
when you don't have access to
 

3187
01:09:36,159 --> 01:09:40,149
when you don't have access to
pre-training style data.

3188
01:09:40,149 --> 01:09:40,159

 

3189
01:09:40,159 --> 01:09:41,749

So there are techniques and you'll see

3190
01:09:41,749 --> 01:09:41,759
So there are techniques and you'll see
 

3191
01:09:41,759 --> 01:09:44,870
So there are techniques and you'll see
one of them in the discussion

3192
01:09:44,870 --> 01:09:44,880
one of them in the discussion
 

3193
01:09:44,880 --> 01:09:48,789
one of them in the discussion
of how you could try to get around this

3194
01:09:48,789 --> 01:09:48,799
of how you could try to get around this
 

3195
01:09:48,799 --> 01:09:50,229
of how you could try to get around this
problem of not having pre-train

3196
01:09:50,229 --> 01:09:50,239
problem of not having pre-train
 

3197
01:09:50,239 --> 01:09:54,310
problem of not having pre-train
pre-training style data uh and you know

3198
01:09:54,310 --> 01:09:54,320
pre-training style data uh and you know
 

3199
01:09:54,320 --> 01:09:56,470
pre-training style data uh and you know
use that for uh doing the finetune to

3200
01:09:56,470 --> 01:09:56,480
use that for uh doing the finetune to
 

3201
01:09:56,480 --> 01:09:57,590
use that for uh doing the finetune to
try to avoid this catastrophic

3202
01:09:57,590 --> 01:09:57,600
try to avoid this catastrophic
 

3203
01:09:57,600 --> 01:09:59,189
try to avoid this catastrophic
forgetting problem. The discussion will

3204
01:09:59,189 --> 01:09:59,199
forgetting problem. The discussion will
 

3205
01:09:59,199 --> 01:10:00,709
forgetting problem. The discussion will
talk more about this including some nice

3206
01:10:00,709 --> 01:10:00,719
talk more about this including some nice
 

3207
01:10:00,719 --> 01:10:02,790
talk more about this including some nice
visualizations of why this can happen.

3208
01:10:02,790 --> 01:10:02,800
visualizations of why this can happen.
 

3209
01:10:02,800 --> 01:10:07,990
visualizations of why this can happen.
Um I wanted to sort of just give you

3210
01:10:07,990 --> 01:10:08,000
Um I wanted to sort of just give you
 

3211
01:10:08,000 --> 01:10:11,350
Um I wanted to sort of just give you
a potentially useful analogy.

3212
01:10:11,350 --> 01:10:11,360
a potentially useful analogy.
 

3213
01:10:11,360 --> 01:10:14,470
a potentially useful analogy.
So, Halloween just happened uh little

3214
01:10:14,470 --> 01:10:14,480
So, Halloween just happened uh little
 

3215
01:10:14,480 --> 01:10:17,030
So, Halloween just happened uh little
while ago and have any of you seen those

3216
01:10:17,030 --> 01:10:17,040
while ago and have any of you seen those
 

3217
01:10:17,040 --> 01:10:19,830
while ago and have any of you seen those
costumes that are like have powered like

3218
01:10:19,830 --> 01:10:19,840
costumes that are like have powered like
 

3219
01:10:19,840 --> 01:10:22,229
costumes that are like have powered like
air blowers? Have you seen those?

3220
01:10:22,229 --> 01:10:22,239
air blowers? Have you seen those?
 

3221
01:10:22,239 --> 01:10:23,590
air blowers? Have you seen those?
Anyone? Raise your hand if you've seen

3222
01:10:23,590 --> 01:10:23,600
Anyone? Raise your hand if you've seen
 

3223
01:10:23,600 --> 01:10:25,030
Anyone? Raise your hand if you've seen
these kind of costumes that have powered

3224
01:10:25,030 --> 01:10:25,040
these kind of costumes that have powered
 

3225
01:10:25,040 --> 01:10:27,750
these kind of costumes that have powered
air blowers. Okay, so many of you have

3226
01:10:27,750 --> 01:10:27,760
air blowers. Okay, so many of you have
 

3227
01:10:27,760 --> 01:10:30,790
air blowers. Okay, so many of you have
not. So think about a costume in which

3228
01:10:30,790 --> 01:10:30,800
not. So think about a costume in which
 

3229
01:10:30,800 --> 01:10:34,149
not. So think about a costume in which
you have air that's f like puffing it up

3230
01:10:34,149 --> 01:10:34,159
you have air that's f like puffing it up
 

3231
01:10:34,159 --> 01:10:37,110
you have air that's f like puffing it up
and filling out this air structure.

3232
01:10:37,110 --> 01:10:37,120
and filling out this air structure.
 

3233
01:10:37,120 --> 01:10:38,950
and filling out this air structure.
Okay,

3234
01:10:38,950 --> 01:10:38,960
Okay,
 

3235
01:10:38,960 --> 01:10:41,910
Okay,
you can think about machine learning as

3236
01:10:41,910 --> 01:10:41,920
you can think about machine learning as
 

3237
01:10:41,920 --> 01:10:44,470
you can think about machine learning as
being like blowing up a balloon, but

3238
01:10:44,470 --> 01:10:44,480
being like blowing up a balloon, but
 

3239
01:10:44,480 --> 01:10:47,750
being like blowing up a balloon, but
it's a leaky balloon. Okay, the data is

3240
01:10:47,750 --> 01:10:47,760
it's a leaky balloon. Okay, the data is
 

3241
01:10:47,760 --> 01:10:50,070
it's a leaky balloon. Okay, the data is
the air that's blowing in and the loss

3242
01:10:50,070 --> 01:10:50,080
the air that's blowing in and the loss
 

3243
01:10:50,080 --> 01:10:54,310
the air that's blowing in and the loss
and weight decay is like these leaks

3244
01:10:54,310 --> 01:10:54,320
and weight decay is like these leaks
 

3245
01:10:54,320 --> 01:10:57,590
and weight decay is like these leaks
that are making stuff go away. you train

3246
01:10:57,590 --> 01:10:57,600
that are making stuff go away. you train
 

3247
01:10:57,600 --> 01:11:00,550
that are making stuff go away. you train
your models with weight decay. And so

3248
01:11:00,550 --> 01:11:00,560
your models with weight decay. And so
 

3249
01:11:00,560 --> 01:11:03,830
your models with weight decay. And so
one intuition you could have is that if

3250
01:11:03,830 --> 01:11:03,840
one intuition you could have is that if
 

3251
01:11:03,840 --> 01:11:07,270
one intuition you could have is that if
you stop puffing air into this arm and

3252
01:11:07,270 --> 01:11:07,280
you stop puffing air into this arm and
 

3253
01:11:07,280 --> 01:11:09,030
you stop puffing air into this arm and
start puffing it into this arm only,

3254
01:11:09,030 --> 01:11:09,040
start puffing it into this arm only,
 

3255
01:11:09,040 --> 01:11:11,910
start puffing it into this arm only,
this arm is slowly going to go,

3256
01:11:11,910 --> 01:11:11,920
this arm is slowly going to go,
 

3257
01:11:11,920 --> 01:11:15,990
this arm is slowly going to go,
you know, like all the air will leave.

3258
01:11:15,990 --> 01:11:16,000
you know, like all the air will leave.
 

3259
01:11:16,000 --> 01:11:19,189
you know, like all the air will leave.
That is a kind of very coarse view of

3260
01:11:19,189 --> 01:11:19,199
That is a kind of very coarse view of
 

3261
01:11:19,199 --> 01:11:21,750
That is a kind of very coarse view of
what causes catastrophic forgetting.

3262
01:11:21,750 --> 01:11:21,760
what causes catastrophic forgetting.
 

3263
01:11:21,760 --> 01:11:23,910
what causes catastrophic forgetting.
Unless you have enough pressure from the

3264
01:11:23,910 --> 01:11:23,920
Unless you have enough pressure from the
 

3265
01:11:23,920 --> 01:11:26,870
Unless you have enough pressure from the
data, keeping the model kind of in

3266
01:11:26,870 --> 01:11:26,880
data, keeping the model kind of in
 

3267
01:11:26,880 --> 01:11:28,390
data, keeping the model kind of in
different directions, it'll degrade.

3268
01:11:28,390 --> 01:11:28,400
different directions, it'll degrade.
 

3269
01:11:28,400 --> 01:11:30,870
different directions, it'll degrade.
Now, that isn't exactly correct because

3270
01:11:30,870 --> 01:11:30,880
Now, that isn't exactly correct because
 

3271
01:11:30,880 --> 01:11:32,950
Now, that isn't exactly correct because
just turning off weight decay won't stop

3272
01:11:32,950 --> 01:11:32,960
just turning off weight decay won't stop
 

3273
01:11:32,960 --> 01:11:34,630
just turning off weight decay won't stop
the problem. So, if you want to

3274
01:11:34,630 --> 01:11:34,640
the problem. So, if you want to
 

3275
01:11:34,640 --> 01:11:36,390
the problem. So, if you want to
understand the next level of detail, you

3276
01:11:36,390 --> 01:11:36,400
understand the next level of detail, you
 

3277
01:11:36,400 --> 01:11:39,189
understand the next level of detail, you
can think of it this way. Your models

3278
01:11:39,189 --> 01:11:39,199
can think of it this way. Your models
 

3279
01:11:39,199 --> 01:11:41,590
can think of it this way. Your models
are never sparse,

3280
01:11:41,590 --> 01:11:41,600
are never sparse,
 

3281
01:11:41,600 --> 01:11:43,270
are never sparse,
right? They're never sparse in the

3282
01:11:43,270 --> 01:11:43,280
right? They're never sparse in the
 

3283
01:11:43,280 --> 01:11:44,950
right? They're never sparse in the
native weights. It's never that this

3284
01:11:44,950 --> 01:11:44,960
native weights. It's never that this
 

3285
01:11:44,960 --> 01:11:47,350
native weights. It's never that this
weight, this particular weight is

3286
01:11:47,350 --> 01:11:47,360
weight, this particular weight is
 

3287
01:11:47,360 --> 01:11:50,709
weight, this particular weight is
controlling this particular attribute.

3288
01:11:50,709 --> 01:11:50,719
controlling this particular attribute.
 

3289
01:11:50,719 --> 01:11:53,110
controlling this particular attribute.
It's always some vector,

3290
01:11:53,110 --> 01:11:53,120
It's always some vector,
 

3291
01:11:53,120 --> 01:11:58,149
It's always some vector,
right? And so the gradients are all in a

3292
01:11:58,149 --> 01:11:58,159
right? And so the gradients are all in a
 

3293
01:11:58,159 --> 01:12:00,630
right? And so the gradients are all in a
way on the task you want are throwing

3294
01:12:00,630 --> 01:12:00,640
way on the task you want are throwing
 

3295
01:12:00,640 --> 01:12:02,310
way on the task you want are throwing
random gradient noise in other

3296
01:12:02,310 --> 01:12:02,320
random gradient noise in other
 

3297
01:12:02,320 --> 01:12:03,910
random gradient noise in other
directions.

3298
01:12:03,910 --> 01:12:03,920
directions.
 

3299
01:12:03,920 --> 01:12:06,070
directions.
The pressure of data is keeping that

3300
01:12:06,070 --> 01:12:06,080
The pressure of data is keeping that
 

3301
01:12:06,080 --> 01:12:08,630
The pressure of data is keeping that
back pushing it back to converge to the

3302
01:12:08,630 --> 01:12:08,640
back pushing it back to converge to the
 

3303
01:12:08,640 --> 01:12:10,390
back pushing it back to converge to the
right thing. But without the

3304
01:12:10,390 --> 01:12:10,400
right thing. But without the
 

3305
01:12:10,400 --> 01:12:13,110
right thing. But without the
counterbalancing pressure of task data,

3306
01:12:13,110 --> 01:12:13,120
counterbalancing pressure of task data,
 

3307
01:12:13,120 --> 01:12:15,910
counterbalancing pressure of task data,
you'll just drift randomly in these

3308
01:12:15,910 --> 01:12:15,920
you'll just drift randomly in these
 

3309
01:12:15,920 --> 01:12:17,350
you'll just drift randomly in these
other directions and you'll lose

3310
01:12:17,350 --> 01:12:17,360
other directions and you'll lose
 

3311
01:12:17,360 --> 01:12:19,270
other directions and you'll lose
capabilities.

3312
01:12:19,270 --> 01:12:19,280
capabilities.
 

3313
01:12:19,280 --> 01:12:21,350
capabilities.
This is like pictures of how you think

3314
01:12:21,350 --> 01:12:21,360
This is like pictures of how you think
 

3315
01:12:21,360 --> 01:12:22,950
This is like pictures of how you think
why this happens. Yeah.

3316
01:12:22,950 --> 01:12:22,960
why this happens. Yeah.
 

3317
01:12:22,960 --> 01:12:25,270
why this happens. Yeah.
>> Does this also happen in the same way

3318
01:12:25,270 --> 01:12:25,280
>> Does this also happen in the same way
 

3319
01:12:25,280 --> 01:12:27,669
>> Does this also happen in the same way
with Laura when you're keeping all the

3320
01:12:27,669 --> 01:12:27,679
with Laura when you're keeping all the
 

3321
01:12:27,679 --> 01:12:31,270
with Laura when you're keeping all the
underlying but add?

3322
01:12:31,270 --> 01:12:31,280
underlying but add?
 

3323
01:12:31,280 --> 01:12:33,430
underlying but add?
>> Great question. So the question was what

3324
01:12:33,430 --> 01:12:33,440
>> Great question. So the question was what
 

3325
01:12:33,440 --> 01:12:36,070
>> Great question. So the question was what
about Lauras? And Lauras in fact suffer

3326
01:12:36,070 --> 01:12:36,080
about Lauras? And Lauras in fact suffer
 

3327
01:12:36,080 --> 01:12:38,470
about Lauras? And Lauras in fact suffer
less from catastrophic forgetting. They

3328
01:12:38,470 --> 01:12:38,480
less from catastrophic forgetting. They
 

3329
01:12:38,480 --> 01:12:40,630
less from catastrophic forgetting. They
still do, but it's less. And the reason

3330
01:12:40,630 --> 01:12:40,640
still do, but it's less. And the reason
 

3331
01:12:40,640 --> 01:12:42,229
still do, but it's less. And the reason
is that a lot of weights are effect a

3332
01:12:42,229 --> 01:12:42,239
is that a lot of weights are effect a
 

3333
01:12:42,239 --> 01:12:44,790
is that a lot of weights are effect a
lot of directions are effectively frozen

3334
01:12:44,790 --> 01:12:44,800
lot of directions are effectively frozen
 

3335
01:12:44,800 --> 01:12:46,390
lot of directions are effectively frozen
um by doing the how you're doing the

3336
01:12:46,390 --> 01:12:46,400
um by doing the how you're doing the
 

3337
01:12:46,400 --> 01:12:49,189
um by doing the how you're doing the
Laura. And so you have less ability to

3338
01:12:49,189 --> 01:12:49,199
Laura. And so you have less ability to
 

3339
01:12:49,199 --> 01:12:53,030
Laura. And so you have less ability to
sort of muck up uh certain uh certain

3340
01:12:53,030 --> 01:12:53,040
sort of muck up uh certain uh certain
 

3341
01:12:53,040 --> 01:12:55,189
sort of muck up uh certain uh certain
directions. Second is weight decay in

3342
01:12:55,189 --> 01:12:55,199
directions. Second is weight decay in
 

3343
01:12:55,199 --> 01:12:57,830
directions. Second is weight decay in
Laura is moving you towards the

3344
01:12:57,830 --> 01:12:57,840
Laura is moving you towards the
 

3345
01:12:57,840 --> 01:12:59,910
Laura is moving you towards the
pre-trained weights

3346
01:12:59,910 --> 01:12:59,920
pre-trained weights
 

3347
01:12:59,920 --> 01:13:02,950
pre-trained weights
and not to zero. So that's also helping

3348
01:13:02,950 --> 01:13:02,960
and not to zero. So that's also helping
 

3349
01:13:02,960 --> 01:13:06,550
and not to zero. So that's also helping
you not forget. So Lauras are are better

3350
01:13:06,550 --> 01:13:06,560
you not forget. So Lauras are are better
 

3351
01:13:06,560 --> 01:13:08,229
you not forget. So Lauras are are better
from the perspective of catastrophic

3352
01:13:08,229 --> 01:13:08,239
from the perspective of catastrophic
 

3353
01:13:08,239 --> 01:13:09,350
from the perspective of catastrophic
healing. They're not completely immune

3354
01:13:09,350 --> 01:13:09,360
healing. They're not completely immune
 

3355
01:13:09,360 --> 01:13:11,910
healing. They're not completely immune
from it though

3356
01:13:11,910 --> 01:13:11,920
from it though
 

3357
01:13:11,920 --> 01:13:13,110
from it though
um because they still have this gradient

3358
01:13:13,110 --> 01:13:13,120
um because they still have this gradient
 

3359
01:13:13,120 --> 01:13:16,070
um because they still have this gradient
in they still have this drift uh that

3360
01:13:16,070 --> 01:13:16,080
in they still have this drift uh that
 

3361
01:13:16,080 --> 01:13:18,390
in they still have this drift uh that
can happen within the dimensions that

3362
01:13:18,390 --> 01:13:18,400
can happen within the dimensions that
 

3363
01:13:18,400 --> 01:13:19,990
can happen within the dimensions that
are there if they can influence that

3364
01:13:19,990 --> 01:13:20,000
are there if they can influence that
 

3365
01:13:20,000 --> 01:13:23,350
are there if they can influence that
feature that that attribute. Okay. So

3366
01:13:23,350 --> 01:13:23,360
feature that that attribute. Okay. So
 

3367
01:13:23,360 --> 01:13:25,510
feature that that attribute. Okay. So
you can read more about this is very

3368
01:13:25,510 --> 01:13:25,520
you can read more about this is very
 

3369
01:13:25,520 --> 01:13:27,189
you can read more about this is very
important practically lots of little

3370
01:13:27,189 --> 01:13:27,199
important practically lots of little
 

3371
01:13:27,199 --> 01:13:29,110
important practically lots of little
tips and tricks that people have but um

3372
01:13:29,110 --> 01:13:29,120
tips and tricks that people have but um
 

3373
01:13:29,120 --> 01:13:31,990
tips and tricks that people have but um
I want to move on to the next thing.

3374
01:13:31,990 --> 01:13:32,000
I want to move on to the next thing.
 

3375
01:13:32,000 --> 01:13:33,110
I want to move on to the next thing.
So I want to talk about generative

3376
01:13:33,110 --> 01:13:33,120
So I want to talk about generative
 

3377
01:13:33,120 --> 01:13:35,990
So I want to talk about generative
models because I want to get to VAES a

3378
01:13:35,990 --> 01:13:36,000
models because I want to get to VAES a
 

3379
01:13:36,000 --> 01:13:38,070
models because I want to get to VAES a
little bit.

3380
01:13:38,070 --> 01:13:38,080
little bit.
 

3381
01:13:38,080 --> 01:13:41,189
little bit.
So

3382
01:13:41,189 --> 01:13:41,199

 

3383
01:13:41,199 --> 01:13:43,830

very briefly you've already seen a basic

3384
01:13:43,830 --> 01:13:43,840
very briefly you've already seen a basic
 

3385
01:13:43,840 --> 01:13:45,110
very briefly you've already seen a basic
kind of generative model which is these

3386
01:13:45,110 --> 01:13:45,120
kind of generative model which is these
 

3387
01:13:45,120 --> 01:13:46,790
kind of generative model which is these
auto reggressive generation models. We

3388
01:13:46,790 --> 01:13:46,800
auto reggressive generation models. We
 

3389
01:13:46,800 --> 01:13:48,950
auto reggressive generation models. We
talked about them a lot. I don't want to

3390
01:13:48,950 --> 01:13:48,960
talked about them a lot. I don't want to
 

3391
01:13:48,960 --> 01:13:50,550
talked about them a lot. I don't want to
um

3392
01:13:50,550 --> 01:13:50,560
um
 

3393
01:13:50,560 --> 01:13:53,590
um
belabor that. So when you think about

3394
01:13:53,590 --> 01:13:53,600
belabor that. So when you think about
 

3395
01:13:53,600 --> 01:13:54,950
belabor that. So when you think about
generative models as we talk about them

3396
01:13:54,950 --> 01:13:54,960
generative models as we talk about them
 

3397
01:13:54,960 --> 01:13:57,030
generative models as we talk about them
in the class we'll think about them in

3398
01:13:57,030 --> 01:13:57,040
in the class we'll think about them in
 

3399
01:13:57,040 --> 01:13:59,030
in the class we'll think about them in
two different categories.

3400
01:13:59,030 --> 01:13:59,040
two different categories.
 

3401
01:13:59,040 --> 01:14:02,070
two different categories.
one unconditional generation which is we

3402
01:14:02,070 --> 01:14:02,080
one unconditional generation which is we
 

3403
01:14:02,080 --> 01:14:04,470
one unconditional generation which is we
have a model we would like to draw a new

3404
01:14:04,470 --> 01:14:04,480
have a model we would like to draw a new
 

3405
01:14:04,480 --> 01:14:06,630
have a model we would like to draw a new
example that's it there's a model we

3406
01:14:06,630 --> 01:14:06,640
example that's it there's a model we
 

3407
01:14:06,640 --> 01:14:08,070
example that's it there's a model we
want a new example we feed randomness

3408
01:14:08,070 --> 01:14:08,080
want a new example we feed randomness
 

3409
01:14:08,080 --> 01:14:11,030
want a new example we feed randomness
into the model and we get a new example

3410
01:14:11,030 --> 01:14:11,040
into the model and we get a new example
 

3411
01:14:11,040 --> 01:14:13,910
into the model and we get a new example
this is interesting to study and useful

3412
01:14:13,910 --> 01:14:13,920
this is interesting to study and useful
 

3413
01:14:13,920 --> 01:14:17,830
this is interesting to study and useful
to understand less useful in practice

3414
01:14:17,830 --> 01:14:17,840
to understand less useful in practice
 

3415
01:14:17,840 --> 01:14:19,669
to understand less useful in practice
conditional generation is more useful in

3416
01:14:19,669 --> 01:14:19,679
conditional generation is more useful in
 

3417
01:14:19,679 --> 01:14:21,669
conditional generation is more useful in
practice conditional generation is I

3418
01:14:21,669 --> 01:14:21,679
practice conditional generation is I
 

3419
01:14:21,679 --> 01:14:23,030
practice conditional generation is I
want to sample from a conditional

3420
01:14:23,030 --> 01:14:23,040
want to sample from a conditional
 

3421
01:14:23,040 --> 01:14:24,550
want to sample from a conditional
distribution

3422
01:14:24,550 --> 01:14:24,560
distribution
 

3423
01:14:24,560 --> 01:14:27,189
distribution
so I have randomness and I have

3424
01:14:27,189 --> 01:14:27,199
so I have randomness and I have
 

3425
01:14:27,199 --> 01:14:28,870
so I have randomness and I have
conditioning and then I get a new

3426
01:14:28,870 --> 01:14:28,880
conditioning and then I get a new
 

3427
01:14:28,880 --> 01:14:31,669
conditioning and then I get a new
example that's distribution. So for

3428
01:14:31,669 --> 01:14:31,679
example that's distribution. So for
 

3429
01:14:31,679 --> 01:14:39,189
example that's distribution. So for
example, this would be give me a new

3430
01:14:39,189 --> 01:14:39,199

 

3431
01:14:39,199 --> 01:14:43,030

cat. Give me a natural image of a cat.

3432
01:14:43,030 --> 01:14:43,040
cat. Give me a natural image of a cat.
 

3433
01:14:43,040 --> 01:14:44,870
cat. Give me a natural image of a cat.
Everyone see this? That's like just

3434
01:14:44,870 --> 01:14:44,880
Everyone see this? That's like just
 

3435
01:14:44,880 --> 01:14:46,630
Everyone see this? That's like just
generative models generally. We'll talk

3436
01:14:46,630 --> 01:14:46,640
generative models generally. We'll talk
 

3437
01:14:46,640 --> 01:14:52,870
generative models generally. We'll talk
about this, but this is what's useful.

3438
01:14:52,870 --> 01:14:52,880

 

3439
01:14:52,880 --> 01:14:54,390

So

3440
01:14:54,390 --> 01:14:54,400
So
 

3441
01:14:54,400 --> 01:14:57,990
So
interestingly, it's useful to understand

3442
01:14:57,990 --> 01:14:58,000
interestingly, it's useful to understand
 

3443
01:14:58,000 --> 01:15:01,750
interestingly, it's useful to understand
approaches that don't work

3444
01:15:01,750 --> 01:15:01,760
approaches that don't work
 

3445
01:15:01,760 --> 01:15:03,910
approaches that don't work
first. So you should always try

3446
01:15:03,910 --> 01:15:03,920
first. So you should always try
 

3447
01:15:03,920 --> 01:15:06,070
first. So you should always try
something the most easy thing. If the

3448
01:15:06,070 --> 01:15:06,080
something the most easy thing. If the
 

3449
01:15:06,080 --> 01:15:07,830
something the most easy thing. If the
easy thing works, great. If the easy

3450
01:15:07,830 --> 01:15:07,840
easy thing works, great. If the easy
 

3451
01:15:07,840 --> 01:15:09,189
easy thing works, great. If the easy
thing doesn't work, you go to something

3452
01:15:09,189 --> 01:15:09,199
thing doesn't work, you go to something
 

3453
01:15:09,199 --> 01:15:10,790
thing doesn't work, you go to something
else. So it's good to understand the

3454
01:15:10,790 --> 01:15:10,800
else. So it's good to understand the
 

3455
01:15:10,800 --> 01:15:12,950
else. So it's good to understand the
easy things that don't work. I'm not

3456
01:15:12,950 --> 01:15:12,960
easy things that don't work. I'm not
 

3457
01:15:12,960 --> 01:15:14,470
easy things that don't work. I'm not
going to spend much time on it. I'll

3458
01:15:14,470 --> 01:15:14,480
going to spend much time on it. I'll
 

3459
01:15:14,480 --> 01:15:15,669
going to spend much time on it. I'll
come back to it next time because I want

3460
01:15:15,669 --> 01:15:15,679
come back to it next time because I want
 

3461
01:15:15,679 --> 01:15:19,189
come back to it next time because I want
to get a little bit into VAS. Um so one

3462
01:15:19,189 --> 01:15:19,199
to get a little bit into VAS. Um so one
 

3463
01:15:19,199 --> 01:15:22,709
to get a little bit into VAS. Um so one
approach is I use a classifier.

3464
01:15:22,709 --> 01:15:22,719
approach is I use a classifier.
 

3465
01:15:22,719 --> 01:15:24,870
approach is I use a classifier.
So classifier would go like this. I've

3466
01:15:24,870 --> 01:15:24,880
So classifier would go like this. I've
 

3467
01:15:24,880 --> 01:15:28,229
So classifier would go like this. I've
trained a classifier for cats

3468
01:15:28,229 --> 01:15:28,239
trained a classifier for cats
 

3469
01:15:28,239 --> 01:15:30,950
trained a classifier for cats
and now I would like to get a generation

3470
01:15:30,950 --> 01:15:30,960
and now I would like to get a generation
 

3471
01:15:30,960 --> 01:15:33,270
and now I would like to get a generation
of an image of a cat.

3472
01:15:33,270 --> 01:15:33,280
of an image of a cat.
 

3473
01:15:33,280 --> 01:15:35,189
of an image of a cat.
First way first try try of doing this is

3474
01:15:35,189 --> 01:15:35,199
First way first try try of doing this is
 

3475
01:15:35,199 --> 01:15:37,910
First way first try try of doing this is
I'll just draw a random image and I'll

3476
01:15:37,910 --> 01:15:37,920
I'll just draw a random image and I'll
 

3477
01:15:37,920 --> 01:15:40,870
I'll just draw a random image and I'll
ask is it a cat? If it's not a cat I'll

3478
01:15:40,870 --> 01:15:40,880
ask is it a cat? If it's not a cat I'll
 

3479
01:15:40,880 --> 01:15:43,189
ask is it a cat? If it's not a cat I'll
draw another random image. Does everyone

3480
01:15:43,189 --> 01:15:43,199
draw another random image. Does everyone
 

3481
01:15:43,199 --> 01:15:44,550
draw another random image. Does everyone
see why this is not going to work very

3482
01:15:44,550 --> 01:15:44,560
see why this is not going to work very
 

3483
01:15:44,560 --> 01:15:46,310
see why this is not going to work very
well?

3484
01:15:46,310 --> 01:15:46,320
well?
 

3485
01:15:46,320 --> 01:15:47,830
well?
Because most random images will be

3486
01:15:47,830 --> 01:15:47,840
Because most random images will be
 

3487
01:15:47,840 --> 01:15:50,550
Because most random images will be
random hiss. They will not be cats. They

3488
01:15:50,550 --> 01:15:50,560
random hiss. They will not be cats. They
 

3489
01:15:50,560 --> 01:15:52,709
random hiss. They will not be cats. They
will never be cats. Right? You will run

3490
01:15:52,709 --> 01:15:52,719
will never be cats. Right? You will run
 

3491
01:15:52,719 --> 01:15:54,070
will never be cats. Right? You will run
this till the end of the universe and

3492
01:15:54,070 --> 01:15:54,080
this till the end of the universe and
 

3493
01:15:54,080 --> 01:15:56,470
this till the end of the universe and
then you'll get one cat. Right? So that

3494
01:15:56,470 --> 01:15:56,480
then you'll get one cat. Right? So that
 

3495
01:15:56,480 --> 01:15:59,030
then you'll get one cat. Right? So that
won't work. So you say fine, but I this

3496
01:15:59,030 --> 01:15:59,040
won't work. So you say fine, but I this
 

3497
01:15:59,040 --> 01:16:01,110
won't work. So you say fine, but I this
thing is differentiable.

3498
01:16:01,110 --> 01:16:01,120
thing is differentiable.
 

3499
01:16:01,120 --> 01:16:03,350
thing is differentiable.
So I'll draw a random place and then

3500
01:16:03,350 --> 01:16:03,360
So I'll draw a random place and then
 

3501
01:16:03,360 --> 01:16:06,310
So I'll draw a random place and then
I'll do gradient ascent on the soft

3502
01:16:06,310 --> 01:16:06,320
I'll do gradient ascent on the soft
 

3503
01:16:06,320 --> 01:16:10,070
I'll do gradient ascent on the soft
prompt which is the image to make it be

3504
01:16:10,070 --> 01:16:10,080
prompt which is the image to make it be
 

3505
01:16:10,080 --> 01:16:13,110
prompt which is the image to make it be
a cat. Does everyone see that? like I'll

3506
01:16:13,110 --> 01:16:13,120
a cat. Does everyone see that? like I'll
 

3507
01:16:13,120 --> 01:16:15,510
a cat. Does everyone see that? like I'll
make it more cat-like because this is a

3508
01:16:15,510 --> 01:16:15,520
make it more cat-like because this is a
 

3509
01:16:15,520 --> 01:16:18,630
make it more cat-like because this is a
score, right? This also you' think it

3510
01:16:18,630 --> 01:16:18,640
score, right? This also you' think it
 

3511
01:16:18,640 --> 01:16:20,310
score, right? This also you' think it
worked. People really expected this to

3512
01:16:20,310 --> 01:16:20,320
worked. People really expected this to
 

3513
01:16:20,320 --> 01:16:22,790
worked. People really expected this to
work, but does not work. We'll talk more

3514
01:16:22,790 --> 01:16:22,800
work, but does not work. We'll talk more
 

3515
01:16:22,800 --> 01:16:25,030
work, but does not work. We'll talk more
about that later. So people say, okay,

3516
01:16:25,030 --> 01:16:25,040
about that later. So people say, okay,
 

3517
01:16:25,040 --> 01:16:26,630
about that later. So people say, okay,
you'll have to figure out in a way you

3518
01:16:26,630 --> 01:16:26,640
you'll have to figure out in a way you
 

3519
01:16:26,640 --> 01:16:27,910
you'll have to figure out in a way you
can think about diffusion models as

3520
01:16:27,910 --> 01:16:27,920
can think about diffusion models as
 

3521
01:16:27,920 --> 01:16:30,470
can think about diffusion models as
trying to solve that problem.

3522
01:16:30,470 --> 01:16:30,480
trying to solve that problem.
 

3523
01:16:30,480 --> 01:16:33,030
trying to solve that problem.
There's another approach which is I can

3524
01:16:33,030 --> 01:16:33,040
There's another approach which is I can
 

3525
01:16:33,040 --> 01:16:35,350
There's another approach which is I can
use an autoenccoder. So I just copy

3526
01:16:35,350 --> 01:16:35,360
use an autoenccoder. So I just copy
 

3527
01:16:35,360 --> 01:16:36,149
use an autoenccoder. So I just copy
stuff from how we talked about

3528
01:16:36,149 --> 01:16:36,159
stuff from how we talked about
 

3529
01:16:36,159 --> 01:16:38,630
stuff from how we talked about
autocoders before. So an autoenccoder I

3530
01:16:38,630 --> 01:16:38,640
autocoders before. So an autoenccoder I
 

3531
01:16:38,640 --> 01:16:40,950
autocoders before. So an autoenccoder I
want to draw samples from X. What can I

3532
01:16:40,950 --> 01:16:40,960
want to draw samples from X. What can I
 

3533
01:16:40,960 --> 01:16:44,229
want to draw samples from X. What can I
do? I can say fine X is too high

3534
01:16:44,229 --> 01:16:44,239
do? I can say fine X is too high
 

3535
01:16:44,239 --> 01:16:46,229
do? I can say fine X is too high
dimensional for me to sample. So I'll

3536
01:16:46,229 --> 01:16:46,239
dimensional for me to sample. So I'll
 

3537
01:16:46,239 --> 01:16:48,390
dimensional for me to sample. So I'll
make an autoenccoder. The autoenccoder

3538
01:16:48,390 --> 01:16:48,400
make an autoenccoder. The autoenccoder
 

3539
01:16:48,400 --> 01:16:50,390
make an autoenccoder. The autoenccoder
will take X drop it into a lower

3540
01:16:50,390 --> 01:16:50,400
will take X drop it into a lower
 

3541
01:16:50,400 --> 01:16:52,870
will take X drop it into a lower
dimensional space and then reconstruct

3542
01:16:52,870 --> 01:16:52,880
dimensional space and then reconstruct
 

3543
01:16:52,880 --> 01:16:54,470
dimensional space and then reconstruct
X.

3544
01:16:54,470 --> 01:16:54,480
X.
 

3545
01:16:54,480 --> 01:16:57,910
X.
So in a traditional perspective, I never

3546
01:16:57,910 --> 01:16:57,920
So in a traditional perspective, I never
 

3547
01:16:57,920 --> 01:16:59,430
So in a traditional perspective, I never
use the decoder. The decoder is just

3548
01:16:59,430 --> 01:16:59,440
use the decoder. The decoder is just
 

3549
01:16:59,440 --> 01:17:00,630
use the decoder. The decoder is just
there to train the encoder, which is a

3550
01:17:00,630 --> 01:17:00,640
there to train the encoder, which is a
 

3551
01:17:00,640 --> 01:17:03,030
there to train the encoder, which is a
good embeder. Everyone remember this?

3552
01:17:03,030 --> 01:17:03,040
good embeder. Everyone remember this?
 

3553
01:17:03,040 --> 01:17:04,470
good embeder. Everyone remember this?
But if this embedding is really

3554
01:17:04,470 --> 01:17:04,480
But if this embedding is really
 

3555
01:17:04,480 --> 01:17:05,830
But if this embedding is really
capturing the essence of what's

3556
01:17:05,830 --> 01:17:05,840
capturing the essence of what's
 

3557
01:17:05,840 --> 01:17:08,229
capturing the essence of what's
important in these images, I could just

3558
01:17:08,229 --> 01:17:08,239
important in these images, I could just
 

3559
01:17:08,239 --> 01:17:09,750
important in these images, I could just
try using the decoder to generate

3560
01:17:09,750 --> 01:17:09,760
try using the decoder to generate
 

3561
01:17:09,760 --> 01:17:11,510
try using the decoder to generate
samples.

3562
01:17:11,510 --> 01:17:11,520
samples.
 

3563
01:17:11,520 --> 01:17:13,910
samples.
So I train an autoenccoder.

3564
01:17:13,910 --> 01:17:13,920
So I train an autoenccoder.
 

3565
01:17:13,920 --> 01:17:16,550
So I train an autoenccoder.
I have a bottleneck and then I say

3566
01:17:16,550 --> 01:17:16,560
I have a bottleneck and then I say
 

3567
01:17:16,560 --> 01:17:18,390
I have a bottleneck and then I say
forget the encoder just feed in random

3568
01:17:18,390 --> 01:17:18,400
forget the encoder just feed in random
 

3569
01:17:18,400 --> 01:17:20,550
forget the encoder just feed in random
noise into the Z.

3570
01:17:20,550 --> 01:17:20,560
noise into the Z.
 

3571
01:17:20,560 --> 01:17:24,149
noise into the Z.
Can I get a sample of an image?

3572
01:17:24,149 --> 01:17:24,159
Can I get a sample of an image?
 

3573
01:17:24,159 --> 01:17:25,910
Can I get a sample of an image?
And the answer is this doesn't work

3574
01:17:25,910 --> 01:17:25,920
And the answer is this doesn't work
 

3575
01:17:25,920 --> 01:17:27,590
And the answer is this doesn't work
either.

3576
01:17:27,590 --> 01:17:27,600
either.
 

3577
01:17:27,600 --> 01:17:31,669
either.
Okay, I will talk more detail about why

3578
01:17:31,669 --> 01:17:31,679
Okay, I will talk more detail about why
 

3579
01:17:31,679 --> 01:17:35,910
Okay, I will talk more detail about why
this doesn't work um kind of in stages.

3580
01:17:35,910 --> 01:17:35,920
this doesn't work um kind of in stages.
 

3581
01:17:35,920 --> 01:17:38,070
this doesn't work um kind of in stages.
So practically speaking what happens if

3582
01:17:38,070 --> 01:17:38,080
So practically speaking what happens if
 

3583
01:17:38,080 --> 01:17:41,110
So practically speaking what happens if
you try doing this

3584
01:17:41,110 --> 01:17:41,120
you try doing this
 

3585
01:17:41,120 --> 01:17:43,910
you try doing this
is

3586
01:17:43,910 --> 01:17:43,920
is
 

3587
01:17:43,920 --> 01:17:45,430
is
if

3588
01:17:45,430 --> 01:17:45,440
if
 

3589
01:17:45,440 --> 01:17:50,870
if
Z too small

3590
01:17:50,870 --> 01:17:50,880

 

3591
01:17:50,880 --> 01:17:53,030

get

3592
01:17:53,030 --> 01:17:53,040
get
 

3593
01:17:53,040 --> 01:17:54,550
get
blurry

3594
01:17:54,550 --> 01:17:54,560
blurry
 

3595
01:17:54,560 --> 01:17:56,630
blurry
junk.

3596
01:17:56,630 --> 01:17:56,640
junk.
 

3597
01:17:56,640 --> 01:17:59,669
junk.
Okay. So

3598
01:17:59,669 --> 01:17:59,679
Okay. So
 

3599
01:17:59,679 --> 01:18:01,590
Okay. So
if you make Z too small D, too low

3600
01:18:01,590 --> 01:18:01,600
if you make Z too small D, too low
 

3601
01:18:01,600 --> 01:18:03,350
if you make Z too small D, too low
dimensional,

3602
01:18:03,350 --> 01:18:03,360
dimensional,
 

3603
01:18:03,360 --> 01:18:05,510
dimensional,
like one-dimensional, you'll get

3604
01:18:05,510 --> 01:18:05,520
like one-dimensional, you'll get
 

3605
01:18:05,520 --> 01:18:07,910
like one-dimensional, you'll get
something that looks like an image, but

3606
01:18:07,910 --> 01:18:07,920
something that looks like an image, but
 

3607
01:18:07,920 --> 01:18:10,470
something that looks like an image, but
it'll be like a grayscale gradient of

3608
01:18:10,470 --> 01:18:10,480
it'll be like a grayscale gradient of
 

3609
01:18:10,480 --> 01:18:13,750
it'll be like a grayscale gradient of
some kind. That's it. If you make it

3610
01:18:13,750 --> 01:18:13,760
some kind. That's it. If you make it
 

3611
01:18:13,760 --> 01:18:15,270
some kind. That's it. If you make it
like two-dimensional, you'll get like

3612
01:18:15,270 --> 01:18:15,280
like two-dimensional, you'll get like
 

3613
01:18:15,280 --> 01:18:17,350
like two-dimensional, you'll get like
some blob,

3614
01:18:17,350 --> 01:18:17,360
some blob,
 

3615
01:18:17,360 --> 01:18:20,149
some blob,
right? It won't be his,

3616
01:18:20,149 --> 01:18:20,159
right? It won't be his,
 

3617
01:18:20,159 --> 01:18:22,709
right? It won't be his,
but it will be like blob. It will not be

3618
01:18:22,709 --> 01:18:22,719
but it will be like blob. It will not be
 

3619
01:18:22,719 --> 01:18:25,430
but it will be like blob. It will not be
a cat, never a cat. And then you keep

3620
01:18:25,430 --> 01:18:25,440
a cat, never a cat. And then you keep
 

3621
01:18:25,440 --> 01:18:27,510
a cat, never a cat. And then you keep
increasing the dimension of Z and you'll

3622
01:18:27,510 --> 01:18:27,520
increasing the dimension of Z and you'll
 

3623
01:18:27,520 --> 01:18:29,590
increasing the dimension of Z and you'll
transition from blobby blobby blob blob

3624
01:18:29,590 --> 01:18:29,600
transition from blobby blobby blob blob
 

3625
01:18:29,600 --> 01:18:32,709
transition from blobby blobby blob blob
blob to hiss,

3626
01:18:32,709 --> 01:18:32,719
blob to hiss,
 

3627
01:18:32,719 --> 01:18:34,950
blob to hiss,
right? You'll never go you along the

3628
01:18:34,950 --> 01:18:34,960
right? You'll never go you along the
 

3629
01:18:34,960 --> 01:18:46,470
right? You'll never go you along the
path. You'll never be a cat. Okay.

3630
01:18:46,470 --> 01:18:46,480

 

3631
01:18:46,480 --> 01:18:49,030

Okay. Like randomly generating Z will

3632
01:18:49,030 --> 01:18:49,040
Okay. Like randomly generating Z will
 

3633
01:18:49,040 --> 01:18:50,709
Okay. Like randomly generating Z will
not give you something that looks like

3634
01:18:50,709 --> 01:18:50,719
not give you something that looks like
 

3635
01:18:50,719 --> 01:18:54,310
not give you something that looks like
an image. So

3636
01:18:54,310 --> 01:18:54,320
an image. So
 

3637
01:18:54,320 --> 01:18:58,070
an image. So
this brings in the idea of

3638
01:18:58,070 --> 01:18:58,080
this brings in the idea of
 

3639
01:18:58,080 --> 01:19:02,149
this brings in the idea of
why not like why isn't this working?

3640
01:19:02,149 --> 01:19:02,159
why not like why isn't this working?
 

3641
01:19:02,159 --> 01:19:06,550
why not like why isn't this working?
So the reason it's not working is that

3642
01:19:06,550 --> 01:19:06,560
So the reason it's not working is that
 

3643
01:19:06,560 --> 01:19:09,189
So the reason it's not working is that
all of the images that exist. So here

3644
01:19:09,189 --> 01:19:09,199
all of the images that exist. So here
 

3645
01:19:09,199 --> 01:19:13,110
all of the images that exist. So here
draw schematic picture

3646
01:19:13,110 --> 01:19:13,120

 

3647
01:19:13,120 --> 01:19:20,229

I'll draw it here.

3648
01:19:20,229 --> 01:19:20,239

 

3649
01:19:20,239 --> 01:19:29,189

This thing is all possible images

3650
01:19:29,189 --> 01:19:29,199

 

3651
01:19:29,199 --> 01:19:32,470

in here.

3652
01:19:32,470 --> 01:19:32,480
in here.
 

3653
01:19:32,480 --> 01:19:38,070
in here.
There's a very thin set

3654
01:19:38,070 --> 01:19:38,080

 

3655
01:19:38,080 --> 01:19:42,310

natural images

3656
01:19:42,310 --> 01:19:42,320

 

3657
01:19:42,320 --> 01:19:45,910

are tiny thin set in this vast space of

3658
01:19:45,910 --> 01:19:45,920
are tiny thin set in this vast space of
 

3659
01:19:45,920 --> 01:19:49,189
are tiny thin set in this vast space of
all images.

3660
01:19:49,189 --> 01:19:49,199
all images.
 

3661
01:19:49,199 --> 01:19:51,030
all images.
And this is called the manifold

3662
01:19:51,030 --> 01:19:51,040
And this is called the manifold
 

3663
01:19:51,040 --> 01:19:53,189
And this is called the manifold
hypothesis in machine learning. Lots of

3664
01:19:53,189 --> 01:19:53,199
hypothesis in machine learning. Lots of
 

3665
01:19:53,199 --> 01:19:55,350
hypothesis in machine learning. Lots of
natural data has this behavior that it

3666
01:19:55,350 --> 01:19:55,360
natural data has this behavior that it
 

3667
01:19:55,360 --> 01:19:57,350
natural data has this behavior that it
lives on a very thin set in a very large

3668
01:19:57,350 --> 01:19:57,360
lives on a very thin set in a very large
 

3669
01:19:57,360 --> 01:20:00,950
lives on a very thin set in a very large
embedding space. And the autoenccoder is

3670
01:20:00,950 --> 01:20:00,960
embedding space. And the autoenccoder is
 

3671
01:20:00,960 --> 01:20:07,030
embedding space. And the autoenccoder is
only fed things on this thin set.

3672
01:20:07,030 --> 01:20:07,040

 

3673
01:20:07,040 --> 01:20:09,430

But

3674
01:20:09,430 --> 01:20:09,440
But
 

3675
01:20:09,440 --> 01:20:13,270
But
if the dimensionality of the encoder, if

3676
01:20:13,270 --> 01:20:13,280
if the dimensionality of the encoder, if
 

3677
01:20:13,280 --> 01:20:17,990
if the dimensionality of the encoder, if
the Z the latent space is small relative

3678
01:20:17,990 --> 01:20:18,000
the Z the latent space is small relative
 

3679
01:20:18,000 --> 01:20:20,070
the Z the latent space is small relative
to the intrinsic dimensionality of

3680
01:20:20,070 --> 01:20:20,080
to the intrinsic dimensionality of
 

3681
01:20:20,080 --> 01:20:22,229
to the intrinsic dimensionality of
images, the m the manifold dimension,

3682
01:20:22,229 --> 01:20:22,239
images, the m the manifold dimension,
 

3683
01:20:22,239 --> 01:20:23,669
images, the m the manifold dimension,
then you'll get these things that are

3684
01:20:23,669 --> 01:20:23,679
then you'll get these things that are
 

3685
01:20:23,679 --> 01:20:25,990
then you'll get these things that are
blobby. They can't represent the

3686
01:20:25,990 --> 01:20:26,000
blobby. They can't represent the
 

3687
01:20:26,000 --> 01:20:28,709
blobby. They can't represent the
richness of an actual image. If the

3688
01:20:28,709 --> 01:20:28,719
richness of an actual image. If the
 

3689
01:20:28,719 --> 01:20:30,950
richness of an actual image. If the
dimensionality is bigger than dimension

3690
01:20:30,950 --> 01:20:30,960
dimensionality is bigger than dimension
 

3691
01:20:30,960 --> 01:20:32,870
dimensionality is bigger than dimension
of the manifold,

3692
01:20:32,870 --> 01:20:32,880
of the manifold,
 

3693
01:20:32,880 --> 01:20:36,310
of the manifold,
that's the thing, right?

3694
01:20:36,310 --> 01:20:36,320
that's the thing, right?
 

3695
01:20:36,320 --> 01:20:39,110
that's the thing, right?
In terms of volume, a piece of paper

3696
01:20:39,110 --> 01:20:39,120
In terms of volume, a piece of paper
 

3697
01:20:39,120 --> 01:20:41,750
In terms of volume, a piece of paper
occupies a tiny volume in

3698
01:20:41,750 --> 01:20:41,760
occupies a tiny volume in
 

3699
01:20:41,760 --> 01:20:44,149
occupies a tiny volume in
threedimensional space. That's the

3700
01:20:44,149 --> 01:20:44,159
threedimensional space. That's the
 

3701
01:20:44,159 --> 01:20:47,350
threedimensional space. That's the
problem with dimensions. That once you

3702
01:20:47,350 --> 01:20:47,360
problem with dimensions. That once you
 

3703
01:20:47,360 --> 01:20:50,470
problem with dimensions. That once you
go to a dimension that's higher,

3704
01:20:50,470 --> 01:20:50,480
go to a dimension that's higher,
 

3705
01:20:50,480 --> 01:20:53,189
go to a dimension that's higher,
the occupied

3706
01:20:53,189 --> 01:20:53,199
the occupied
 

3707
01:20:53,199 --> 01:20:54,870
the occupied
by this lower dimensional object is

3708
01:20:54,870 --> 01:20:54,880
by this lower dimensional object is
 

3709
01:20:54,880 --> 01:20:57,030
by this lower dimensional object is
zero.

3710
01:20:57,030 --> 01:20:57,040
zero.
 

3711
01:20:57,040 --> 01:20:58,630
zero.
And that's the problem. the instant you

3712
01:20:58,630 --> 01:20:58,640
And that's the problem. the instant you
 

3713
01:20:58,640 --> 01:21:01,510
And that's the problem. the instant you
get to Z to a larger dimension. Most of

3714
01:21:01,510 --> 01:21:01,520
get to Z to a larger dimension. Most of
 

3715
01:21:01,520 --> 01:21:05,590
get to Z to a larger dimension. Most of
the space in Z can't land on

3716
01:21:05,590 --> 01:21:05,600
the space in Z can't land on
 

3717
01:21:05,600 --> 01:21:07,110
the space in Z can't land on
this is a this is a onetoone mapping,

3718
01:21:07,110 --> 01:21:07,120
this is a this is a onetoone mapping,
 

3719
01:21:07,120 --> 01:21:08,470
this is a this is a onetoone mapping,
right? So this is not going to land on

3720
01:21:08,470 --> 01:21:08,480
right? So this is not going to land on
 

3721
01:21:08,480 --> 01:21:10,390
right? So this is not going to land on
something that makes sense here. And

3722
01:21:10,390 --> 01:21:10,400
something that makes sense here. And
 

3723
01:21:10,400 --> 01:21:11,590
something that makes sense here. And
there's nothing in training that's

3724
01:21:11,590 --> 01:21:11,600
there's nothing in training that's
 

3725
01:21:11,600 --> 01:21:13,830
there's nothing in training that's
forcing it to.

3726
01:21:13,830 --> 01:21:13,840
forcing it to.
 

3727
01:21:13,840 --> 01:21:16,550
forcing it to.
And so that's the problem. So you have

3728
01:21:16,550 --> 01:21:16,560
And so that's the problem. So you have
 

3729
01:21:16,560 --> 01:21:19,189
And so that's the problem. So you have
to deal with the problem that

3730
01:21:19,189 --> 01:21:19,199
to deal with the problem that
 

3731
01:21:19,199 --> 01:21:21,910
to deal with the problem that
this thing an autoenccoder during

3732
01:21:21,910 --> 01:21:21,920
this thing an autoenccoder during
 

3733
01:21:21,920 --> 01:21:24,709
this thing an autoenccoder during
training has nothing forcing it to try

3734
01:21:24,709 --> 01:21:24,719
training has nothing forcing it to try
 

3735
01:21:24,719 --> 01:21:26,630
training has nothing forcing it to try
to make be good at generation

3736
01:21:26,630 --> 01:21:26,640
to make be good at generation
 

3737
01:21:26,640 --> 01:21:28,070
to make be good at generation
generation.

3738
01:21:28,070 --> 01:21:28,080
generation.
 

3739
01:21:28,080 --> 01:21:30,870
generation.
And so it won't because the dimensions

3740
01:21:30,870 --> 01:21:30,880
And so it won't because the dimensions
 

3741
01:21:30,880 --> 01:21:34,630
And so it won't because the dimensions
won't match match up correctly.

3742
01:21:34,630 --> 01:21:34,640
won't match match up correctly.
 

3743
01:21:34,640 --> 01:21:39,270
won't match match up correctly.
So to do this one key idea to understand

3744
01:21:39,270 --> 01:21:39,280
So to do this one key idea to understand
 

3745
01:21:39,280 --> 01:21:40,550
So to do this one key idea to understand
is something called the variational

3746
01:21:40,550 --> 01:21:40,560
is something called the variational
 

3747
01:21:40,560 --> 01:21:43,189
is something called the variational
autoenccoder approach.

3748
01:21:43,189 --> 01:21:43,199
autoenccoder approach.
 

3749
01:21:43,199 --> 01:21:45,189
autoenccoder approach.
And

3750
01:21:45,189 --> 01:21:45,199
And
 

3751
01:21:45,199 --> 01:21:46,870
And
the first aspect of variational

3752
01:21:46,870 --> 01:21:46,880
the first aspect of variational
 

3753
01:21:46,880 --> 01:21:50,310
the first aspect of variational
autoenccoder approach is I want to use

3754
01:21:50,310 --> 01:21:50,320
autoenccoder approach is I want to use
 

3755
01:21:50,320 --> 01:21:52,390
autoenccoder approach is I want to use
this for generation.

3756
01:21:52,390 --> 01:21:52,400
this for generation.
 

3757
01:21:52,400 --> 01:21:55,350
this for generation.
Okay, which means I need to make at

3758
01:21:55,350 --> 01:21:55,360
Okay, which means I need to make at
 

3759
01:21:55,360 --> 01:21:57,110
Okay, which means I need to make at
generation time, I'm going to be have Z

3760
01:21:57,110 --> 01:21:57,120
generation time, I'm going to be have Z
 

3761
01:21:57,120 --> 01:21:58,790
generation time, I'm going to be have Z
be random.

3762
01:21:58,790 --> 01:21:58,800
be random.
 

3763
01:21:58,800 --> 01:22:00,229
be random.
And the general rule is if you're going

3764
01:22:00,229 --> 01:22:00,239
And the general rule is if you're going
 

3765
01:22:00,239 --> 01:22:02,310
And the general rule is if you're going
to be doing something at test time, you

3766
01:22:02,310 --> 01:22:02,320
to be doing something at test time, you
 

3767
01:22:02,320 --> 01:22:05,350
to be doing something at test time, you
should be doing it at training time.

3768
01:22:05,350 --> 01:22:05,360
should be doing it at training time.
 

3769
01:22:05,360 --> 01:22:08,709
should be doing it at training time.
General machine learning philosophy. So

3770
01:22:08,709 --> 01:22:08,719
General machine learning philosophy. So
 

3771
01:22:08,719 --> 01:22:11,430
General machine learning philosophy. So
if I want Z to be random at test time, Z

3772
01:22:11,430 --> 01:22:11,440
if I want Z to be random at test time, Z
 

3773
01:22:11,440 --> 01:22:15,990
if I want Z to be random at test time, Z
has to be random during training too.

3774
01:22:15,990 --> 01:22:16,000
has to be random during training too.
 

3775
01:22:16,000 --> 01:22:17,669
has to be random during training too.
Okay.

3776
01:22:17,669 --> 01:22:17,679
Okay.
 

3777
01:22:17,679 --> 01:22:19,430
Okay.
Now,

3778
01:22:19,430 --> 01:22:19,440
Now,
 

3779
01:22:19,440 --> 01:22:21,830
Now,
second thing, I'm going to be sampling Z

3780
01:22:21,830 --> 01:22:21,840
second thing, I'm going to be sampling Z
 

3781
01:22:21,840 --> 01:22:24,709
second thing, I'm going to be sampling Z
from some distribution.

3782
01:22:24,709 --> 01:22:24,719
from some distribution.
 

3783
01:22:24,719 --> 01:22:26,229
from some distribution.
I can't say I'm going to sample Z from

3784
01:22:26,229 --> 01:22:26,239
I can't say I'm going to sample Z from
 

3785
01:22:26,239 --> 01:22:28,070
I can't say I'm going to sample Z from
the distribution of images because I

3786
01:22:28,070 --> 01:22:28,080
the distribution of images because I
 

3787
01:22:28,080 --> 01:22:29,270
the distribution of images because I
don't have the distribution of images.

3788
01:22:29,270 --> 01:22:29,280
don't have the distribution of images.
 

3789
01:22:29,280 --> 01:22:30,709
don't have the distribution of images.
That's the whole problem. So, I have to

3790
01:22:30,709 --> 01:22:30,719
That's the whole problem. So, I have to
 

3791
01:22:30,719 --> 01:22:33,750
That's the whole problem. So, I have to
sample Z from some known distribution.

3792
01:22:33,750 --> 01:22:33,760
sample Z from some known distribution.
 

3793
01:22:33,760 --> 01:22:36,470
sample Z from some known distribution.
However, during autoenccoding training,

3794
01:22:36,470 --> 01:22:36,480
However, during autoenccoding training,
 

3795
01:22:36,480 --> 01:22:38,950
However, during autoenccoding training,
there's nothing that's controlling the

3796
01:22:38,950 --> 01:22:38,960
there's nothing that's controlling the
 

3797
01:22:38,960 --> 01:22:41,590
there's nothing that's controlling the
distribution of Z.

3798
01:22:41,590 --> 01:22:41,600
distribution of Z.
 

3799
01:22:41,600 --> 01:22:43,110
distribution of Z.
Z is just an intermediate to get to

3800
01:22:43,110 --> 01:22:43,120
Z is just an intermediate to get to
 

3801
01:22:43,120 --> 01:22:45,990
Z is just an intermediate to get to
Xhat. There's a loss on Xhat and X in

3802
01:22:45,990 --> 01:22:46,000
Xhat. There's a loss on Xhat and X in
 

3803
01:22:46,000 --> 01:22:48,709
Xhat. There's a loss on Xhat and X in
autoenccoding. So I need to add a loss

3804
01:22:48,709 --> 01:22:48,719
autoenccoding. So I need to add a loss
 

3805
01:22:48,719 --> 01:22:51,030
autoenccoding. So I need to add a loss
on the distribution of Z to make it

3806
01:22:51,030 --> 01:22:51,040
on the distribution of Z to make it
 

3807
01:22:51,040 --> 01:22:52,790
on the distribution of Z to make it
close to the distribution that I want to

3808
01:22:52,790 --> 01:22:52,800
close to the distribution that I want to
 

3809
01:22:52,800 --> 01:22:55,750
close to the distribution that I want to
sample from. Why? Because when I use the

3810
01:22:55,750 --> 01:22:55,760
sample from. Why? Because when I use the
 

3811
01:22:55,760 --> 01:22:59,350
sample from. Why? Because when I use the
decoder at generation time, I want to be

3812
01:22:59,350 --> 01:22:59,360
decoder at generation time, I want to be
 

3813
01:22:59,360 --> 01:23:01,189
decoder at generation time, I want to be
hitting it in a way that it was actually

3814
01:23:01,189 --> 01:23:01,199
hitting it in a way that it was actually
 

3815
01:23:01,199 --> 01:23:02,950
hitting it in a way that it was actually
trained and not with something that is

3816
01:23:02,950 --> 01:23:02,960
trained and not with something that is
 

3817
01:23:02,960 --> 01:23:04,790
trained and not with something that is
never seen during training at all,

3818
01:23:04,790 --> 01:23:04,800
never seen during training at all,
 

3819
01:23:04,800 --> 01:23:07,830
never seen during training at all,
completely alien.

3820
01:23:07,830 --> 01:23:07,840
completely alien.
 

3821
01:23:07,840 --> 01:23:08,870
completely alien.
Because if hit with something completely

3822
01:23:08,870 --> 01:23:08,880
Because if hit with something completely
 

3823
01:23:08,880 --> 01:23:13,510
Because if hit with something completely
alien, I'll get some random hiss out.

3824
01:23:13,510 --> 01:23:13,520

 

3825
01:23:13,520 --> 01:23:16,149

And then so those are my two main

3826
01:23:16,149 --> 01:23:16,159
And then so those are my two main
 

3827
01:23:16,159 --> 01:23:18,310
And then so those are my two main
ingredients but I have to also train it

3828
01:23:18,310 --> 01:23:18,320
ingredients but I have to also train it
 

3829
01:23:18,320 --> 01:23:19,750
ingredients but I have to also train it
which means it has to actually work with

3830
01:23:19,750 --> 01:23:19,760
which means it has to actually work with
 

3831
01:23:19,760 --> 01:23:22,470
which means it has to actually work with
SGD for training.

3832
01:23:22,470 --> 01:23:22,480
SGD for training.
 

3833
01:23:22,480 --> 01:23:22,790
SGD for training.
>> Yeah.

3834
01:23:22,790 --> 01:23:22,800
>> Yeah.
 

3835
01:23:22,800 --> 01:23:25,430
>> Yeah.
>> Yeah. So the main idea here is to let Z

3836
01:23:25,430 --> 01:23:25,440
>> Yeah. So the main idea here is to let Z
 

3837
01:23:25,440 --> 01:23:27,990
>> Yeah. So the main idea here is to let Z
land in that small manifold that we

3838
01:23:27,990 --> 01:23:28,000
land in that small manifold that we
 

3839
01:23:28,000 --> 01:23:29,510
land in that small manifold that we
specified.

3840
01:23:29,510 --> 01:23:29,520
specified.
 

3841
01:23:29,520 --> 01:23:31,350
specified.
>> So it's a good question. The question is

3842
01:23:31,350 --> 01:23:31,360
>> So it's a good question. The question is
 

3843
01:23:31,360 --> 01:23:33,270
>> So it's a good question. The question is
is the idea to make Z land in the

3844
01:23:33,270 --> 01:23:33,280
is the idea to make Z land in the
 

3845
01:23:33,280 --> 01:23:34,950
is the idea to make Z land in the
manifold? The answer is actually

3846
01:23:34,950 --> 01:23:34,960
manifold? The answer is actually
 

3847
01:23:34,960 --> 01:23:36,470
manifold? The answer is actually
different.

3848
01:23:36,470 --> 01:23:36,480
different.
 

3849
01:23:36,480 --> 01:23:40,390
different.
The answer is I want to make the decoder

3850
01:23:40,390 --> 01:23:40,400
The answer is I want to make the decoder
 

3851
01:23:40,400 --> 01:23:45,350
The answer is I want to make the decoder
D be robust to random noise in Z. So

3852
01:23:45,350 --> 01:23:45,360
D be robust to random noise in Z. So
 

3853
01:23:45,360 --> 01:23:47,669
D be robust to random noise in Z. So
that even if we don't land exactly where

3854
01:23:47,669 --> 01:23:47,679
that even if we don't land exactly where
 

3855
01:23:47,679 --> 01:23:51,030
that even if we don't land exactly where
we need to land, the decoder pulls us in

3856
01:23:51,030 --> 01:23:51,040
we need to land, the decoder pulls us in
 

3857
01:23:51,040 --> 01:23:53,750
we need to land, the decoder pulls us in
to where we need to be.

3858
01:23:53,750 --> 01:23:53,760
to where we need to be.
 

3859
01:23:53,760 --> 01:23:55,830
to where we need to be.
Whereas if we just have an autoenccoder,

3860
01:23:55,830 --> 01:23:55,840
Whereas if we just have an autoenccoder,
 

3861
01:23:55,840 --> 01:23:57,189
Whereas if we just have an autoenccoder,
there's nothing pressing the decoder to

3862
01:23:57,189 --> 01:23:57,199
there's nothing pressing the decoder to
 

3863
01:23:57,199 --> 01:24:01,350
there's nothing pressing the decoder to
do that. So I am out of time. Um, I'd

3864
01:24:01,350 --> 01:24:01,360
do that. So I am out of time. Um, I'd
 

3865
01:24:01,360 --> 01:24:02,550
do that. So I am out of time. Um, I'd
wanted to be able to talk more about

3866
01:24:02,550 --> 01:24:02,560
wanted to be able to talk more about
 

3867
01:24:02,560 --> 01:24:04,470
wanted to be able to talk more about
autoenccoders, but I will do that at

3868
01:24:04,470 --> 01:24:04,480
autoenccoders, but I will do that at
 

3869
01:24:04,480 --> 01:24:09,270
autoenccoders, but I will do that at
VAES next time. Um, we will

3870
01:24:09,270 --> 01:24:09,280
VAES next time. Um, we will
 

3871
01:24:09,280 --> 01:24:11,430
VAES next time. Um, we will
I will on Thursday I'll talk about it.

3872
01:24:11,430 --> 01:24:11,440
I will on Thursday I'll talk about it.
 

3873
01:24:11,440 --> 01:24:13,990
I will on Thursday I'll talk about it.
Discussion will not talk about uh VAEs.

3874
01:24:13,990 --> 01:24:14,000
Discussion will not talk about uh VAEs.
 

3875
01:24:14,000 --> 01:24:15,590
Discussion will not talk about uh VAEs.
Uh, but discussion will talk about

3876
01:24:15,590 --> 01:24:15,600
Uh, but discussion will talk about
 

3877
01:24:15,600 --> 01:24:16,470
Uh, but discussion will talk about
catastrophic forgetting.

3878
01:24:16,470 --> 01:24:16,480
catastrophic forgetting.
 

3879
01:24:16,480 --> 01:24:17,669
catastrophic forgetting.
>> You announced the survey.

3880
01:24:17,669 --> 01:24:17,679
>> You announced the survey.
 

3881
01:24:17,679 --> 01:24:18,790
>> You announced the survey.
>> Yeah, I announced the survey at the

3882
01:24:18,790 --> 01:24:18,800
>> Yeah, I announced the survey at the
 

3883
01:24:18,800 --> 01:24:21,590
>> Yeah, I announced the survey at the
beginning. Yeah. So, everyone please go

3884
01:24:21,590 --> 01:24:21,600
beginning. Yeah. So, everyone please go
 

3885
01:24:21,600 --> 01:24:24,229
beginning. Yeah. So, everyone please go
fill out the survey. Very important. So,

3886
01:24:24,229 --> 01:24:24,239
fill out the survey. Very important. So,
 

3887
01:24:24,239 --> 01:24:27,430
fill out the survey. Very important. So,
I have office hours now. I will uh take

3888
01:24:27,430 --> 01:24:27,440
I have office hours now. I will uh take
 

3889
01:24:27,440 --> 01:24:29,110
I have office hours now. I will uh take
questions on lecture and projects and

3890
01:24:29,110 --> 01:24:29,120
questions on lecture and projects and
 

3891
01:24:29,120 --> 01:24:31,760
questions on lecture and projects and
stuff.

