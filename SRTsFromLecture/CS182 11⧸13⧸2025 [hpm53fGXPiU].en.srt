1
00:00:00,080 --> 00:00:03,909

confusion about is that when you uh when

2
00:00:03,909 --> 00:00:03,919
confusion about is that when you uh when
 

3
00:00:03,919 --> 00:00:05,269
confusion about is that when you uh when
you compute gradients so this cross

4
00:00:05,269 --> 00:00:05,279
you compute gradients so this cross
 

5
00:00:05,279 --> 00:00:07,030
you compute gradients so this cross
entropy loss is generating a gradient

6
00:00:07,030 --> 00:00:07,040
entropy loss is generating a gradient
 

7
00:00:07,040 --> 00:00:09,990
entropy loss is generating a gradient
what happens the gradient comes down for

8
00:00:09,990 --> 00:00:10,000
what happens the gradient comes down for
 

9
00:00:10,000 --> 00:00:12,070
what happens the gradient comes down for
a full fine tune it comes down hits

10
00:00:12,070 --> 00:00:12,080
a full fine tune it comes down hits
 

11
00:00:12,080 --> 00:00:13,509
a full fine tune it comes down hits
everything here but because there's

12
00:00:13,509 --> 00:00:13,519
everything here but because there's
 

13
00:00:13,519 --> 00:00:16,470
everything here but because there's
attention blocks connecting across time

14
00:00:16,470 --> 00:00:16,480
attention blocks connecting across time
 

15
00:00:16,480 --> 00:00:20,390
attention blocks connecting across time
the gradient also flows back this way

16
00:00:20,390 --> 00:00:20,400
the gradient also flows back this way
 

17
00:00:20,400 --> 00:00:23,189
the gradient also flows back this way
okay so the attention is going making

18
00:00:23,189 --> 00:00:23,199
okay so the attention is going making
 

19
00:00:23,199 --> 00:00:27,029
okay so the attention is going making
the gradients flow back to everything

20
00:00:27,029 --> 00:00:27,039
the gradients flow back to everything
 

21
00:00:27,039 --> 00:00:30,790
the gradients flow back to everything
so just because there's no loss

22
00:00:30,790 --> 00:00:30,800
so just because there's no loss
 

23
00:00:30,800 --> 00:00:33,190
so just because there's no loss
on this question part doesn't mean that

24
00:00:33,190 --> 00:00:33,200
on this question part doesn't mean that
 

25
00:00:33,200 --> 00:00:35,590
on this question part doesn't mean that
the gradients aren't flowing over here.

26
00:00:35,590 --> 00:00:35,600
the gradients aren't flowing over here.
 

27
00:00:35,600 --> 00:00:36,950
the gradients aren't flowing over here.
Okay, that was like a one little point

28
00:00:36,950 --> 00:00:36,960
Okay, that was like a one little point
 

29
00:00:36,960 --> 00:00:37,910
Okay, that was like a one little point
of confusion. I want to make sure

30
00:00:37,910 --> 00:00:37,920
of confusion. I want to make sure
 

31
00:00:37,920 --> 00:00:41,830
of confusion. I want to make sure
everyone wasn't confused by that.

32
00:00:41,830 --> 00:00:41,840
everyone wasn't confused by that.
 

33
00:00:41,840 --> 00:00:44,470
everyone wasn't confused by that.
Okay, so then we said, okay, we want to

34
00:00:44,470 --> 00:00:44,480
Okay, so then we said, okay, we want to
 

35
00:00:44,480 --> 00:00:46,069
Okay, so then we said, okay, we want to
think about fine-tuning or using a

36
00:00:46,069 --> 00:00:46,079
think about fine-tuning or using a
 

37
00:00:46,079 --> 00:00:47,990
think about fine-tuning or using a
pre-trained model for certain task, we

38
00:00:47,990 --> 00:00:48,000
pre-trained model for certain task, we
 

39
00:00:48,000 --> 00:00:49,350
pre-trained model for certain task, we
introduced this idea. So here I've kind

40
00:00:49,350 --> 00:00:49,360
introduced this idea. So here I've kind
 

41
00:00:49,360 --> 00:00:51,670
introduced this idea. So here I've kind
of labeled them with more like shaded

42
00:00:51,670 --> 00:00:51,680
of labeled them with more like shaded
 

43
00:00:51,680 --> 00:00:54,389
of labeled them with more like shaded
numbers, okay, uh than I did last time.

44
00:00:54,389 --> 00:00:54,399
numbers, okay, uh than I did last time.
 

45
00:00:54,399 --> 00:00:58,310
numbers, okay, uh than I did last time.
So we said because we know that these

46
00:00:58,310 --> 00:00:58,320
So we said because we know that these
 

47
00:00:58,320 --> 00:01:00,470
So we said because we know that these
large language models and other

48
00:01:00,470 --> 00:01:00,480
large language models and other
 

49
00:01:00,480 --> 00:01:04,149
large language models and other
generative models uh can exhibit this in

50
00:01:04,149 --> 00:01:04,159
generative models uh can exhibit this in
 

51
00:01:04,159 --> 00:01:08,149
generative models uh can exhibit this in
context learning behavior. It's possible

52
00:01:08,149 --> 00:01:08,159
context learning behavior. It's possible
 

53
00:01:08,159 --> 00:01:10,390
context learning behavior. It's possible
to have this pure prompting style of

54
00:01:10,390 --> 00:01:10,400
to have this pure prompting style of
 

55
00:01:10,400 --> 00:01:15,190
to have this pure prompting style of
using for doing a new task. Okay. So in

56
00:01:15,190 --> 00:01:15,200
using for doing a new task. Okay. So in
 

57
00:01:15,200 --> 00:01:18,149
using for doing a new task. Okay. So in
kind of pure prompting using just plain

58
00:01:18,149 --> 00:01:18,159
kind of pure prompting using just plain
 

59
00:01:18,159 --> 00:01:20,149
kind of pure prompting using just plain
ordinary incontext learning pure

60
00:01:20,149 --> 00:01:20,159
ordinary incontext learning pure
 

61
00:01:20,159 --> 00:01:21,590
ordinary incontext learning pure
prompting is you give a bunch of

62
00:01:21,590 --> 00:01:21,600
prompting is you give a bunch of
 

63
00:01:21,600 --> 00:01:23,109
prompting is you give a bunch of
examples and then you ask it for

64
00:01:23,109 --> 00:01:23,119
examples and then you ask it for
 

65
00:01:23,119 --> 00:01:26,469
examples and then you ask it for
something new uh as the way that you uh

66
00:01:26,469 --> 00:01:26,479
something new uh as the way that you uh
 

67
00:01:26,479 --> 00:01:29,510
something new uh as the way that you uh
do a new task. But after a model has

68
00:01:29,510 --> 00:01:29,520
do a new task. But after a model has
 

69
00:01:29,520 --> 00:01:32,069
do a new task. But after a model has
been instruction tuned, you can also

70
00:01:32,069 --> 00:01:32,079
been instruction tuned, you can also
 

71
00:01:32,079 --> 00:01:35,590
been instruction tuned, you can also
give a combination of instructions

72
00:01:35,590 --> 00:01:35,600
give a combination of instructions
 

73
00:01:35,600 --> 00:01:37,190
give a combination of instructions
and

74
00:01:37,190 --> 00:01:37,200
and
 

75
00:01:37,200 --> 00:01:40,149
and
examples and have it do the new task.

76
00:01:40,149 --> 00:01:40,159
examples and have it do the new task.
 

77
00:01:40,159 --> 00:01:42,390
examples and have it do the new task.
And in fact, optimizing the prompts is

78
00:01:42,390 --> 00:01:42,400
And in fact, optimizing the prompts is
 

79
00:01:42,400 --> 00:01:43,830
And in fact, optimizing the prompts is
something that can be done. People saw

80
00:01:43,830 --> 00:01:43,840
something that can be done. People saw
 

81
00:01:43,840 --> 00:01:44,710
something that can be done. People saw
that they could do this as humans. We

82
00:01:44,710 --> 00:01:44,720
that they could do this as humans. We
 

83
00:01:44,720 --> 00:01:46,230
that they could do this as humans. We
talked last time about there are

84
00:01:46,230 --> 00:01:46,240
talked last time about there are
 

85
00:01:46,240 --> 00:01:48,710
talked last time about there are
packages out there that will do uh

86
00:01:48,710 --> 00:01:48,720
packages out there that will do uh
 

87
00:01:48,720 --> 00:01:50,550
packages out there that will do uh
prompt optimization

88
00:01:50,550 --> 00:01:50,560
prompt optimization
 

89
00:01:50,560 --> 00:01:52,870
prompt optimization
uh for you automatedly. Of course, you

90
00:01:52,870 --> 00:01:52,880
uh for you automatedly. Of course, you
 

91
00:01:52,880 --> 00:01:56,469
uh for you automatedly. Of course, you
have to give it a bunch of examples,

92
00:01:56,469 --> 00:01:56,479
have to give it a bunch of examples,
 

93
00:01:56,479 --> 00:01:57,749
have to give it a bunch of examples,
right? If you want to have it optimize

94
00:01:57,749 --> 00:01:57,759
right? If you want to have it optimize
 

95
00:01:57,759 --> 00:01:59,350
right? If you want to have it optimize
the prompt, you want any automated thing

96
00:01:59,350 --> 00:01:59,360
the prompt, you want any automated thing
 

97
00:01:59,360 --> 00:02:01,190
the prompt, you want any automated thing
to optimize something for you, it has to

98
00:02:01,190 --> 00:02:01,200
to optimize something for you, it has to
 

99
00:02:01,200 --> 00:02:03,910
to optimize something for you, it has to
be able to score itself. So it needs to

100
00:02:03,910 --> 00:02:03,920
be able to score itself. So it needs to
 

101
00:02:03,920 --> 00:02:05,990
be able to score itself. So it needs to
know you know some way of scoring itself

102
00:02:05,990 --> 00:02:06,000
know you know some way of scoring itself
 

103
00:02:06,000 --> 00:02:07,749
know you know some way of scoring itself
evaluating itself and then it can do

104
00:02:07,749 --> 00:02:07,759
evaluating itself and then it can do
 

105
00:02:07,759 --> 00:02:09,910
evaluating itself and then it can do
some kind of keeping the prompts be hard

106
00:02:09,910 --> 00:02:09,920
some kind of keeping the prompts be hard
 

107
00:02:09,920 --> 00:02:12,229
some kind of keeping the prompts be hard
prompts and do an optimization approach

108
00:02:12,229 --> 00:02:12,239
prompts and do an optimization approach
 

109
00:02:12,239 --> 00:02:14,390
prompts and do an optimization approach
that keeps things hard as you know

110
00:02:14,390 --> 00:02:14,400
that keeps things hard as you know
 

111
00:02:14,400 --> 00:02:16,309
that keeps things hard as you know
discrete sets of things for prompting

112
00:02:16,309 --> 00:02:16,319
discrete sets of things for prompting
 

113
00:02:16,319 --> 00:02:19,910
discrete sets of things for prompting
but still climbs a hill. Okay, so these

114
00:02:19,910 --> 00:02:19,920
but still climbs a hill. Okay, so these
 

115
00:02:19,920 --> 00:02:22,070
but still climbs a hill. Okay, so these
are packages one can use. And then we

116
00:02:22,070 --> 00:02:22,080
are packages one can use. And then we
 

117
00:02:22,080 --> 00:02:24,390
are packages one can use. And then we
shifted to the idea of saying okay,

118
00:02:24,390 --> 00:02:24,400
shifted to the idea of saying okay,
 

119
00:02:24,400 --> 00:02:25,589
shifted to the idea of saying okay,
we're not going to talk much about that

120
00:02:25,589 --> 00:02:25,599
we're not going to talk much about that
 

121
00:02:25,599 --> 00:02:27,990
we're not going to talk much about that
approach. We talk about soft prompting

122
00:02:27,990 --> 00:02:28,000
approach. We talk about soft prompting
 

123
00:02:28,000 --> 00:02:29,430
approach. We talk about soft prompting
which is we're going to assume that we

124
00:02:29,430 --> 00:02:29,440
which is we're going to assume that we
 

125
00:02:29,440 --> 00:02:31,430
which is we're going to assume that we
have access to the model and can run

126
00:02:31,430 --> 00:02:31,440
have access to the model and can run
 

127
00:02:31,440 --> 00:02:33,910
have access to the model and can run
gradients through it and we're going to

128
00:02:33,910 --> 00:02:33,920
gradients through it and we're going to
 

129
00:02:33,920 --> 00:02:35,990
gradients through it and we're going to
allow ourselves the flexibility of

130
00:02:35,990 --> 00:02:36,000
allow ourselves the flexibility of
 

131
00:02:36,000 --> 00:02:39,030
allow ourselves the flexibility of
having a prompt not be hard, meaning not

132
00:02:39,030 --> 00:02:39,040
having a prompt not be hard, meaning not
 

133
00:02:39,040 --> 00:02:41,910
having a prompt not be hard, meaning not
be a set of tokens. So the way we did

134
00:02:41,910 --> 00:02:41,920
be a set of tokens. So the way we did
 

135
00:02:41,920 --> 00:02:46,390
be a set of tokens. So the way we did
that was that we said, "Okay,

136
00:02:46,390 --> 00:02:46,400
that was that we said, "Okay,
 

137
00:02:46,400 --> 00:02:49,110
that was that we said, "Okay,
we're gonna So this is a a re a redrawn

138
00:02:49,110 --> 00:02:49,120
we're gonna So this is a a re a redrawn
 

139
00:02:49,120 --> 00:02:52,550
we're gonna So this is a a re a redrawn
version from last time.

140
00:02:52,550 --> 00:02:52,560
version from last time.
 

141
00:02:52,560 --> 00:02:54,949
version from last time.
We're still going to have the question

142
00:02:54,949 --> 00:02:54,959
We're still going to have the question
 

143
00:02:54,959 --> 00:02:57,430
We're still going to have the question
and a correct answer that we want it to

144
00:02:57,430 --> 00:02:57,440
and a correct answer that we want it to
 

145
00:02:57,440 --> 00:02:59,670
and a correct answer that we want it to
match. Okay, that's still going to be

146
00:02:59,670 --> 00:02:59,680
match. Okay, that's still going to be
 

147
00:02:59,680 --> 00:03:02,949
match. Okay, that's still going to be
there. We need something to optimize.

148
00:03:02,949 --> 00:03:02,959
there. We need something to optimize.
 

149
00:03:02,959 --> 00:03:04,309
there. We need something to optimize.
So the thing we're going to optimize is

150
00:03:04,309 --> 00:03:04,319
So the thing we're going to optimize is
 

151
00:03:04,319 --> 00:03:06,149
So the thing we're going to optimize is
think was the pre-prompt the thing that

152
00:03:06,149 --> 00:03:06,159
think was the pre-prompt the thing that
 

153
00:03:06,159 --> 00:03:08,790
think was the pre-prompt the thing that
goes before the question except instead

154
00:03:08,790 --> 00:03:08,800
goes before the question except instead
 

155
00:03:08,800 --> 00:03:11,110
goes before the question except instead
of forcing it to be hard we're just

156
00:03:11,110 --> 00:03:11,120
of forcing it to be hard we're just
 

157
00:03:11,120 --> 00:03:12,149
of forcing it to be hard we're just
going to allow ourselves to have a

158
00:03:12,149 --> 00:03:12,159
going to allow ourselves to have a
 

159
00:03:12,159 --> 00:03:15,430
going to allow ourselves to have a
custom embedding that we use. So however

160
00:03:15,430 --> 00:03:15,440
custom embedding that we use. So however
 

161
00:03:15,440 --> 00:03:18,309
custom embedding that we use. So however
long this is if it was 100 tokens long

162
00:03:18,309 --> 00:03:18,319
long this is if it was 100 tokens long
 

163
00:03:18,319 --> 00:03:20,070
long this is if it was 100 tokens long
it would just be a hundred different

164
00:03:20,070 --> 00:03:20,080
it would just be a hundred different
 

165
00:03:20,080 --> 00:03:22,630
it would just be a hundred different
embeddings one after the other and all

166
00:03:22,630 --> 00:03:22,640
embeddings one after the other and all
 

167
00:03:22,640 --> 00:03:25,830
embeddings one after the other and all
of them are learnable parameters. So

168
00:03:25,830 --> 00:03:25,840
of them are learnable parameters. So
 

169
00:03:25,840 --> 00:03:27,910
of them are learnable parameters. So
instead of going through an embedder you

170
00:03:27,910 --> 00:03:27,920
instead of going through an embedder you
 

171
00:03:27,920 --> 00:03:29,430
instead of going through an embedder you
can do it this way. Last time I talked

172
00:03:29,430 --> 00:03:29,440
can do it this way. Last time I talked
 

173
00:03:29,440 --> 00:03:30,630
can do it this way. Last time I talked
about it as if you want to think about

174
00:03:30,630 --> 00:03:30,640
about it as if you want to think about
 

175
00:03:30,640 --> 00:03:32,149
about it as if you want to think about
it, you can think about it as having a

176
00:03:32,149 --> 00:03:32,159
it, you can think about it as having a
 

177
00:03:32,159 --> 00:03:33,670
it, you can think about it as having a
hundred custom tokens that are going to

178
00:03:33,670 --> 00:03:33,680
hundred custom tokens that are going to
 

179
00:03:33,680 --> 00:03:35,190
hundred custom tokens that are going to
be learned and all the other tokens

180
00:03:35,190 --> 00:03:35,200
be learned and all the other tokens
 

181
00:03:35,200 --> 00:03:37,350
be learned and all the other tokens
embeddings is fixed. Both of these views

182
00:03:37,350 --> 00:03:37,360
embeddings is fixed. Both of these views
 

183
00:03:37,360 --> 00:03:39,270
embeddings is fixed. Both of these views
are identical.

184
00:03:39,270 --> 00:03:39,280
are identical.
 

185
00:03:39,280 --> 00:03:41,589
are identical.
And as we said before, as I just said,

186
00:03:41,589 --> 00:03:41,599
And as we said before, as I just said,
 

187
00:03:41,599 --> 00:03:43,910
And as we said before, as I just said,
the gradients when they come back, they

188
00:03:43,910 --> 00:03:43,920
the gradients when they come back, they
 

189
00:03:43,920 --> 00:03:46,550
the gradients when they come back, they
encounter the attention mechanism or in

190
00:03:46,550 --> 00:03:46,560
encounter the attention mechanism or in
 

191
00:03:46,560 --> 00:03:48,550
encounter the attention mechanism or in
a state space model, the state space

192
00:03:48,550 --> 00:03:48,560
a state space model, the state space
 

193
00:03:48,560 --> 00:03:51,910
a state space model, the state space
convolutions or the state uh recurrence

194
00:03:51,910 --> 00:03:51,920
convolutions or the state uh recurrence
 

195
00:03:51,920 --> 00:03:54,470
convolutions or the state uh recurrence
and then they can go back. So that I've

196
00:03:54,470 --> 00:03:54,480
and then they can go back. So that I've
 

197
00:03:54,480 --> 00:03:55,990
and then they can go back. So that I've
drawn in these highlighted colors of all

198
00:03:55,990 --> 00:03:56,000
drawn in these highlighted colors of all
 

199
00:03:56,000 --> 00:03:59,110
drawn in these highlighted colors of all
these gradients going back. Yes. Is the

200
00:03:59,110 --> 00:03:59,120
these gradients going back. Yes. Is the
 

201
00:03:59,120 --> 00:04:00,949
these gradients going back. Yes. Is the
transformer itself frozen and you're

202
00:04:00,949 --> 00:04:00,959
transformer itself frozen and you're
 

203
00:04:00,959 --> 00:04:02,869
transformer itself frozen and you're
just optimizing the prompt or are you

204
00:04:02,869 --> 00:04:02,879
just optimizing the prompt or are you
 

205
00:04:02,879 --> 00:04:04,390
just optimizing the prompt or are you
optimizing the whole thing?

206
00:04:04,390 --> 00:04:04,400
optimizing the whole thing?
 

207
00:04:04,400 --> 00:04:06,630
optimizing the whole thing?
>> Excellent question. The gradients are

208
00:04:06,630 --> 00:04:06,640
>> Excellent question. The gradients are
 

209
00:04:06,640 --> 00:04:07,990
>> Excellent question. The gradients are
flowing through everything because

210
00:04:07,990 --> 00:04:08,000
flowing through everything because
 

211
00:04:08,000 --> 00:04:11,110
flowing through everything because
that's how back prop works. However, the

212
00:04:11,110 --> 00:04:11,120
that's how back prop works. However, the
 

213
00:04:11,120 --> 00:04:14,149
that's how back prop works. However, the
only learnable parameters is this custom

214
00:04:14,149 --> 00:04:14,159
only learnable parameters is this custom
 

215
00:04:14,159 --> 00:04:16,629
only learnable parameters is this custom
embedding. When I say the only learnable

216
00:04:16,629 --> 00:04:16,639
embedding. When I say the only learnable
 

217
00:04:16,639 --> 00:04:17,990
embedding. When I say the only learnable
parameters are this custom embedding, it

218
00:04:17,990 --> 00:04:18,000
parameters are this custom embedding, it
 

219
00:04:18,000 --> 00:04:19,509
parameters are this custom embedding, it
means all the other parameters are

220
00:04:19,509 --> 00:04:19,519
means all the other parameters are
 

221
00:04:19,519 --> 00:04:24,469
means all the other parameters are
frozen. So this is a bit uh subtle. So

222
00:04:24,469 --> 00:04:24,479
frozen. So this is a bit uh subtle. So
 

223
00:04:24,479 --> 00:04:26,550
frozen. So this is a bit uh subtle. So
back prop if you remember is going to go

224
00:04:26,550 --> 00:04:26,560
back prop if you remember is going to go
 

225
00:04:26,560 --> 00:04:27,909
back prop if you remember is going to go
back and propagate the gradients

226
00:04:27,909 --> 00:04:27,919
back and propagate the gradients
 

227
00:04:27,919 --> 00:04:29,749
back and propagate the gradients
backwards and as it does this it

228
00:04:29,749 --> 00:04:29,759
backwards and as it does this it
 

229
00:04:29,759 --> 00:04:31,030
backwards and as it does this it
computes all these intermediate

230
00:04:31,030 --> 00:04:31,040
computes all these intermediate
 

231
00:04:31,040 --> 00:04:33,670
computes all these intermediate
gradients.

232
00:04:33,670 --> 00:04:33,680
gradients.
 

233
00:04:33,680 --> 00:04:35,030
gradients.
Now what we're going to do is we're not

234
00:04:35,030 --> 00:04:35,040
Now what we're going to do is we're not
 

235
00:04:35,040 --> 00:04:36,469
Now what we're going to do is we're not
going to take those gradients all the

236
00:04:36,469 --> 00:04:36,479
going to take those gradients all the
 

237
00:04:36,479 --> 00:04:39,030
going to take those gradients all the
way down to par the parameters there but

238
00:04:39,030 --> 00:04:39,040
way down to par the parameters there but
 

239
00:04:39,040 --> 00:04:40,629
way down to par the parameters there but
they might run through the parameters.

240
00:04:40,629 --> 00:04:40,639
they might run through the parameters.
 

241
00:04:40,639 --> 00:04:41,749
they might run through the parameters.
You might have to compute this for

242
00:04:41,749 --> 00:04:41,759
You might have to compute this for
 

243
00:04:41,759 --> 00:04:44,550
You might have to compute this for
backrop and but that's okay. Even if

244
00:04:44,550 --> 00:04:44,560
backrop and but that's okay. Even if
 

245
00:04:44,560 --> 00:04:46,070
backrop and but that's okay. Even if
they're computed for backrop we're not

246
00:04:46,070 --> 00:04:46,080
they're computed for backrop we're not
 

247
00:04:46,080 --> 00:04:47,350
they're computed for backrop we're not
going to use them. We're not going to

248
00:04:47,350 --> 00:04:47,360
going to use them. We're not going to
 

249
00:04:47,360 --> 00:04:48,629
going to use them. We're not going to
apply them. We're only going to apply

250
00:04:48,629 --> 00:04:48,639
apply them. We're only going to apply
 

251
00:04:48,639 --> 00:04:50,550
apply them. We're only going to apply
the updates to the parameters we're

252
00:04:50,550 --> 00:04:50,560
the updates to the parameters we're
 

253
00:04:50,560 --> 00:04:53,510
the updates to the parameters we're
learning which are these guys.

254
00:04:53,510 --> 00:04:53,520
learning which are these guys.
 

255
00:04:53,520 --> 00:04:55,830
learning which are these guys.
Okay, so the reason I bring this up is

256
00:04:55,830 --> 00:04:55,840
Okay, so the reason I bring this up is
 

257
00:04:55,840 --> 00:04:58,390
Okay, so the reason I bring this up is
that if you think about implementing

258
00:04:58,390 --> 00:04:58,400
that if you think about implementing
 

259
00:04:58,400 --> 00:05:01,030
that if you think about implementing
this on hardware, we're we're going to

260
00:05:01,030 --> 00:05:01,040
this on hardware, we're we're going to
 

261
00:05:01,040 --> 00:05:02,950
this on hardware, we're we're going to
cut the number of learnable parameters

262
00:05:02,950 --> 00:05:02,960
cut the number of learnable parameters
 

263
00:05:02,960 --> 00:05:04,790
cut the number of learnable parameters
down a lot potentially with these

264
00:05:04,790 --> 00:05:04,800
down a lot potentially with these
 

265
00:05:04,800 --> 00:05:05,990
down a lot potentially with these
parameter efficient fine-tuning

266
00:05:05,990 --> 00:05:06,000
parameter efficient fine-tuning
 

267
00:05:06,000 --> 00:05:07,590
parameter efficient fine-tuning
approaches. Like we talked last time

268
00:05:07,590 --> 00:05:07,600
approaches. Like we talked last time
 

269
00:05:07,600 --> 00:05:09,749
approaches. Like we talked last time
about how short the custom embeddings

270
00:05:09,749 --> 00:05:09,759
about how short the custom embeddings
 

271
00:05:09,759 --> 00:05:11,510
about how short the custom embeddings
are relative to the 8 billion parameters

272
00:05:11,510 --> 00:05:11,520
are relative to the 8 billion parameters
 

273
00:05:11,520 --> 00:05:14,629
are relative to the 8 billion parameters
you might have in your uh large model.

274
00:05:14,629 --> 00:05:14,639
you might have in your uh large model.
 

275
00:05:14,639 --> 00:05:17,270
you might have in your uh large model.
But you still have to compute all these

276
00:05:17,270 --> 00:05:17,280
But you still have to compute all these
 

277
00:05:17,280 --> 00:05:20,469
But you still have to compute all these
activations and back prop through them.

278
00:05:20,469 --> 00:05:20,479
activations and back prop through them.
 

279
00:05:20,479 --> 00:05:22,070
activations and back prop through them.
So we gave you this homework problem. If

280
00:05:22,070 --> 00:05:22,080
So we gave you this homework problem. If
 

281
00:05:22,080 --> 00:05:24,230
So we gave you this homework problem. If
you remember, we talked about this great

282
00:05:24,230 --> 00:05:24,240
you remember, we talked about this great
 

283
00:05:24,240 --> 00:05:28,230
you remember, we talked about this great
this idea of um regen recalculating

284
00:05:28,230 --> 00:05:28,240
this idea of um regen recalculating
 

285
00:05:28,240 --> 00:05:29,749
this idea of um regen recalculating
intermediate activations because you

286
00:05:29,749 --> 00:05:29,759
intermediate activations because you
 

287
00:05:29,759 --> 00:05:31,909
intermediate activations because you
might run out of memory of storing them.

288
00:05:31,909 --> 00:05:31,919
might run out of memory of storing them.
 

289
00:05:31,919 --> 00:05:33,830
might run out of memory of storing them.
So that memory consideration of storing

290
00:05:33,830 --> 00:05:33,840
So that memory consideration of storing
 

291
00:05:33,840 --> 00:05:35,270
So that memory consideration of storing
the intermediate activations or

292
00:05:35,270 --> 00:05:35,280
the intermediate activations or
 

293
00:05:35,280 --> 00:05:36,550
the intermediate activations or
potentially having to regenerate them

294
00:05:36,550 --> 00:05:36,560
potentially having to regenerate them
 

295
00:05:36,560 --> 00:05:38,950
potentially having to regenerate them
that still exists. Having a parameter

296
00:05:38,950 --> 00:05:38,960
that still exists. Having a parameter
 

297
00:05:38,960 --> 00:05:40,390
that still exists. Having a parameter
efficient fine-tune where you have only

298
00:05:40,390 --> 00:05:40,400
efficient fine-tune where you have only
 

299
00:05:40,400 --> 00:05:42,070
efficient fine-tune where you have only
fewer learnable parameters doesn't

300
00:05:42,070 --> 00:05:42,080
fewer learnable parameters doesn't
 

301
00:05:42,080 --> 00:05:44,629
fewer learnable parameters doesn't
change that memory requirement.

302
00:05:44,629 --> 00:05:44,639
change that memory requirement.
 

303
00:05:44,639 --> 00:05:47,430
change that memory requirement.
The memory requirement it's changing is

304
00:05:47,430 --> 00:05:47,440
The memory requirement it's changing is
 

305
00:05:47,440 --> 00:05:49,189
The memory requirement it's changing is
the memory requirement for the

306
00:05:49,189 --> 00:05:49,199
the memory requirement for the
 

307
00:05:49,199 --> 00:05:51,749
the memory requirement for the
parameters that are being updated.

308
00:05:51,749 --> 00:05:51,759
parameters that are being updated.
 

309
00:05:51,759 --> 00:05:54,310
parameters that are being updated.
and potentially the momentum terms on

310
00:05:54,310 --> 00:05:54,320
and potentially the momentum terms on
 

311
00:05:54,320 --> 00:05:56,150
and potentially the momentum terms on
the parameters being updated and for

312
00:05:56,150 --> 00:05:56,160
the parameters being updated and for
 

313
00:05:56,160 --> 00:05:58,070
the parameters being updated and for
something like atom w the kind of

314
00:05:58,070 --> 00:05:58,080
something like atom w the kind of
 

315
00:05:58,080 --> 00:05:59,990
something like atom w the kind of
keeping track of scale par that you have

316
00:05:59,990 --> 00:06:00,000
keeping track of scale par that you have
 

317
00:06:00,000 --> 00:06:02,550
keeping track of scale par that you have
to do. So Adam W if you remember or atom

318
00:06:02,550 --> 00:06:02,560
to do. So Adam W if you remember or atom
 

319
00:06:02,560 --> 00:06:05,189
to do. So Adam W if you remember or atom
generally tracks 3x

320
00:06:05,189 --> 00:06:05,199
generally tracks 3x
 

321
00:06:05,199 --> 00:06:08,870
generally tracks 3x
you need 3x you know parameter momentum

322
00:06:08,870 --> 00:06:08,880
you need 3x you know parameter momentum
 

323
00:06:08,880 --> 00:06:11,510
you need 3x you know parameter momentum
and scale. So that's the memory that

324
00:06:11,510 --> 00:06:11,520
and scale. So that's the memory that
 

325
00:06:11,520 --> 00:06:13,909
and scale. So that's the memory that
does shrink only the only ones you

326
00:06:13,909 --> 00:06:13,919
does shrink only the only ones you
 

327
00:06:13,919 --> 00:06:18,550
does shrink only the only ones you
actually can optimize over. Okay. So

328
00:06:18,550 --> 00:06:18,560
actually can optimize over. Okay. So
 

329
00:06:18,560 --> 00:06:20,070
actually can optimize over. Okay. So
any questions on this? There were some

330
00:06:20,070 --> 00:06:20,080
any questions on this? There were some
 

331
00:06:20,080 --> 00:06:21,189
any questions on this? There were some
questions last time. Hopefully I've

332
00:06:21,189 --> 00:06:21,199
questions last time. Hopefully I've
 

333
00:06:21,199 --> 00:06:22,629
questions last time. Hopefully I've
addressed the confusions that people had

334
00:06:22,629 --> 00:06:22,639
addressed the confusions that people had
 

335
00:06:22,639 --> 00:06:27,110
addressed the confusions that people had
before um with this short recap.

336
00:06:27,110 --> 00:06:27,120
before um with this short recap.
 

337
00:06:27,120 --> 00:06:29,430
before um with this short recap.
Everyone's good.

338
00:06:29,430 --> 00:06:29,440
Everyone's good.
 

339
00:06:29,440 --> 00:06:33,270
Everyone's good.
Okay, so going back up here,

340
00:06:33,270 --> 00:06:33,280
Okay, so going back up here,
 

341
00:06:33,280 --> 00:06:35,670
Okay, so going back up here,
I went very fast through the idea of

342
00:06:35,670 --> 00:06:35,680
I went very fast through the idea of
 

343
00:06:35,680 --> 00:06:37,590
I went very fast through the idea of
saying, okay, instead of doing it at the

344
00:06:37,590 --> 00:06:37,600
saying, okay, instead of doing it at the
 

345
00:06:37,600 --> 00:06:39,350
saying, okay, instead of doing it at the
level of the embedding for a soft

346
00:06:39,350 --> 00:06:39,360
level of the embedding for a soft
 

347
00:06:39,360 --> 00:06:41,189
level of the embedding for a soft
prompt, you can also have this soft

348
00:06:41,189 --> 00:06:41,199
prompt, you can also have this soft
 

349
00:06:41,199 --> 00:06:44,629
prompt, you can also have this soft
prefix idea where instead of making the

350
00:06:44,629 --> 00:06:44,639
prefix idea where instead of making the
 

351
00:06:44,639 --> 00:06:48,150
prefix idea where instead of making the
learnable parameters be a fake embedding

352
00:06:48,150 --> 00:06:48,160
learnable parameters be a fake embedding
 

353
00:06:48,160 --> 00:06:50,790
learnable parameters be a fake embedding
for a prompt, you directly act on

354
00:06:50,790 --> 00:06:50,800
for a prompt, you directly act on
 

355
00:06:50,800 --> 00:06:53,749
for a prompt, you directly act on
everything that's influencing

356
00:06:53,749 --> 00:06:53,759
everything that's influencing
 

357
00:06:53,759 --> 00:06:56,710
everything that's influencing
uh the generation. So remember what's

358
00:06:56,710 --> 00:06:56,720
uh the generation. So remember what's
 

359
00:06:56,720 --> 00:06:59,029
uh the generation. So remember what's
what's making the gradients pass this

360
00:06:59,029 --> 00:06:59,039
what's making the gradients pass this
 

361
00:06:59,039 --> 00:07:01,510
what's making the gradients pass this
way is actual dependence and that

362
00:07:01,510 --> 00:07:01,520
way is actual dependence and that
 

363
00:07:01,520 --> 00:07:03,510
way is actual dependence and that
dependence in a transformer is going

364
00:07:03,510 --> 00:07:03,520
dependence in a transformer is going
 

365
00:07:03,520 --> 00:07:06,230
dependence in a transformer is going
through the key value pairs in a state

366
00:07:06,230 --> 00:07:06,240
through the key value pairs in a state
 

367
00:07:06,240 --> 00:07:09,589
through the key value pairs in a state
space model is going through the state.

368
00:07:09,589 --> 00:07:09,599
space model is going through the state.
 

369
00:07:09,599 --> 00:07:12,230
space model is going through the state.
So the gradients are flowing through

370
00:07:12,230 --> 00:07:12,240
So the gradients are flowing through
 

371
00:07:12,240 --> 00:07:14,950
So the gradients are flowing through
that thing.

372
00:07:14,950 --> 00:07:14,960
that thing.
 

373
00:07:14,960 --> 00:07:18,469
that thing.
So in soft prefixes what you do is you

374
00:07:18,469 --> 00:07:18,479
So in soft prefixes what you do is you
 

375
00:07:18,479 --> 00:07:21,430
So in soft prefixes what you do is you
simply wherever

376
00:07:21,430 --> 00:07:21,440
simply wherever
 

377
00:07:21,440 --> 00:07:24,390
simply wherever
the gradient flows back it's flowing

378
00:07:24,390 --> 00:07:24,400
the gradient flows back it's flowing
 

379
00:07:24,400 --> 00:07:26,150
the gradient flows back it's flowing
back through for example attention which

380
00:07:26,150 --> 00:07:26,160
back through for example attention which
 

381
00:07:26,160 --> 00:07:27,350
back through for example attention which
means the way it's going back the

382
00:07:27,350 --> 00:07:27,360
means the way it's going back the
 

383
00:07:27,360 --> 00:07:28,629
means the way it's going back the
queries are being generated attention

384
00:07:28,629 --> 00:07:28,639
queries are being generated attention
 

385
00:07:28,639 --> 00:07:30,790
queries are being generated attention
here.

386
00:07:30,790 --> 00:07:30,800
here.
 

387
00:07:30,800 --> 00:07:33,430
here.
So the query path for the gradient just

388
00:07:33,430 --> 00:07:33,440
So the query path for the gradient just
 

389
00:07:33,440 --> 00:07:35,270
So the query path for the gradient just
goes vertically

390
00:07:35,270 --> 00:07:35,280
goes vertically
 

391
00:07:35,280 --> 00:07:38,710
goes vertically
in attention but the keys and values are

392
00:07:38,710 --> 00:07:38,720
in attention but the keys and values are
 

393
00:07:38,720 --> 00:07:40,710
in attention but the keys and values are
in the past.

394
00:07:40,710 --> 00:07:40,720
in the past.
 

395
00:07:40,720 --> 00:07:43,830
in the past.
Everyone know this right? So the keys

396
00:07:43,830 --> 00:07:43,840
Everyone know this right? So the keys
 

397
00:07:43,840 --> 00:07:46,309
Everyone know this right? So the keys
and values that path of the computation

398
00:07:46,309 --> 00:07:46,319
and values that path of the computation
 

399
00:07:46,319 --> 00:07:49,110
and values that path of the computation
sends gradients back into the past too.

400
00:07:49,110 --> 00:07:49,120
sends gradients back into the past too.
 

401
00:07:49,120 --> 00:07:50,950
sends gradients back into the past too.
So when those gradients go back into the

402
00:07:50,950 --> 00:07:50,960
So when those gradients go back into the
 

403
00:07:50,960 --> 00:07:53,029
So when those gradients go back into the
past on those keys and values you can

404
00:07:53,029 --> 00:07:53,039
past on those keys and values you can
 

405
00:07:53,039 --> 00:07:55,270
past on those keys and values you can
just say okay I'm going to make all

406
00:07:55,270 --> 00:07:55,280
just say okay I'm going to make all
 

407
00:07:55,280 --> 00:07:57,270
just say okay I'm going to make all
those keys and values learnable

408
00:07:57,270 --> 00:07:57,280
those keys and values learnable
 

409
00:07:57,280 --> 00:07:59,510
those keys and values learnable
parameters.

410
00:07:59,510 --> 00:07:59,520
parameters.
 

411
00:07:59,520 --> 00:08:01,350
parameters.
Okay that means the keys and values that

412
00:08:01,350 --> 00:08:01,360
Okay that means the keys and values that
 

413
00:08:01,360 --> 00:08:04,150
Okay that means the keys and values that
exist back here not the keys and values

414
00:08:04,150 --> 00:08:04,160
exist back here not the keys and values
 

415
00:08:04,160 --> 00:08:06,230
exist back here not the keys and values
that exist over here. Keys and values

416
00:08:06,230 --> 00:08:06,240
that exist over here. Keys and values
 

417
00:08:06,240 --> 00:08:08,070
that exist over here. Keys and values
exist over here are computed based on

418
00:08:08,070 --> 00:08:08,080
exist over here are computed based on
 

419
00:08:08,080 --> 00:08:09,430
exist over here are computed based on
the question and then the auto

420
00:08:09,430 --> 00:08:09,440
the question and then the auto
 

421
00:08:09,440 --> 00:08:12,230
the question and then the auto
reggressive generation or whatever. But

422
00:08:12,230 --> 00:08:12,240
reggressive generation or whatever. But
 

423
00:08:12,240 --> 00:08:14,869
reggressive generation or whatever. But
back here, this is the part that's the

424
00:08:14,869 --> 00:08:14,879
back here, this is the part that's the
 

425
00:08:14,879 --> 00:08:17,430
back here, this is the part that's the
frozen pre-prompt. So these keys and

426
00:08:17,430 --> 00:08:17,440
frozen pre-prompt. So these keys and
 

427
00:08:17,440 --> 00:08:19,589
frozen pre-prompt. So these keys and
values are always the same. So instead

428
00:08:19,589 --> 00:08:19,599
values are always the same. So instead
 

429
00:08:19,599 --> 00:08:21,350
values are always the same. So instead
of saying that all these keys and values

430
00:08:21,350 --> 00:08:21,360
of saying that all these keys and values
 

431
00:08:21,360 --> 00:08:24,070
of saying that all these keys and values
have to come from a embedding, they can

432
00:08:24,070 --> 00:08:24,080
have to come from a embedding, they can
 

433
00:08:24,080 --> 00:08:26,390
have to come from a embedding, they can
just be their own thing.

434
00:08:26,390 --> 00:08:26,400
just be their own thing.
 

435
00:08:26,400 --> 00:08:28,710
just be their own thing.
Everyone get the idea.

436
00:08:28,710 --> 00:08:28,720
Everyone get the idea.
 

437
00:08:28,720 --> 00:08:30,309
Everyone get the idea.
Ask me a question if you don't because I

438
00:08:30,309 --> 00:08:30,319
Ask me a question if you don't because I
 

439
00:08:30,319 --> 00:08:31,589
Ask me a question if you don't because I
got office hours. I got questions about

440
00:08:31,589 --> 00:08:31,599
got office hours. I got questions about
 

441
00:08:31,599 --> 00:08:33,589
got office hours. I got questions about
this. So I want to make sure.

442
00:08:33,589 --> 00:08:33,599
this. So I want to make sure.
 

443
00:08:33,599 --> 00:08:34,310
this. So I want to make sure.
>> Yeah.

444
00:08:34,310 --> 00:08:34,320
>> Yeah.
 

445
00:08:34,320 --> 00:08:35,990
>> Yeah.
>> So then would you learn the keys and

446
00:08:35,990 --> 00:08:36,000
>> So then would you learn the keys and
 

447
00:08:36,000 --> 00:08:37,990
>> So then would you learn the keys and
values for each block independent of

448
00:08:37,990 --> 00:08:38,000
values for each block independent of
 

449
00:08:38,000 --> 00:08:39,990
values for each block independent of
each other or would you still preserve

450
00:08:39,990 --> 00:08:40,000
each other or would you still preserve
 

451
00:08:40,000 --> 00:08:42,469
each other or would you still preserve
their

452
00:08:42,469 --> 00:08:42,479
their
 

453
00:08:42,479 --> 00:08:44,550
their
Great question. So the question was, let

454
00:08:44,550 --> 00:08:44,560
Great question. So the question was, let
 

455
00:08:44,560 --> 00:08:48,470
Great question. So the question was, let
me walk over here and say it. So

456
00:08:48,470 --> 00:08:48,480
me walk over here and say it. So
 

457
00:08:48,480 --> 00:08:50,790
me walk over here and say it. So
maybe the picture is a bit misleading.

458
00:08:50,790 --> 00:08:50,800
maybe the picture is a bit misleading.
 

459
00:08:50,800 --> 00:09:00,470
maybe the picture is a bit misleading.
So let me change the picture.

460
00:09:00,470 --> 00:09:00,480

 

461
00:09:00,480 --> 00:09:07,990

Oops. Okay. I didn't mean to do that.

462
00:09:07,990 --> 00:09:08,000

 

463
00:09:08,000 --> 00:09:12,389

Okay, good.

464
00:09:12,389 --> 00:09:12,399

 

465
00:09:12,399 --> 00:09:14,150

So

466
00:09:14,150 --> 00:09:14,160
So
 

467
00:09:14,160 --> 00:09:16,470
So
the transformer network has residual

468
00:09:16,470 --> 00:09:16,480
the transformer network has residual
 

469
00:09:16,480 --> 00:09:17,910
the transformer network has residual
connections and stuff moving upward

470
00:09:17,910 --> 00:09:17,920
connections and stuff moving upward
 

471
00:09:17,920 --> 00:09:20,790
connections and stuff moving upward
that's used to do computations

472
00:09:20,790 --> 00:09:20,800
that's used to do computations
 

473
00:09:20,800 --> 00:09:23,670
that's used to do computations
for the past here.

474
00:09:23,670 --> 00:09:23,680
for the past here.
 

475
00:09:23,680 --> 00:09:24,870
for the past here.
We're not doing any of those

476
00:09:24,870 --> 00:09:24,880
We're not doing any of those
 

477
00:09:24,880 --> 00:09:26,550
We're not doing any of those
computations,

478
00:09:26,550 --> 00:09:26,560
computations,
 

479
00:09:26,560 --> 00:09:28,230
computations,
right? We're just going to have keys and

480
00:09:28,230 --> 00:09:28,240
right? We're just going to have keys and
 

481
00:09:28,240 --> 00:09:30,870
right? We're just going to have keys and
values sitting up here. So when the

482
00:09:30,870 --> 00:09:30,880
values sitting up here. So when the
 

483
00:09:30,880 --> 00:09:32,790
values sitting up here. So when the
gradient comes back and it hits these

484
00:09:32,790 --> 00:09:32,800
gradient comes back and it hits these
 

485
00:09:32,800 --> 00:09:36,070
gradient comes back and it hits these
keys and values, it's okay. I will

486
00:09:36,070 --> 00:09:36,080
keys and values, it's okay. I will
 

487
00:09:36,080 --> 00:09:37,590
keys and values, it's okay. I will
update those keys and values with the

488
00:09:37,590 --> 00:09:37,600
update those keys and values with the
 

489
00:09:37,600 --> 00:09:39,590
update those keys and values with the
gradient. those keys and values because

490
00:09:39,590 --> 00:09:39,600
gradient. those keys and values because
 

491
00:09:39,600 --> 00:09:41,430
gradient. those keys and values because
they are learnable parameters, they

492
00:09:41,430 --> 00:09:41,440
they are learnable parameters, they
 

493
00:09:41,440 --> 00:09:44,230
they are learnable parameters, they
don't depend on anything.

494
00:09:44,230 --> 00:09:44,240
don't depend on anything.
 

495
00:09:44,240 --> 00:09:46,630
don't depend on anything.
Does everyone see that? Like during

496
00:09:46,630 --> 00:09:46,640
Does everyone see that? Like during
 

497
00:09:46,640 --> 00:09:47,829
Does everyone see that? Like during
execution, they don't depend on

498
00:09:47,829 --> 00:09:47,839
execution, they don't depend on
 

499
00:09:47,839 --> 00:09:49,750
execution, they don't depend on
anything. They're just parameters. So

500
00:09:49,750 --> 00:09:49,760
anything. They're just parameters. So
 

501
00:09:49,760 --> 00:09:51,509
anything. They're just parameters. So
they don't send gradients to anything

502
00:09:51,509 --> 00:09:51,519
they don't send gradients to anything
 

503
00:09:51,519 --> 00:09:53,030
they don't send gradients to anything
else.

504
00:09:53,030 --> 00:09:53,040
else.
 

505
00:09:53,040 --> 00:09:55,269
else.
So the gradients stop at these keys and

506
00:09:55,269 --> 00:09:55,279
So the gradients stop at these keys and
 

507
00:09:55,279 --> 00:09:57,030
So the gradients stop at these keys and
values when they're going back. I've

508
00:09:57,030 --> 00:09:57,040
values when they're going back. I've
 

509
00:09:57,040 --> 00:09:58,550
values when they're going back. I've
drawn it for one block, but remember

510
00:09:58,550 --> 00:09:58,560
drawn it for one block, but remember
 

511
00:09:58,560 --> 00:10:01,750
drawn it for one block, but remember
there's the entire length of this uh you

512
00:10:01,750 --> 00:10:01,760
there's the entire length of this uh you
 

513
00:10:01,760 --> 00:10:04,630
there's the entire length of this uh you
know prefix could be 100 long. So

514
00:10:04,630 --> 00:10:04,640
know prefix could be 100 long. So
 

515
00:10:04,640 --> 00:10:06,150
know prefix could be 100 long. So
there's a hundred different key value

516
00:10:06,150 --> 00:10:06,160
there's a hundred different key value
 

517
00:10:06,160 --> 00:10:08,870
there's a hundred different key value
pairs associated back here. for example

518
00:10:08,870 --> 00:10:08,880
pairs associated back here. for example
 

519
00:10:08,880 --> 00:10:11,190
pairs associated back here. for example
that are then rep there's different ones

520
00:10:11,190 --> 00:10:11,200
that are then rep there's different ones
 

521
00:10:11,200 --> 00:10:13,590
that are then rep there's different ones
across the different layers. All of

522
00:10:13,590 --> 00:10:13,600
across the different layers. All of
 

523
00:10:13,600 --> 00:10:15,670
across the different layers. All of
those are learnable parameters in this

524
00:10:15,670 --> 00:10:15,680
those are learnable parameters in this
 

525
00:10:15,680 --> 00:10:17,750
those are learnable parameters in this
soft prefix approach. Now as you can

526
00:10:17,750 --> 00:10:17,760
soft prefix approach. Now as you can
 

527
00:10:17,760 --> 00:10:20,150
soft prefix approach. Now as you can
imagine from these kind of extremes you

528
00:10:20,150 --> 00:10:20,160
imagine from these kind of extremes you
 

529
00:10:20,160 --> 00:10:22,710
imagine from these kind of extremes you
can construct hybrids too where you say

530
00:10:22,710 --> 00:10:22,720
can construct hybrids too where you say
 

531
00:10:22,720 --> 00:10:24,470
can construct hybrids too where you say
some of these key values are learned

532
00:10:24,470 --> 00:10:24,480
some of these key values are learned
 

533
00:10:24,480 --> 00:10:25,590
some of these key values are learned
some of them are generated from an

534
00:10:25,590 --> 00:10:25,600
some of them are generated from an
 

535
00:10:25,600 --> 00:10:27,430
some of them are generated from an
embedding like all this possibility

536
00:10:27,430 --> 00:10:27,440
embedding like all this possibility
 

537
00:10:27,440 --> 00:10:30,870
embedding like all this possibility
exists. Okay but you get the idea.

538
00:10:30,870 --> 00:10:30,880
exists. Okay but you get the idea.
 

539
00:10:30,880 --> 00:10:33,509
exists. Okay but you get the idea.
>> Yeah. When this do you have to use the

540
00:10:33,509 --> 00:10:33,519
>> Yeah. When this do you have to use the
 

541
00:10:33,519 --> 00:10:35,110
>> Yeah. When this do you have to use the
same question or can you use like

542
00:10:35,110 --> 00:10:35,120
same question or can you use like
 

543
00:10:35,120 --> 00:10:38,389
same question or can you use like
similar sort of questions

544
00:10:38,389 --> 00:10:38,399
similar sort of questions
 

545
00:10:38,399 --> 00:10:40,150
similar sort of questions
like this?

546
00:10:40,150 --> 00:10:40,160
like this?
 

547
00:10:40,160 --> 00:10:43,509
like this?
>> Excellent. Excellent. Uh so let me make

548
00:10:43,509 --> 00:10:43,519
>> Excellent. Excellent. Uh so let me make
 

549
00:10:43,519 --> 00:10:45,430
>> Excellent. Excellent. Uh so let me make
that very clear by making an edit to the

550
00:10:45,430 --> 00:10:45,440
that very clear by making an edit to the
 

551
00:10:45,440 --> 00:11:00,710
that very clear by making an edit to the
slide.

552
00:11:00,710 --> 00:11:00,720

 

553
00:11:00,720 --> 00:11:04,230

Okay. Yeah. So, we talk about these

554
00:11:04,230 --> 00:11:04,240
Okay. Yeah. So, we talk about these
 

555
00:11:04,240 --> 00:11:07,829
Okay. Yeah. So, we talk about these
pre-prompts. The pre-prompt is for

556
00:11:07,829 --> 00:11:07,839
pre-prompts. The pre-prompt is for
 

557
00:11:07,839 --> 00:11:09,509
pre-prompts. The pre-prompt is for
whatever you consider the thing you're

558
00:11:09,509 --> 00:11:09,519
whatever you consider the thing you're
 

559
00:11:09,519 --> 00:11:12,069
whatever you consider the thing you're
fine-tuning for. The thing you're

560
00:11:12,069 --> 00:11:12,079
fine-tuning for. The thing you're
 

561
00:11:12,079 --> 00:11:14,630
fine-tuning for. The thing you're
fine-tuning for isn't a specific answer.

562
00:11:14,630 --> 00:11:14,640
fine-tuning for isn't a specific answer.
 

563
00:11:14,640 --> 00:11:16,069
fine-tuning for isn't a specific answer.
You don't need to train a model to give

564
00:11:16,069 --> 00:11:16,079
You don't need to train a model to give
 

565
00:11:16,079 --> 00:11:17,670
You don't need to train a model to give
a specific answer. Just copy and paste

566
00:11:17,670 --> 00:11:17,680
a specific answer. Just copy and paste
 

567
00:11:17,680 --> 00:11:20,150
a specific answer. Just copy and paste
that specific answer. So you want a

568
00:11:20,150 --> 00:11:20,160
that specific answer. So you want a
 

569
00:11:20,160 --> 00:11:22,389
that specific answer. So you want a
model that will respond to a whole bunch

570
00:11:22,389 --> 00:11:22,399
model that will respond to a whole bunch
 

571
00:11:22,399 --> 00:11:24,069
model that will respond to a whole bunch
of kinds of questions. Now those

572
00:11:24,069 --> 00:11:24,079
of kinds of questions. Now those
 

573
00:11:24,079 --> 00:11:25,110
of kinds of questions. Now those
questions might have their own

574
00:11:25,110 --> 00:11:25,120
questions might have their own
 

575
00:11:25,120 --> 00:11:27,190
questions might have their own
hierarchical structure. For example,

576
00:11:27,190 --> 00:11:27,200
hierarchical structure. For example,
 

577
00:11:27,200 --> 00:11:29,269
hierarchical structure. For example,
they might be now I'm going to ask you a

578
00:11:29,269 --> 00:11:29,279
they might be now I'm going to ask you a
 

579
00:11:29,279 --> 00:11:32,550
they might be now I'm going to ask you a
question about this. Now then a question

580
00:11:32,550 --> 00:11:32,560
question about this. Now then a question
 

581
00:11:32,560 --> 00:11:34,710
question about this. Now then a question
about this proceeds. All of that I'm

582
00:11:34,710 --> 00:11:34,720
about this proceeds. All of that I'm
 

583
00:11:34,720 --> 00:11:37,590
about this proceeds. All of that I'm
grouping into question during training

584
00:11:37,590 --> 00:11:37,600
grouping into question during training
 

585
00:11:37,600 --> 00:11:39,910
grouping into question during training
meaning fine-tuning training. You will

586
00:11:39,910 --> 00:11:39,920
meaning fine-tuning training. You will
 

587
00:11:39,920 --> 00:11:41,829
meaning fine-tuning training. You will
have a whole collection of these

588
00:11:41,829 --> 00:11:41,839
have a whole collection of these
 

589
00:11:41,839 --> 00:11:43,430
have a whole collection of these
questions and their corresponding

590
00:11:43,430 --> 00:11:43,440
questions and their corresponding
 

591
00:11:43,440 --> 00:11:46,069
questions and their corresponding
answers that you want. You will train

592
00:11:46,069 --> 00:11:46,079
answers that you want. You will train
 

593
00:11:46,079 --> 00:11:48,230
answers that you want. You will train
for all of that. And when using it,

594
00:11:48,230 --> 00:11:48,240
for all of that. And when using it,
 

595
00:11:48,240 --> 00:11:50,630
for all of that. And when using it,
you'll also use it with this part will

596
00:11:50,630 --> 00:11:50,640
you'll also use it with this part will
 

597
00:11:50,640 --> 00:11:52,550
you'll also use it with this part will
be fixed and then you'll stick in

598
00:11:52,550 --> 00:11:52,560
be fixed and then you'll stick in
 

599
00:11:52,560 --> 00:11:55,030
be fixed and then you'll stick in
whatever held out for testing, right?

600
00:11:55,030 --> 00:11:55,040
whatever held out for testing, right?
 

601
00:11:55,040 --> 00:11:56,870
whatever held out for testing, right?
Held out question and you expect to get

602
00:11:56,870 --> 00:11:56,880
Held out question and you expect to get
 

603
00:11:56,880 --> 00:12:00,389
Held out question and you expect to get
an answer you want out. Okay, so yeah, I

604
00:12:00,389 --> 00:12:00,399
an answer you want out. Okay, so yeah, I
 

605
00:12:00,399 --> 00:12:02,230
an answer you want out. Okay, so yeah, I
put the I hopefully putting these eyes

606
00:12:02,230 --> 00:12:02,240
put the I hopefully putting these eyes
 

607
00:12:02,240 --> 00:12:03,670
put the I hopefully putting these eyes
will help you just like there's lots

608
00:12:03,670 --> 00:12:03,680
will help you just like there's lots
 

609
00:12:03,680 --> 00:12:04,870
will help you just like there's lots
like a training set. There's lots of

610
00:12:04,870 --> 00:12:04,880
like a training set. There's lots of
 

611
00:12:04,880 --> 00:12:06,629
like a training set. There's lots of
them

612
00:12:06,629 --> 00:12:06,639
them
 

613
00:12:06,639 --> 00:12:08,150
them
for for doing fine tuning. Fine tuning

614
00:12:08,150 --> 00:12:08,160
for for doing fine tuning. Fine tuning
 

615
00:12:08,160 --> 00:12:09,509
for for doing fine tuning. Fine tuning
requires a training set. It's a kind of

616
00:12:09,509 --> 00:12:09,519
requires a training set. It's a kind of
 

617
00:12:09,519 --> 00:12:12,230
requires a training set. It's a kind of
training.

618
00:12:12,230 --> 00:12:12,240
training.
 

619
00:12:12,240 --> 00:12:14,389
training.
Does that answer your question? Is

620
00:12:14,389 --> 00:12:14,399
Does that answer your question? Is
 

621
00:12:14,399 --> 00:12:16,629
Does that answer your question? Is
everyone good with this?

622
00:12:16,629 --> 00:12:16,639
everyone good with this?
 

623
00:12:16,639 --> 00:12:19,269
everyone good with this?
Okay. So,

624
00:12:19,269 --> 00:12:19,279
Okay. So,
 

625
00:12:19,279 --> 00:12:21,350
Okay. So,
um what happened was this is this is

626
00:12:21,350 --> 00:12:21,360
um what happened was this is this is
 

627
00:12:21,360 --> 00:12:23,030
um what happened was this is this is
from last time just you know the number

628
00:12:23,030 --> 00:12:23,040
from last time just you know the number
 

629
00:12:23,040 --> 00:12:24,949
from last time just you know the number
of parameters we're talking about for a

630
00:12:24,949 --> 00:12:24,959
of parameters we're talking about for a
 

631
00:12:24,959 --> 00:12:29,030
of parameters we're talking about for a
soft prompt is like for 100 long and

632
00:12:29,030 --> 00:12:29,040
soft prompt is like for 100 long and
 

633
00:12:29,040 --> 00:12:32,710
soft prompt is like for 100 long and
embedding is 4,96 it's like just 40k.

634
00:12:32,710 --> 00:12:32,720
embedding is 4,96 it's like just 40k.
 

635
00:12:32,720 --> 00:12:35,590
embedding is 4,96 it's like just 40k.
It's much much smaller than the size of

636
00:12:35,590 --> 00:12:35,600
It's much much smaller than the size of
 

637
00:12:35,600 --> 00:12:37,190
It's much much smaller than the size of
a a substantial model like seven or

638
00:12:37,190 --> 00:12:37,200
a a substantial model like seven or
 

639
00:12:37,200 --> 00:12:41,590
a a substantial model like seven or
eight billion. So it's very short and

640
00:12:41,590 --> 00:12:41,600
eight billion. So it's very short and
 

641
00:12:41,600 --> 00:12:43,829
eight billion. So it's very short and
even with the soft prefix you get up

642
00:12:43,829 --> 00:12:43,839
even with the soft prefix you get up
 

643
00:12:43,839 --> 00:12:45,990
even with the soft prefix you get up
with this calculation up to something

644
00:12:45,990 --> 00:12:46,000
with this calculation up to something
 

645
00:12:46,000 --> 00:12:49,829
with this calculation up to something
like 262,000 which is still way shorter

646
00:12:49,829 --> 00:12:49,839
like 262,000 which is still way shorter
 

647
00:12:49,839 --> 00:12:51,590
like 262,000 which is still way shorter
than

648
00:12:51,590 --> 00:12:51,600
than
 

649
00:12:51,600 --> 00:12:54,310
than
the size of the model or 26 million

650
00:12:54,310 --> 00:12:54,320
the size of the model or 26 million
 

651
00:12:54,320 --> 00:12:57,430
the size of the model or 26 million
times 100 right so 2 26 million is still

652
00:12:57,430 --> 00:12:57,440
times 100 right so 2 26 million is still
 

653
00:12:57,440 --> 00:12:59,269
times 100 right so 2 26 million is still
way smaller than seven or eight billion

654
00:12:59,269 --> 00:12:59,279
way smaller than seven or eight billion
 

655
00:12:59,279 --> 00:13:02,710
way smaller than seven or eight billion
so it's substantially smaller and what I

656
00:13:02,710 --> 00:13:02,720
so it's substantially smaller and what I
 

657
00:13:02,720 --> 00:13:05,269
so it's substantially smaller and what I
mentioned last time is that as models

658
00:13:05,269 --> 00:13:05,279
mentioned last time is that as models
 

659
00:13:05,279 --> 00:13:08,150
mentioned last time is that as models
get larger people empirically observed

660
00:13:08,150 --> 00:13:08,160
get larger people empirically observed
 

661
00:13:08,160 --> 00:13:11,990
get larger people empirically observed
that this soft prompting and prefix soft

662
00:13:11,990 --> 00:13:12,000
that this soft prompting and prefix soft
 

663
00:13:12,000 --> 00:13:15,590
that this soft prompting and prefix soft
prefix type fine-tuning did very well.

664
00:13:15,590 --> 00:13:15,600
prefix type fine-tuning did very well.
 

665
00:13:15,600 --> 00:13:17,590
prefix type fine-tuning did very well.
What I mean by doing very well, people

666
00:13:17,590 --> 00:13:17,600
What I mean by doing very well, people
 

667
00:13:17,600 --> 00:13:18,949
What I mean by doing very well, people
would do experiments where they would

668
00:13:18,949 --> 00:13:18,959
would do experiments where they would
 

669
00:13:18,959 --> 00:13:21,750
would do experiments where they would
say, I will do a full fine-tune.

670
00:13:21,750 --> 00:13:21,760
say, I will do a full fine-tune.
 

671
00:13:21,760 --> 00:13:23,430
say, I will do a full fine-tune.
I'll see what I can do with a full fine

672
00:13:23,430 --> 00:13:23,440
I'll see what I can do with a full fine
 

673
00:13:23,440 --> 00:13:27,670
I'll see what I can do with a full fine
tune and I'll do a just a soft prompt or

674
00:13:27,670 --> 00:13:27,680
tune and I'll do a just a soft prompt or
 

675
00:13:27,680 --> 00:13:30,470
tune and I'll do a just a soft prompt or
just a soft prefix. And what they were

676
00:13:30,470 --> 00:13:30,480
just a soft prefix. And what they were
 

677
00:13:30,480 --> 00:13:32,790
just a soft prefix. And what they were
finding is that

678
00:13:32,790 --> 00:13:32,800
finding is that
 

679
00:13:32,800 --> 00:13:35,670
finding is that
the model would do as well as a full

680
00:13:35,670 --> 00:13:35,680
the model would do as well as a full
 

681
00:13:35,680 --> 00:13:39,350
the model would do as well as a full
fine-tune. and in some cases better than

682
00:13:39,350 --> 00:13:39,360
fine-tune. and in some cases better than
 

683
00:13:39,360 --> 00:13:42,069
fine-tune. and in some cases better than
a full fine tune

684
00:13:42,069 --> 00:13:42,079
a full fine tune
 

685
00:13:42,079 --> 00:13:45,030
a full fine tune
by just doing this. Okay, so I'll pause

686
00:13:45,030 --> 00:13:45,040
by just doing this. Okay, so I'll pause
 

687
00:13:45,040 --> 00:13:46,790
by just doing this. Okay, so I'll pause
here for a moment and ask you guys a

688
00:13:46,790 --> 00:13:46,800
here for a moment and ask you guys a
 

689
00:13:46,800 --> 00:13:48,550
here for a moment and ask you guys a
question based on what you know about

690
00:13:48,550 --> 00:13:48,560
question based on what you know about
 

691
00:13:48,560 --> 00:13:53,829
question based on what you know about
machine learning. Uh why do you think

692
00:13:53,829 --> 00:13:53,839
machine learning. Uh why do you think
 

693
00:13:53,839 --> 00:13:56,470
machine learning. Uh why do you think
conceivably it could do better with a

694
00:13:56,470 --> 00:13:56,480
conceivably it could do better with a
 

695
00:13:56,480 --> 00:13:59,590
conceivably it could do better with a
soft prompt or a soft prefix where you

696
00:13:59,590 --> 00:13:59,600
soft prompt or a soft prefix where you
 

697
00:13:59,600 --> 00:14:03,670
soft prompt or a soft prefix where you
only have 40,000 parameters or 26

698
00:14:03,670 --> 00:14:03,680
only have 40,000 parameters or 26
 

699
00:14:03,680 --> 00:14:06,389
only have 40,000 parameters or 26
million parameters you're uh learning

700
00:14:06,389 --> 00:14:06,399
million parameters you're uh learning
 

701
00:14:06,399 --> 00:14:08,550
million parameters you're uh learning
during the fine-tuning process versus

702
00:14:08,550 --> 00:14:08,560
during the fine-tuning process versus
 

703
00:14:08,560 --> 00:14:10,629
during the fine-tuning process versus
doing a full fine-tune in which case

704
00:14:10,629 --> 00:14:10,639
doing a full fine-tune in which case
 

705
00:14:10,639 --> 00:14:13,670
doing a full fine-tune in which case
you're twe tweaking all seven or eight

706
00:14:13,670 --> 00:14:13,680
you're twe tweaking all seven or eight
 

707
00:14:13,680 --> 00:14:15,430
you're twe tweaking all seven or eight
billion parameters in this example.

708
00:14:15,430 --> 00:14:15,440
billion parameters in this example.
 

709
00:14:15,440 --> 00:14:17,350
billion parameters in this example.
Anyone want to tell me why from your

710
00:14:17,350 --> 00:14:17,360
Anyone want to tell me why from your
 

711
00:14:17,360 --> 00:14:19,829
Anyone want to tell me why from your
machine learning knowledge, why might it

712
00:14:19,829 --> 00:14:19,839
machine learning knowledge, why might it
 

713
00:14:19,839 --> 00:14:22,949
machine learning knowledge, why might it
do better uh with uh with parameter

714
00:14:22,949 --> 00:14:22,959
do better uh with uh with parameter
 

715
00:14:22,959 --> 00:14:25,430
do better uh with uh with parameter
efficient fine tune?

716
00:14:25,430 --> 00:14:25,440
efficient fine tune?
 

717
00:14:25,440 --> 00:14:27,670
efficient fine tune?
Okay, so I'll pause a minute. I noticed

718
00:14:27,670 --> 00:14:27,680
Okay, so I'll pause a minute. I noticed
 

719
00:14:27,680 --> 00:14:29,110
Okay, so I'll pause a minute. I noticed
people's facial expressions. So I'm

720
00:14:29,110 --> 00:14:29,120
people's facial expressions. So I'm
 

721
00:14:29,120 --> 00:14:31,110
people's facial expressions. So I'm
going to like say how many of you I'm

722
00:14:31,110 --> 00:14:31,120
going to like say how many of you I'm
 

723
00:14:31,120 --> 00:14:32,949
going to like say how many of you I'm
not going to call on you. How many of

724
00:14:32,949 --> 00:14:32,959
not going to call on you. How many of
 

725
00:14:32,959 --> 00:14:36,710
not going to call on you. How many of
you feel like you know the answer? Raise

726
00:14:36,710 --> 00:14:36,720
you feel like you know the answer? Raise
 

727
00:14:36,720 --> 00:14:41,350
you feel like you know the answer? Raise
your hand. I'm not going to call on you.

728
00:14:41,350 --> 00:14:41,360

 

729
00:14:41,360 --> 00:14:43,350

Okay. How many of you feel like you

730
00:14:43,350 --> 00:14:43,360
Okay. How many of you feel like you
 

731
00:14:43,360 --> 00:14:47,269
Okay. How many of you feel like you
don't know the answer? raise your hand.

732
00:14:47,269 --> 00:14:47,279
don't know the answer? raise your hand.
 

733
00:14:47,279 --> 00:14:49,350
don't know the answer? raise your hand.
Okay. So, a lot of you actually feel

734
00:14:49,350 --> 00:14:49,360
Okay. So, a lot of you actually feel
 

735
00:14:49,360 --> 00:14:51,750
Okay. So, a lot of you actually feel
like you don't I wasn't sure. So, it's

736
00:14:51,750 --> 00:14:51,760
like you don't I wasn't sure. So, it's
 

737
00:14:51,760 --> 00:14:53,670
like you don't I wasn't sure. So, it's
good to make sure that stuff comes back

738
00:14:53,670 --> 00:14:53,680
good to make sure that stuff comes back
 

739
00:14:53,680 --> 00:14:54,870
good to make sure that stuff comes back
into your mind because we're talking

740
00:14:54,870 --> 00:14:54,880
into your mind because we're talking
 

741
00:14:54,880 --> 00:14:56,310
into your mind because we're talking
about everything we're talking about now

742
00:14:56,310 --> 00:14:56,320
about everything we're talking about now
 

743
00:14:56,320 --> 00:14:57,430
about everything we're talking about now
for a little bit is going to be about

744
00:14:57,430 --> 00:14:57,440
for a little bit is going to be about
 

745
00:14:57,440 --> 00:14:59,910
for a little bit is going to be about
fine-tuning. So, it's good to remember

746
00:14:59,910 --> 00:14:59,920
fine-tuning. So, it's good to remember
 

747
00:14:59,920 --> 00:15:04,470
fine-tuning. So, it's good to remember
stuff about basic learning. Um,

748
00:15:04,470 --> 00:15:04,480
stuff about basic learning. Um,
 

749
00:15:04,480 --> 00:15:06,710
stuff about basic learning. Um,
in machine learning problems, let me go

750
00:15:06,710 --> 00:15:06,720
in machine learning problems, let me go
 

751
00:15:06,720 --> 00:15:08,470
in machine learning problems, let me go
back to examples you've seen there. If I

752
00:15:08,470 --> 00:15:08,480
back to examples you've seen there. If I
 

753
00:15:08,480 --> 00:15:11,670
back to examples you've seen there. If I
told you I'm I have a data set and I

754
00:15:11,670 --> 00:15:11,680
told you I'm I have a data set and I
 

755
00:15:11,680 --> 00:15:13,910
told you I'm I have a data set and I
want to do regression on it and I fit a

756
00:15:13,910 --> 00:15:13,920
want to do regression on it and I fit a
 

757
00:15:13,920 --> 00:15:17,670
want to do regression on it and I fit a
line and the line fit better than

758
00:15:17,670 --> 00:15:17,680
line and the line fit better than
 

759
00:15:17,680 --> 00:15:20,230
line and the line fit better than
fitting a 500 degree polomial for

760
00:15:20,230 --> 00:15:20,240
fitting a 500 degree polomial for
 

761
00:15:20,240 --> 00:15:22,150
fitting a 500 degree polomial for
generalization.

762
00:15:22,150 --> 00:15:22,160
generalization.
 

763
00:15:22,160 --> 00:15:24,790
generalization.
Everyone remember that kind of story. If

764
00:15:24,790 --> 00:15:24,800
Everyone remember that kind of story. If
 

765
00:15:24,800 --> 00:15:26,949
Everyone remember that kind of story. If
I asked you that question and I asked

766
00:15:26,949 --> 00:15:26,959
I asked you that question and I asked
 

767
00:15:26,959 --> 00:15:31,030
I asked you that question and I asked
you why might this be happening

768
00:15:31,030 --> 00:15:31,040
you why might this be happening
 

769
00:15:31,040 --> 00:15:33,430
you why might this be happening
where a line is a degree two a degree

770
00:15:33,430 --> 00:15:33,440
where a line is a degree two a degree
 

771
00:15:33,440 --> 00:15:35,910
where a line is a degree two a degree
one but means two parameters and degree

772
00:15:35,910 --> 00:15:35,920
one but means two parameters and degree
 

773
00:15:35,920 --> 00:15:39,350
one but means two parameters and degree
500 is 501 parameters okay now I'm not

774
00:15:39,350 --> 00:15:39,360
500 is 501 parameters okay now I'm not
 

775
00:15:39,360 --> 00:15:41,189
500 is 501 parameters okay now I'm not
going to call on you how many people

776
00:15:41,189 --> 00:15:41,199
going to call on you how many people
 

777
00:15:41,199 --> 00:15:45,430
going to call on you how many people
feel you could answer that question

778
00:15:45,430 --> 00:15:45,440
feel you could answer that question
 

779
00:15:45,440 --> 00:15:49,990
feel you could answer that question
very good okay now having said that how

780
00:15:49,990 --> 00:15:50,000
very good okay now having said that how
 

781
00:15:50,000 --> 00:15:52,310
very good okay now having said that how
many people feel you could answer this

782
00:15:52,310 --> 00:15:52,320
many people feel you could answer this
 

783
00:15:52,320 --> 00:15:55,670
many people feel you could answer this
question which is why is it again as a

784
00:15:55,670 --> 00:15:55,680
question which is why is it again as a
 

785
00:15:55,680 --> 00:15:59,189
question which is why is it again as a
hypothesis why a soft prompt with 40,000

786
00:15:59,189 --> 00:15:59,199
hypothesis why a soft prompt with 40,000
 

787
00:15:59,199 --> 00:16:02,230
hypothesis why a soft prompt with 40,000
parameters or a soft prefix with 26

788
00:16:02,230 --> 00:16:02,240
parameters or a soft prefix with 26
 

789
00:16:02,240 --> 00:16:05,509
parameters or a soft prefix with 26
million parameters might do better than

790
00:16:05,509 --> 00:16:05,519
million parameters might do better than
 

791
00:16:05,519 --> 00:16:09,350
million parameters might do better than
a full fine tune raise your hand

792
00:16:09,350 --> 00:16:09,360
a full fine tune raise your hand
 

793
00:16:09,360 --> 00:16:12,470
a full fine tune raise your hand
okay good

794
00:16:12,470 --> 00:16:12,480
okay good
 

795
00:16:12,480 --> 00:16:16,470
okay good
now I will ask who wants to say

796
00:16:16,470 --> 00:16:16,480
now I will ask who wants to say
 

797
00:16:16,480 --> 00:16:20,069
now I will ask who wants to say
something raise your hand okay less

798
00:16:20,069 --> 00:16:20,079
something raise your hand okay less
 

799
00:16:20,079 --> 00:16:21,670
something raise your hand okay less
people okay

800
00:16:21,670 --> 00:16:21,680
people okay
 

801
00:16:21,680 --> 00:16:24,310
people okay
>> uh the standard bias variance tradeoff.

802
00:16:24,310 --> 00:16:24,320
>> uh the standard bias variance tradeoff.
 

803
00:16:24,320 --> 00:16:27,670
>> uh the standard bias variance tradeoff.
Um the smaller parameter model probably

804
00:16:27,670 --> 00:16:27,680
Um the smaller parameter model probably
 

805
00:16:27,680 --> 00:16:29,670
Um the smaller parameter model probably
is better generalizable to a bunch of

806
00:16:29,670 --> 00:16:29,680
is better generalizable to a bunch of
 

807
00:16:29,680 --> 00:16:32,550
is better generalizable to a bunch of
problems whereas the massive model is

808
00:16:32,550 --> 00:16:32,560
problems whereas the massive model is
 

809
00:16:32,560 --> 00:16:34,629
problems whereas the massive model is
just overtrained.

810
00:16:34,629 --> 00:16:34,639
just overtrained.
 

811
00:16:34,639 --> 00:16:37,269
just overtrained.
>> Okay, so use the different word but um

812
00:16:37,269 --> 00:16:37,279
>> Okay, so use the different word but um
 

813
00:16:37,279 --> 00:16:39,030
>> Okay, so use the different word but um
the classic explanation for this is a

814
00:16:39,030 --> 00:16:39,040
the classic explanation for this is a
 

815
00:16:39,040 --> 00:16:41,910
the classic explanation for this is a
kind of overfitting right which is that

816
00:16:41,910 --> 00:16:41,920
kind of overfitting right which is that
 

817
00:16:41,920 --> 00:16:44,230
kind of overfitting right which is that
what you have is you have so many

818
00:16:44,230 --> 00:16:44,240
what you have is you have so many
 

819
00:16:44,240 --> 00:16:47,189
what you have is you have so many
parameters that you have fit to the

820
00:16:47,189 --> 00:16:47,199
parameters that you have fit to the
 

821
00:16:47,199 --> 00:16:49,509
parameters that you have fit to the
details of your training set in such a

822
00:16:49,509 --> 00:16:49,519
details of your training set in such a
 

823
00:16:49,519 --> 00:16:51,829
details of your training set in such a
way that isn't the right way to

824
00:16:51,829 --> 00:16:51,839
way that isn't the right way to
 

825
00:16:51,839 --> 00:16:53,430
way that isn't the right way to
generalize. But with fewer parameters,

826
00:16:53,430 --> 00:16:53,440
generalize. But with fewer parameters,
 

827
00:16:53,440 --> 00:16:55,670
generalize. But with fewer parameters,
you're more inclined to, you know,

828
00:16:55,670 --> 00:16:55,680
you're more inclined to, you know,
 

829
00:16:55,680 --> 00:16:57,829
you're more inclined to, you know,
generalize properly. So this is should

830
00:16:57,829 --> 00:16:57,839
generalize properly. So this is should
 

831
00:16:57,839 --> 00:17:00,550
generalize properly. So this is should
be your gut reaction. Of course, you

832
00:17:00,550 --> 00:17:00,560
be your gut reaction. Of course, you
 

833
00:17:00,560 --> 00:17:02,550
be your gut reaction. Of course, you
might be bothered, and you probably

834
00:17:02,550 --> 00:17:02,560
might be bothered, and you probably
 

835
00:17:02,560 --> 00:17:06,549
might be bothered, and you probably
should be um because this is 40,000, and

836
00:17:06,549 --> 00:17:06,559
should be um because this is 40,000, and
 

837
00:17:06,559 --> 00:17:07,750
should be um because this is 40,000, and
the size of things you might be

838
00:17:07,750 --> 00:17:07,760
the size of things you might be
 

839
00:17:07,760 --> 00:17:10,230
the size of things you might be
fine-tuning on might be much smaller

840
00:17:10,230 --> 00:17:10,240
fine-tuning on might be much smaller
 

841
00:17:10,240 --> 00:17:12,630
fine-tuning on might be much smaller
than 40,000.

842
00:17:12,630 --> 00:17:12,640
than 40,000.
 

843
00:17:12,640 --> 00:17:16,150
than 40,000.
Okay? So the amount of things you might

844
00:17:16,150 --> 00:17:16,160
Okay? So the amount of things you might
 

845
00:17:16,160 --> 00:17:17,669
Okay? So the amount of things you might
be fine- tuning on might be a thousand

846
00:17:17,669 --> 00:17:17,679
be fine- tuning on might be a thousand
 

847
00:17:17,679 --> 00:17:19,429
be fine- tuning on might be a thousand
examples.

848
00:17:19,429 --> 00:17:19,439
examples.
 

849
00:17:19,439 --> 00:17:23,029
examples.
So it's this is still a little bit

850
00:17:23,029 --> 00:17:23,039
So it's this is still a little bit
 

851
00:17:23,039 --> 00:17:25,909
So it's this is still a little bit
weird. Okay, I don't want to I don't

852
00:17:25,909 --> 00:17:25,919
weird. Okay, I don't want to I don't
 

853
00:17:25,919 --> 00:17:27,909
weird. Okay, I don't want to I don't
want to mislead you, but it should give

854
00:17:27,909 --> 00:17:27,919
want to mislead you, but it should give
 

855
00:17:27,919 --> 00:17:29,430
want to mislead you, but it should give
you an idea that it's not completely

856
00:17:29,430 --> 00:17:29,440
you an idea that it's not completely
 

857
00:17:29,440 --> 00:17:31,350
you an idea that it's not completely
crazy for the idea of a parameter

858
00:17:31,350 --> 00:17:31,360
crazy for the idea of a parameter
 

859
00:17:31,360 --> 00:17:33,350
crazy for the idea of a parameter
efficient way to do better for

860
00:17:33,350 --> 00:17:33,360
efficient way to do better for
 

861
00:17:33,360 --> 00:17:35,430
efficient way to do better for
generalization than that. But there's

862
00:17:35,430 --> 00:17:35,440
generalization than that. But there's
 

863
00:17:35,440 --> 00:17:37,190
generalization than that. But there's
still something to be explained here if

864
00:17:37,190 --> 00:17:37,200
still something to be explained here if
 

865
00:17:37,200 --> 00:17:39,110
still something to be explained here if
you do it with a fall because both are

866
00:17:39,110 --> 00:17:39,120
you do it with a fall because both are
 

867
00:17:39,120 --> 00:17:40,870
you do it with a fall because both are
too big.

868
00:17:40,870 --> 00:17:40,880
too big.
 

869
00:17:40,880 --> 00:17:43,029
too big.
Um

870
00:17:43,029 --> 00:17:43,039
Um
 

871
00:17:43,039 --> 00:17:47,909
Um
okay, good. So now once you saw this

872
00:17:47,909 --> 00:17:47,919
okay, good. So now once you saw this
 

873
00:17:47,919 --> 00:17:50,789
okay, good. So now once you saw this
people also saw that depending on the

874
00:17:50,789 --> 00:17:50,799
people also saw that depending on the
 

875
00:17:50,799 --> 00:17:54,310
people also saw that depending on the
kind of problem soft prefixes and soft

876
00:17:54,310 --> 00:17:54,320
kind of problem soft prefixes and soft
 

877
00:17:54,320 --> 00:17:58,470
kind of problem soft prefixes and soft
prompts would do well at generalizing

878
00:17:58,470 --> 00:17:58,480
prompts would do well at generalizing
 

879
00:17:58,480 --> 00:18:00,150
prompts would do well at generalizing
but at other kinds of problems they

880
00:18:00,150 --> 00:18:00,160
but at other kinds of problems they
 

881
00:18:00,160 --> 00:18:03,510
but at other kinds of problems they
would not do so well at generalizing

882
00:18:03,510 --> 00:18:03,520
would not do so well at generalizing
 

883
00:18:03,520 --> 00:18:06,630
would not do so well at generalizing
as compared to a full fine tune.

884
00:18:06,630 --> 00:18:06,640
as compared to a full fine tune.
 

885
00:18:06,640 --> 00:18:11,590
as compared to a full fine tune.
So at that point

886
00:18:11,590 --> 00:18:11,600

 

887
00:18:11,600 --> 00:18:14,870

once you understood that it might be

888
00:18:14,870 --> 00:18:14,880
once you understood that it might be
 

889
00:18:14,880 --> 00:18:18,789
once you understood that it might be
possible to only adjust a few parameters

890
00:18:18,789 --> 00:18:18,799
possible to only adjust a few parameters
 

891
00:18:18,799 --> 00:18:23,350
possible to only adjust a few parameters
and get good generalization on uh some

892
00:18:23,350 --> 00:18:23,360
and get good generalization on uh some
 

893
00:18:23,360 --> 00:18:26,390
and get good generalization on uh some
particular subro some particular problem

894
00:18:26,390 --> 00:18:26,400
particular subro some particular problem
 

895
00:18:26,400 --> 00:18:28,070
particular subro some particular problem
that people said okay what could I do

896
00:18:28,070 --> 00:18:28,080
that people said okay what could I do
 

897
00:18:28,080 --> 00:18:30,310
that people said okay what could I do
that's more general than just doing a

898
00:18:30,310 --> 00:18:30,320
that's more general than just doing a
 

899
00:18:30,320 --> 00:18:33,430
that's more general than just doing a
soft prompt or a soft prefix and that's

900
00:18:33,430 --> 00:18:33,440
soft prompt or a soft prefix and that's
 

901
00:18:33,440 --> 00:18:38,630
soft prompt or a soft prefix and that's
what brings us to the idea of luras,

902
00:18:38,630 --> 00:18:38,640
what brings us to the idea of luras,
 

903
00:18:38,640 --> 00:18:41,430
what brings us to the idea of luras,
okay, which are

904
00:18:41,430 --> 00:18:41,440
okay, which are
 

905
00:18:41,440 --> 00:18:43,990
okay, which are
low rank adaptations.

906
00:18:43,990 --> 00:18:44,000
low rank adaptations.
 

907
00:18:44,000 --> 00:18:45,590
low rank adaptations.
So, some of you have already heard this

908
00:18:45,590 --> 00:18:45,600
So, some of you have already heard this
 

909
00:18:45,600 --> 00:18:48,150
So, some of you have already heard this
term. How many of you have heard of all?

910
00:18:48,150 --> 00:18:48,160
term. How many of you have heard of all?
 

911
00:18:48,160 --> 00:18:51,110
term. How many of you have heard of all?
A lot of you. Okay, so but now we can

912
00:18:51,110 --> 00:18:51,120
A lot of you. Okay, so but now we can
 

913
00:18:51,120 --> 00:18:53,669
A lot of you. Okay, so but now we can
talk about you know it in more detail.

914
00:18:53,669 --> 00:18:53,679
talk about you know it in more detail.
 

915
00:18:53,679 --> 00:18:56,470
talk about you know it in more detail.
So the idea in Allora is to say let's

916
00:18:56,470 --> 00:18:56,480
So the idea in Allora is to say let's
 

917
00:18:56,480 --> 00:18:59,110
So the idea in Allora is to say let's
step back and not think about something

918
00:18:59,110 --> 00:18:59,120
step back and not think about something
 

919
00:18:59,120 --> 00:19:02,390
step back and not think about something
that's restricted to this uh prompting

920
00:19:02,390 --> 00:19:02,400
that's restricted to this uh prompting
 

921
00:19:02,400 --> 00:19:05,029
that's restricted to this uh prompting
setup. But just generally if I want to

922
00:19:05,029 --> 00:19:05,039
setup. But just generally if I want to
 

923
00:19:05,039 --> 00:19:07,510
setup. But just generally if I want to
take a model and only adjust a small

924
00:19:07,510 --> 00:19:07,520
take a model and only adjust a small
 

925
00:19:07,520 --> 00:19:10,549
take a model and only adjust a small
number of parameters, what can I do as a

926
00:19:10,549 --> 00:19:10,559
number of parameters, what can I do as a
 

927
00:19:10,559 --> 00:19:13,830
number of parameters, what can I do as a
generic recipe?

928
00:19:13,830 --> 00:19:13,840
generic recipe?
 

929
00:19:13,840 --> 00:19:17,029
generic recipe?
So very early on people would try things

930
00:19:17,029 --> 00:19:17,039
So very early on people would try things
 

931
00:19:17,039 --> 00:19:19,590
So very early on people would try things
like saying I'm just going to fine-tune

932
00:19:19,590 --> 00:19:19,600
like saying I'm just going to fine-tune
 

933
00:19:19,600 --> 00:19:22,630
like saying I'm just going to fine-tune
this these particular layers which is

934
00:19:22,630 --> 00:19:22,640
this these particular layers which is
 

935
00:19:22,640 --> 00:19:23,830
this these particular layers which is
still reasonable. They would experience

936
00:19:23,830 --> 00:19:23,840
still reasonable. They would experience
 

937
00:19:23,840 --> 00:19:25,110
still reasonable. They would experience
that you know sometimes for certain

938
00:19:25,110 --> 00:19:25,120
that you know sometimes for certain
 

939
00:19:25,120 --> 00:19:26,950
that you know sometimes for certain
kinds of problems tuning certain layers

940
00:19:26,950 --> 00:19:26,960
kinds of problems tuning certain layers
 

941
00:19:26,960 --> 00:19:29,110
kinds of problems tuning certain layers
was important

942
00:19:29,110 --> 00:19:29,120
was important
 

943
00:19:29,120 --> 00:19:31,990
was important
as compared to others but this is like a

944
00:19:31,990 --> 00:19:32,000
as compared to others but this is like a
 

945
00:19:32,000 --> 00:19:35,990
as compared to others but this is like a
more of a generic kind of recipe.

946
00:19:35,990 --> 00:19:36,000
more of a generic kind of recipe.
 

947
00:19:36,000 --> 00:19:40,549
more of a generic kind of recipe.
So the idea of Allora is to say that

948
00:19:40,549 --> 00:19:40,559
So the idea of Allora is to say that
 

949
00:19:40,559 --> 00:19:44,390
So the idea of Allora is to say that
when I have a pre-trained model, what I

950
00:19:44,390 --> 00:19:44,400
when I have a pre-trained model, what I
 

951
00:19:44,400 --> 00:19:47,029
when I have a pre-trained model, what I
have is a whole bunch of weight

952
00:19:47,029 --> 00:19:47,039
have is a whole bunch of weight
 

953
00:19:47,039 --> 00:19:48,630
have is a whole bunch of weight
matrices.

954
00:19:48,630 --> 00:19:48,640
matrices.
 

955
00:19:48,640 --> 00:19:50,470
matrices.
Okay, lots and lots of weight matrices.

956
00:19:50,470 --> 00:19:50,480
Okay, lots and lots of weight matrices.
 

957
00:19:50,480 --> 00:19:52,150
Okay, lots and lots of weight matrices.
I also have bias vectors, but I have lot

958
00:19:52,150 --> 00:19:52,160
I also have bias vectors, but I have lot
 

959
00:19:52,160 --> 00:19:53,510
I also have bias vectors, but I have lot
whole bunches of weight matrices and

960
00:19:53,510 --> 00:19:53,520
whole bunches of weight matrices and
 

961
00:19:53,520 --> 00:19:55,830
whole bunches of weight matrices and
bias vectors. Most of the parameter

962
00:19:55,830 --> 00:19:55,840
bias vectors. Most of the parameter
 

963
00:19:55,840 --> 00:20:00,070
bias vectors. Most of the parameter
count is in these weight matrices.

964
00:20:00,070 --> 00:20:00,080
count is in these weight matrices.
 

965
00:20:00,080 --> 00:20:02,710
count is in these weight matrices.
Okay.

966
00:20:02,710 --> 00:20:02,720
Okay.
 

967
00:20:02,720 --> 00:20:08,230
Okay.
Now,

968
00:20:08,230 --> 00:20:08,240

 

969
00:20:08,240 --> 00:20:10,470

a quick aside.

970
00:20:10,470 --> 00:20:10,480
a quick aside.
 

971
00:20:10,480 --> 00:20:12,070
a quick aside.
I didn't plan on saying this, but I just

972
00:20:12,070 --> 00:20:12,080
I didn't plan on saying this, but I just
 

973
00:20:12,080 --> 00:20:13,110
I didn't plan on saying this, but I just
thought of it, so I should probably

974
00:20:13,110 --> 00:20:13,120
thought of it, so I should probably
 

975
00:20:13,120 --> 00:20:23,350
thought of it, so I should probably
mention it. So, note

976
00:20:23,350 --> 00:20:23,360

 

977
00:20:23,360 --> 00:20:29,590

embedded

978
00:20:29,590 --> 00:20:29,600

 

979
00:20:29,600 --> 00:20:31,350

better

980
00:20:31,350 --> 00:20:31,360
better
 

981
00:20:31,360 --> 00:20:32,950
better
slash

982
00:20:32,950 --> 00:20:32,960
slash
 

983
00:20:32,960 --> 00:20:39,029
slash
output layer

984
00:20:39,029 --> 00:20:39,039

 

985
00:20:39,039 --> 00:20:43,190

are different.

986
00:20:43,190 --> 00:20:43,200

 

987
00:20:43,200 --> 00:20:45,830

Okay, so what do I mean by that? We have

988
00:20:45,830 --> 00:20:45,840
Okay, so what do I mean by that? We have
 

989
00:20:45,840 --> 00:20:47,430
Okay, so what do I mean by that? We have
these big models, right? So I'll use the

990
00:20:47,430 --> 00:20:47,440
these big models, right? So I'll use the
 

991
00:20:47,440 --> 00:20:50,630
these big models, right? So I'll use the
transformer model as the example.

992
00:20:50,630 --> 00:20:50,640
transformer model as the example.
 

993
00:20:50,640 --> 00:20:53,270
transformer model as the example.
So

994
00:20:53,270 --> 00:20:53,280
So
 

995
00:20:53,280 --> 00:20:56,310
So
you have this embedder and this debtor

996
00:20:56,310 --> 00:20:56,320
you have this embedder and this debtor
 

997
00:20:56,320 --> 00:20:58,789
you have this embedder and this debtor
and you also have inside here you have

998
00:20:58,789 --> 00:20:58,799
and you also have inside here you have
 

999
00:20:58,799 --> 00:21:02,710
and you also have inside here you have
you know norms attention state space

1000
00:21:02,710 --> 00:21:02,720
you know norms attention state space
 

1001
00:21:02,720 --> 00:21:04,710
you know norms attention state space
models

1002
00:21:04,710 --> 00:21:04,720
models
 

1003
00:21:04,720 --> 00:21:07,270
models
feed forward networks of different types

1004
00:21:07,270 --> 00:21:07,280
feed forward networks of different types
 

1005
00:21:07,280 --> 00:21:10,950
feed forward networks of different types
of stuff. Everything inside here there's

1006
00:21:10,950 --> 00:21:10,960
of stuff. Everything inside here there's
 

1007
00:21:10,960 --> 00:21:14,230
of stuff. Everything inside here there's
bias type things and there's matrix type

1008
00:21:14,230 --> 00:21:14,240
bias type things and there's matrix type
 

1009
00:21:14,240 --> 00:21:17,510
bias type things and there's matrix type
things and you know where those are

1010
00:21:17,510 --> 00:21:17,520
things and you know where those are
 

1011
00:21:17,520 --> 00:21:20,230
things and you know where those are
right everyone good with that and the

1012
00:21:20,230 --> 00:21:20,240
right everyone good with that and the
 

1013
00:21:20,240 --> 00:21:22,870
right everyone good with that and the
matrices actually are matrices what do I

1014
00:21:22,870 --> 00:21:22,880
matrices actually are matrices what do I
 

1015
00:21:22,880 --> 00:21:24,870
matrices actually are matrices what do I
mean by matrices being matrices I mean

1016
00:21:24,870 --> 00:21:24,880
mean by matrices being matrices I mean
 

1017
00:21:24,880 --> 00:21:25,990
mean by matrices being matrices I mean
you might say well the matrix is a

1018
00:21:25,990 --> 00:21:26,000
you might say well the matrix is a
 

1019
00:21:26,000 --> 00:21:27,909
you might say well the matrix is a
matrix well I mean is the matrixes

1020
00:21:27,909 --> 00:21:27,919
matrix well I mean is the matrixes
 

1021
00:21:27,919 --> 00:21:31,669
matrix well I mean is the matrixes
acting in a matrix- like way it's you

1022
00:21:31,669 --> 00:21:31,679
acting in a matrix- like way it's you
 

1023
00:21:31,679 --> 00:21:33,909
acting in a matrix- like way it's you
know it takes vectors and it multiplies

1024
00:21:33,909 --> 00:21:33,919
know it takes vectors and it multiplies
 

1025
00:21:33,919 --> 00:21:36,789
know it takes vectors and it multiplies
them and the entire vector the different

1026
00:21:36,789 --> 00:21:36,799
them and the entire vector the different
 

1027
00:21:36,799 --> 00:21:38,470
them and the entire vector the different
components of the entire vector while

1028
00:21:38,470 --> 00:21:38,480
components of the entire vector while
 

1029
00:21:38,480 --> 00:21:40,310
components of the entire vector while
not the same kind of have the same

1030
00:21:40,310 --> 00:21:40,320
not the same kind of have the same
 

1031
00:21:40,320 --> 00:21:43,510
not the same kind of have the same
qualitative character. Uh and you could

1032
00:21:43,510 --> 00:21:43,520
qualitative character. Uh and you could
 

1033
00:21:43,520 --> 00:21:46,149
qualitative character. Uh and you could
imagine key thing is you could imagine

1034
00:21:46,149 --> 00:21:46,159
imagine key thing is you could imagine
 

1035
00:21:46,159 --> 00:21:49,990
imagine key thing is you could imagine
rotating it and changing coordinates.

1036
00:21:49,990 --> 00:21:50,000
rotating it and changing coordinates.
 

1037
00:21:50,000 --> 00:21:51,669
rotating it and changing coordinates.
Okay, you could imagine rotating and

1038
00:21:51,669 --> 00:21:51,679
Okay, you could imagine rotating and
 

1039
00:21:51,679 --> 00:21:53,110
Okay, you could imagine rotating and
changing coordinates and it' still kind

1040
00:21:53,110 --> 00:21:53,120
changing coordinates and it' still kind
 

1041
00:21:53,120 --> 00:21:58,950
changing coordinates and it' still kind
of be the same. Everyone with me? Okay,

1042
00:21:58,950 --> 00:21:58,960
of be the same. Everyone with me? Okay,
 

1043
00:21:58,960 --> 00:22:00,870
of be the same. Everyone with me? Okay,
so you have all these matrix matrix type

1044
00:22:00,870 --> 00:22:00,880
so you have all these matrix matrix type
 

1045
00:22:00,880 --> 00:22:04,470
so you have all these matrix matrix type
things here inside here.

1046
00:22:04,470 --> 00:22:04,480
things here inside here.
 

1047
00:22:04,480 --> 00:22:08,310
things here inside here.
the embed and the debeter. Although you

1048
00:22:08,310 --> 00:22:08,320
the embed and the debeter. Although you
 

1049
00:22:08,320 --> 00:22:09,909
the embed and the debeter. Although you
might think of them and represent them

1050
00:22:09,909 --> 00:22:09,919
might think of them and represent them
 

1051
00:22:09,919 --> 00:22:13,990
might think of them and represent them
in PyTorch as a big matrix, they're not

1052
00:22:13,990 --> 00:22:14,000
in PyTorch as a big matrix, they're not
 

1053
00:22:14,000 --> 00:22:19,190
in PyTorch as a big matrix, they're not
quite the same in their matrixiness

1054
00:22:19,190 --> 00:22:19,200
quite the same in their matrixiness
 

1055
00:22:19,200 --> 00:22:20,789
quite the same in their matrixiness
because you can't really think about

1056
00:22:20,789 --> 00:22:20,799
because you can't really think about
 

1057
00:22:20,799 --> 00:22:23,590
because you can't really think about
potentially rotating them and being the

1058
00:22:23,590 --> 00:22:23,600
potentially rotating them and being the
 

1059
00:22:23,600 --> 00:22:27,110
potentially rotating them and being the
same on the input side because the DMB

1060
00:22:27,110 --> 00:22:27,120
same on the input side because the DMB
 

1061
00:22:27,120 --> 00:22:28,789
same on the input side because the DMB
the embedtor for example takes one hot

1062
00:22:28,789 --> 00:22:28,799
the embedtor for example takes one hot
 

1063
00:22:28,799 --> 00:22:30,230
the embedtor for example takes one hot
vectors in. It's made for one hot

1064
00:22:30,230 --> 00:22:30,240
vectors in. It's made for one hot
 

1065
00:22:30,240 --> 00:22:32,870
vectors in. It's made for one hot
vectors. The D embedder is generating

1066
00:22:32,870 --> 00:22:32,880
vectors. The D embedder is generating
 

1067
00:22:32,880 --> 00:22:36,070
vectors. The D embedder is generating
scores for different tokens,

1068
00:22:36,070 --> 00:22:36,080
scores for different tokens,
 

1069
00:22:36,080 --> 00:22:37,990
scores for different tokens,
right? It's it's really the embedder and

1070
00:22:37,990 --> 00:22:38,000
right? It's it's really the embedder and
 

1071
00:22:38,000 --> 00:22:39,510
right? It's it's really the embedder and
the D embedter are collections of

1072
00:22:39,510 --> 00:22:39,520
the D embedter are collections of
 

1073
00:22:39,520 --> 00:22:41,029
the D embedter are collections of
vectors.

1074
00:22:41,029 --> 00:22:41,039
vectors.
 

1075
00:22:41,039 --> 00:22:43,590
vectors.
They're not really a matrix. I mean,

1076
00:22:43,590 --> 00:22:43,600
They're not really a matrix. I mean,
 

1077
00:22:43,600 --> 00:22:45,110
They're not really a matrix. I mean,
they are a matrix, but they're also kind

1078
00:22:45,110 --> 00:22:45,120
they are a matrix, but they're also kind
 

1079
00:22:45,120 --> 00:22:48,070
they are a matrix, but they're also kind
of not. So, we talk about low rank

1080
00:22:48,070 --> 00:22:48,080
of not. So, we talk about low rank
 

1081
00:22:48,080 --> 00:22:50,390
of not. So, we talk about low rank
updates. This has this is for some

1082
00:22:50,390 --> 00:22:50,400
updates. This has this is for some
 

1083
00:22:50,400 --> 00:22:52,149
updates. This has this is for some
things that are matrix- like in their

1084
00:22:52,149 --> 00:22:52,159
things that are matrix- like in their
 

1085
00:22:52,159 --> 00:22:55,190
things that are matrix- like in their
nature. One doesn't do Lauras for the

1086
00:22:55,190 --> 00:22:55,200
nature. One doesn't do Lauras for the
 

1087
00:22:55,200 --> 00:23:00,390
nature. One doesn't do Lauras for the
embed or the debtor.

1088
00:23:00,390 --> 00:23:00,400

 

1089
00:23:00,400 --> 00:23:01,909

If you're going to have have to have a

1090
00:23:01,909 --> 00:23:01,919
If you're going to have have to have a
 

1091
00:23:01,919 --> 00:23:04,390
If you're going to have have to have a
custom output layer, you will just make

1092
00:23:04,390 --> 00:23:04,400
custom output layer, you will just make
 

1093
00:23:04,400 --> 00:23:06,789
custom output layer, you will just make
it custom.

1094
00:23:06,789 --> 00:23:06,799
it custom.
 

1095
00:23:06,799 --> 00:23:08,950
it custom.
And if you want to adjust the embed, you

1096
00:23:08,950 --> 00:23:08,960
And if you want to adjust the embed, you
 

1097
00:23:08,960 --> 00:23:10,870
And if you want to adjust the embed, you
can have special tokens you add, but you

1098
00:23:10,870 --> 00:23:10,880
can have special tokens you add, but you
 

1099
00:23:10,880 --> 00:23:12,950
can have special tokens you add, but you
don't like do a lower on the embed

1100
00:23:12,950 --> 00:23:12,960
don't like do a lower on the embed
 

1101
00:23:12,960 --> 00:23:15,510
don't like do a lower on the embed
typically.

1102
00:23:15,510 --> 00:23:15,520
typically.
 

1103
00:23:15,520 --> 00:23:17,990
typically.
Okay. So aside, so now you have a

1104
00:23:17,990 --> 00:23:18,000
Okay. So aside, so now you have a
 

1105
00:23:18,000 --> 00:23:20,789
Okay. So aside, so now you have a
matrix. So for example, think about an

1106
00:23:20,789 --> 00:23:20,799
matrix. So for example, think about an
 

1107
00:23:20,799 --> 00:23:25,510
matrix. So for example, think about an
MLP matrix, right? In an MLP matrix, a

1108
00:23:25,510 --> 00:23:25,520
MLP matrix, right? In an MLP matrix, a
 

1109
00:23:25,520 --> 00:23:27,909
MLP matrix, right? In an MLP matrix, a
standard MLP, you have something that

1110
00:23:27,909 --> 00:23:27,919
standard MLP, you have something that
 

1111
00:23:27,919 --> 00:23:32,630
standard MLP, you have something that
will take uh a 4,000 long vector and

1112
00:23:32,630 --> 00:23:32,640
will take uh a 4,000 long vector and
 

1113
00:23:32,640 --> 00:23:36,070
will take uh a 4,000 long vector and
expand it to 16,000, four times larger.

1114
00:23:36,070 --> 00:23:36,080
expand it to 16,000, four times larger.
 

1115
00:23:36,080 --> 00:23:39,750
expand it to 16,000, four times larger.
You might then relue it or in a current

1116
00:23:39,750 --> 00:23:39,760
You might then relue it or in a current
 

1117
00:23:39,760 --> 00:23:42,149
You might then relue it or in a current
modern very modern approach, ReLU square

1118
00:23:42,149 --> 00:23:42,159
modern very modern approach, ReLU square
 

1119
00:23:42,159 --> 00:23:44,950
modern very modern approach, ReLU square
it. And then you have another MLP on the

1120
00:23:44,950 --> 00:23:44,960
it. And then you have another MLP on the
 

1121
00:23:44,960 --> 00:23:46,549
it. And then you have another MLP on the
top that'll like shrink it back down to

1122
00:23:46,549 --> 00:23:46,559
top that'll like shrink it back down to
 

1123
00:23:46,559 --> 00:23:49,110
top that'll like shrink it back down to
4,000. Everyone remember? So this

1124
00:23:49,110 --> 00:23:49,120
4,000. Everyone remember? So this
 

1125
00:23:49,120 --> 00:23:53,990
4,000. Everyone remember? So this
example of a weight matrix. So 4,000 by

1126
00:23:53,990 --> 00:23:54,000
example of a weight matrix. So 4,000 by
 

1127
00:23:54,000 --> 00:23:57,430
example of a weight matrix. So 4,000 by
16,000 has 64 million parameters in here

1128
00:23:57,430 --> 00:23:57,440
16,000 has 64 million parameters in here
 

1129
00:23:57,440 --> 00:23:59,990
16,000 has 64 million parameters in here
in this matrix. And there's many of

1130
00:23:59,990 --> 00:24:00,000
in this matrix. And there's many of
 

1131
00:24:00,000 --> 00:24:01,750
in this matrix. And there's many of
these in your model. That's how models

1132
00:24:01,750 --> 00:24:01,760
these in your model. That's how models
 

1133
00:24:01,760 --> 00:24:06,789
these in your model. That's how models
get big.

1134
00:24:06,789 --> 00:24:06,799

 

1135
00:24:06,799 --> 00:24:09,430

So what you can do in Laura, the key

1136
00:24:09,430 --> 00:24:09,440
So what you can do in Laura, the key
 

1137
00:24:09,440 --> 00:24:11,830
So what you can do in Laura, the key
idea of a Laura is to say that instead

1138
00:24:11,830 --> 00:24:11,840
idea of a Laura is to say that instead
 

1139
00:24:11,840 --> 00:24:13,750
idea of a Laura is to say that instead
of trying to update every parameter

1140
00:24:13,750 --> 00:24:13,760
of trying to update every parameter
 

1141
00:24:13,760 --> 00:24:17,029
of trying to update every parameter
inside that matrix independently,

1142
00:24:17,029 --> 00:24:17,039
inside that matrix independently,
 

1143
00:24:17,039 --> 00:24:18,630
inside that matrix independently,
you're going to let all the parameters

1144
00:24:18,630 --> 00:24:18,640
you're going to let all the parameters
 

1145
00:24:18,640 --> 00:24:22,149
you're going to let all the parameters
inside that matrix move, but only in a

1146
00:24:22,149 --> 00:24:22,159
inside that matrix move, but only in a
 

1147
00:24:22,159 --> 00:24:25,669
inside that matrix move, but only in a
low rank way. So your updates are just

1148
00:24:25,669 --> 00:24:25,679
low rank way. So your updates are just
 

1149
00:24:25,679 --> 00:24:27,909
low rank way. So your updates are just
going to be

1150
00:24:27,909 --> 00:24:27,919
going to be
 

1151
00:24:27,919 --> 00:24:32,149
going to be
uh delta W's and delta W's are required

1152
00:24:32,149 --> 00:24:32,159
uh delta W's and delta W's are required
 

1153
00:24:32,159 --> 00:24:36,950
uh delta W's and delta W's are required
to have their rank be small.

1154
00:24:36,950 --> 00:24:36,960
to have their rank be small.
 

1155
00:24:36,960 --> 00:24:39,750
to have their rank be small.
And what you want is not that every

1156
00:24:39,750 --> 00:24:39,760
And what you want is not that every
 

1157
00:24:39,760 --> 00:24:43,269
And what you want is not that every
individual update is low rank but

1158
00:24:43,269 --> 00:24:43,279
individual update is low rank but
 

1159
00:24:43,279 --> 00:24:46,310
individual update is low rank but
together you might change rank a lot.

1160
00:24:46,310 --> 00:24:46,320
together you might change rank a lot.
 

1161
00:24:46,320 --> 00:24:47,909
together you might change rank a lot.
You might change might change the matrix

1162
00:24:47,909 --> 00:24:47,919
You might change might change the matrix
 

1163
00:24:47,919 --> 00:24:50,070
You might change might change the matrix
a lot in a high rank way. Remember the

1164
00:24:50,070 --> 00:24:50,080
a lot in a high rank way. Remember the
 

1165
00:24:50,080 --> 00:24:52,230
a lot in a high rank way. Remember the
argument for m up. So remember in the m

1166
00:24:52,230 --> 00:24:52,240
argument for m up. So remember in the m
 

1167
00:24:52,240 --> 00:24:55,350
argument for m up. So remember in the m
argument we said that by default if you

1168
00:24:55,350 --> 00:24:55,360
argument we said that by default if you
 

1169
00:24:55,360 --> 00:24:57,590
argument we said that by default if you
have batch size one your weight matrix

1170
00:24:57,590 --> 00:24:57,600
have batch size one your weight matrix
 

1171
00:24:57,600 --> 00:25:00,149
have batch size one your weight matrix
update is going to be rank one. Remember

1172
00:25:00,149 --> 00:25:00,159
update is going to be rank one. Remember
 

1173
00:25:00,159 --> 00:25:01,990
update is going to be rank one. Remember
that argument. But when you apply it

1174
00:25:01,990 --> 00:25:02,000
that argument. But when you apply it
 

1175
00:25:02,000 --> 00:25:04,310
that argument. But when you apply it
over and over again, the net update is

1176
00:25:04,310 --> 00:25:04,320
over and over again, the net update is
 

1177
00:25:04,320 --> 00:25:06,390
over and over again, the net update is
not rank one.

1178
00:25:06,390 --> 00:25:06,400
not rank one.
 

1179
00:25:06,400 --> 00:25:08,630
not rank one.
Here we're going to say that throughout

1180
00:25:08,630 --> 00:25:08,640
Here we're going to say that throughout
 

1181
00:25:08,640 --> 00:25:11,750
Here we're going to say that throughout
the entire adaptation process,

1182
00:25:11,750 --> 00:25:11,760
the entire adaptation process,
 

1183
00:25:11,760 --> 00:25:13,830
the entire adaptation process,
we're only going to make a low rank

1184
00:25:13,830 --> 00:25:13,840
we're only going to make a low rank
 

1185
00:25:13,840 --> 00:25:16,470
we're only going to make a low rank
update.

1186
00:25:16,470 --> 00:25:16,480
update.
 

1187
00:25:16,480 --> 00:25:18,149
update.
Okay?

1188
00:25:18,149 --> 00:25:18,159
Okay?
 

1189
00:25:18,159 --> 00:25:22,149
Okay?
So this part we're going to freeze. This

1190
00:25:22,149 --> 00:25:22,159
So this part we're going to freeze. This
 

1191
00:25:22,159 --> 00:25:23,909
So this part we're going to freeze. This
is not going to change during training.

1192
00:25:23,909 --> 00:25:23,919
is not going to change during training.
 

1193
00:25:23,919 --> 00:25:25,990
is not going to change during training.
And only this delta W will change during

1194
00:25:25,990 --> 00:25:26,000
And only this delta W will change during
 

1195
00:25:26,000 --> 00:25:28,390
And only this delta W will change during
training. And delta W is required to

1196
00:25:28,390 --> 00:25:28,400
training. And delta W is required to
 

1197
00:25:28,400 --> 00:25:32,390
training. And delta W is required to
have a rank that's small.

1198
00:25:32,390 --> 00:25:32,400
have a rank that's small.
 

1199
00:25:32,400 --> 00:25:34,789
have a rank that's small.
This is the idea.

1200
00:25:34,789 --> 00:25:34,799
This is the idea.
 

1201
00:25:34,799 --> 00:25:36,630
This is the idea.
So,

1202
00:25:36,630 --> 00:25:36,640
So,
 

1203
00:25:36,640 --> 00:25:38,149
So,
so in this case, you know, default rank

1204
00:25:38,149 --> 00:25:38,159
so in this case, you know, default rank
 

1205
00:25:38,159 --> 00:25:41,029
so in this case, you know, default rank
of this matrix is like 4,000. Um, this

1206
00:25:41,029 --> 00:25:41,039
of this matrix is like 4,000. Um, this
 

1207
00:25:41,039 --> 00:25:45,990
of this matrix is like 4,000. Um, this
will be much smaller like 20 or five or

1208
00:25:45,990 --> 00:25:46,000
will be much smaller like 20 or five or
 

1209
00:25:46,000 --> 00:25:50,470
will be much smaller like 20 or five or
one something much smaller.

1210
00:25:50,470 --> 00:25:50,480
one something much smaller.
 

1211
00:25:50,480 --> 00:25:52,310
one something much smaller.
This is the idea. So now how do you do

1212
00:25:52,310 --> 00:25:52,320
This is the idea. So now how do you do
 

1213
00:25:52,320 --> 00:25:54,630
This is the idea. So now how do you do
this?

1214
00:25:54,630 --> 00:25:54,640
this?
 

1215
00:25:54,640 --> 00:25:56,870
this?
So

1216
00:25:56,870 --> 00:25:56,880
So
 

1217
00:25:56,880 --> 00:25:59,430
So
the standard approach and there are

1218
00:25:59,430 --> 00:25:59,440
the standard approach and there are
 

1219
00:25:59,440 --> 00:26:01,430
the standard approach and there are
emerging approaches that go a little bit

1220
00:26:01,430 --> 00:26:01,440
emerging approaches that go a little bit
 

1221
00:26:01,440 --> 00:26:02,950
emerging approaches that go a little bit
beyond away from this standard approach

1222
00:26:02,950 --> 00:26:02,960
beyond away from this standard approach
 

1223
00:26:02,960 --> 00:26:03,990
beyond away from this standard approach
but I'm going to talk about the standard

1224
00:26:03,990 --> 00:26:04,000
but I'm going to talk about the standard
 

1225
00:26:04,000 --> 00:26:06,789
but I'm going to talk about the standard
approach says

1226
00:26:06,789 --> 00:26:06,799
approach says
 

1227
00:26:06,799 --> 00:26:08,549
approach says
if I'm going to make an update I have to

1228
00:26:08,549 --> 00:26:08,559
if I'm going to make an update I have to
 

1229
00:26:08,559 --> 00:26:09,990
if I'm going to make an update I have to
have parameters because eventually I'm

1230
00:26:09,990 --> 00:26:10,000
have parameters because eventually I'm
 

1231
00:26:10,000 --> 00:26:11,269
have parameters because eventually I'm
going to run gradient descent of some

1232
00:26:11,269 --> 00:26:11,279
going to run gradient descent of some
 

1233
00:26:11,279 --> 00:26:12,630
going to run gradient descent of some
kind and it's going to have to update

1234
00:26:12,630 --> 00:26:12,640
kind and it's going to have to update
 

1235
00:26:12,640 --> 00:26:14,950
kind and it's going to have to update
parameters. So I can't just say I want

1236
00:26:14,950 --> 00:26:14,960
parameters. So I can't just say I want
 

1237
00:26:14,960 --> 00:26:17,909
parameters. So I can't just say I want
delta w to be a rank r matrix. I have to

1238
00:26:17,909 --> 00:26:17,919
delta w to be a rank r matrix. I have to
 

1239
00:26:17,919 --> 00:26:19,750
delta w to be a rank r matrix. I have to
have a representation that makes it be

1240
00:26:19,750 --> 00:26:19,760
have a representation that makes it be
 

1241
00:26:19,760 --> 00:26:22,230
have a representation that makes it be
rank r. So the easiest way to make make

1242
00:26:22,230 --> 00:26:22,240
rank r. So the easiest way to make make
 

1243
00:26:22,240 --> 00:26:25,350
rank r. So the easiest way to make make
a matrix be rank R is to say that it's a

1244
00:26:25,350 --> 00:26:25,360
a matrix be rank R is to say that it's a
 

1245
00:26:25,360 --> 00:26:28,070
a matrix be rank R is to say that it's a
product of two matrices whose structural

1246
00:26:28,070 --> 00:26:28,080
product of two matrices whose structural
 

1247
00:26:28,080 --> 00:26:30,230
product of two matrices whose structural
rank cannot exceed R. Like you look at

1248
00:26:30,230 --> 00:26:30,240
rank cannot exceed R. Like you look at
 

1249
00:26:30,240 --> 00:26:31,510
rank cannot exceed R. Like you look at
them and there's no way that things rank

1250
00:26:31,510 --> 00:26:31,520
them and there's no way that things rank
 

1251
00:26:31,520 --> 00:26:33,430
them and there's no way that things rank
can exceed R.

1252
00:26:33,430 --> 00:26:33,440
can exceed R.
 

1253
00:26:33,440 --> 00:26:35,669
can exceed R.
And so at that once you've made that

1254
00:26:35,669 --> 00:26:35,679
And so at that once you've made that
 

1255
00:26:35,679 --> 00:26:37,669
And so at that once you've made that
decision,

1256
00:26:37,669 --> 00:26:37,679
decision,
 

1257
00:26:37,679 --> 00:26:39,590
decision,
the choices are made for you. Okay, so

1258
00:26:39,590 --> 00:26:39,600
the choices are made for you. Okay, so
 

1259
00:26:39,600 --> 00:26:43,110
the choices are made for you. Okay, so
if we wanted this matrix to have 4,000

1260
00:26:43,110 --> 00:26:43,120
if we wanted this matrix to have 4,000
 

1261
00:26:43,120 --> 00:26:45,590
if we wanted this matrix to have 4,000
uh different columns and 16,000 rows,

1262
00:26:45,590 --> 00:26:45,600
uh different columns and 16,000 rows,
 

1263
00:26:45,600 --> 00:26:50,549
uh different columns and 16,000 rows,
then we can write delta W as B * A

1264
00:26:50,549 --> 00:26:50,559
then we can write delta W as B * A
 

1265
00:26:50,559 --> 00:26:54,390
then we can write delta W as B * A
where B is going to have 16,000 rows and

1266
00:26:54,390 --> 00:26:54,400
where B is going to have 16,000 rows and
 

1267
00:26:54,400 --> 00:26:58,149
where B is going to have 16,000 rows and
R and A is going to have 4,000 columns

1268
00:26:58,149 --> 00:26:58,159
R and A is going to have 4,000 columns
 

1269
00:26:58,159 --> 00:27:01,110
R and A is going to have 4,000 columns
and then the number

1270
00:27:01,110 --> 00:27:01,120
and then the number
 

1271
00:27:01,120 --> 00:27:03,830
and then the number
R adjust to whatever you want the rank

1272
00:27:03,830 --> 00:27:03,840
R adjust to whatever you want the rank
 

1273
00:27:03,840 --> 00:27:05,830
R adjust to whatever you want the rank
uh to be.

1274
00:27:05,830 --> 00:27:05,840
uh to be.
 

1275
00:27:05,840 --> 00:27:08,390
uh to be.
Everyone with me here?

1276
00:27:08,390 --> 00:27:08,400
Everyone with me here?
 

1277
00:27:08,400 --> 00:27:11,029
Everyone with me here?
So

1278
00:27:11,029 --> 00:27:11,039
So
 

1279
00:27:11,039 --> 00:27:12,710
So
ask a question if you don't understand

1280
00:27:12,710 --> 00:27:12,720
ask a question if you don't understand
 

1281
00:27:12,720 --> 00:27:20,070
ask a question if you don't understand
what's going on here.

1282
00:27:20,070 --> 00:27:20,080

 

1283
00:27:20,080 --> 00:27:24,549

So what does this mean?

1284
00:27:24,549 --> 00:27:24,559
So what does this mean?
 

1285
00:27:24,559 --> 00:27:28,390
So what does this mean?
It means that

1286
00:27:28,390 --> 00:27:28,400

 

1287
00:27:28,400 --> 00:27:30,230

in a graphical way you can think of it

1288
00:27:30,230 --> 00:27:30,240
in a graphical way you can think of it
 

1289
00:27:30,240 --> 00:27:33,669
in a graphical way you can think of it
this way that numbers come in and before

1290
00:27:33,669 --> 00:27:33,679
this way that numbers come in and before
 

1291
00:27:33,679 --> 00:27:35,350
this way that numbers come in and before
these get multiplied a vector came in

1292
00:27:35,350 --> 00:27:35,360
these get multiplied a vector came in
 

1293
00:27:35,360 --> 00:27:38,149
these get multiplied a vector came in
and it got multiplied by warst star.

1294
00:27:38,149 --> 00:27:38,159
and it got multiplied by warst star.
 

1295
00:27:38,159 --> 00:27:41,510
and it got multiplied by warst star.
Instead we're adding another path

1296
00:27:41,510 --> 00:27:41,520
Instead we're adding another path
 

1297
00:27:41,520 --> 00:27:43,830
Instead we're adding another path
uh wherever we have this particular war

1298
00:27:43,830 --> 00:27:43,840
uh wherever we have this particular war
 

1299
00:27:43,840 --> 00:27:47,029
uh wherever we have this particular war
star a parallel path in which first you

1300
00:27:47,029 --> 00:27:47,039
star a parallel path in which first you
 

1301
00:27:47,039 --> 00:27:50,390
star a parallel path in which first you
meet a matrix a and then you meet matrix

1302
00:27:50,390 --> 00:27:50,400
meet a matrix a and then you meet matrix
 

1303
00:27:50,400 --> 00:27:52,870
meet a matrix a and then you meet matrix
b and then you add together that's what

1304
00:27:52,870 --> 00:27:52,880
b and then you add together that's what
 

1305
00:27:52,880 --> 00:27:55,590
b and then you add together that's what
this means so this part is frozen and

1306
00:27:55,590 --> 00:27:55,600
this means so this part is frozen and
 

1307
00:27:55,600 --> 00:27:59,110
this means so this part is frozen and
this part is learned

1308
00:27:59,110 --> 00:27:59,120
this part is learned
 

1309
00:27:59,120 --> 00:28:02,310
this part is learned
everyone good okay so now I'm going to

1310
00:28:02,310 --> 00:28:02,320
everyone good okay so now I'm going to
 

1311
00:28:02,320 --> 00:28:04,470
everyone good okay so now I'm going to
first tell you what the current sort of

1312
00:28:04,470 --> 00:28:04,480
first tell you what the current sort of
 

1313
00:28:04,480 --> 00:28:06,070
first tell you what the current sort of
best practice is here and then we're

1314
00:28:06,070 --> 00:28:06,080
best practice is here and then we're
 

1315
00:28:06,080 --> 00:28:07,190
best practice is here and then we're
going to like talk through it in more

1316
00:28:07,190 --> 00:28:07,200
going to like talk through it in more
 

1317
00:28:07,200 --> 00:28:09,590
going to like talk through it in more
detail

1318
00:28:09,590 --> 00:28:09,600

 

1319
00:28:09,600 --> 00:28:12,870

Current best practice is you should

1320
00:28:12,870 --> 00:28:12,880
Current best practice is you should
 

1321
00:28:12,880 --> 00:28:15,990
Current best practice is you should
optimize this using an optimizer

1322
00:28:15,990 --> 00:28:16,000
optimize this using an optimizer
 

1323
00:28:16,000 --> 00:28:19,430
optimize this using an optimizer
uh like atomw.

1324
00:28:19,430 --> 00:28:19,440
uh like atomw.
 

1325
00:28:19,440 --> 00:28:23,830
uh like atomw.
You should not be optimizing this using

1326
00:28:23,830 --> 00:28:23,840
You should not be optimizing this using
 

1327
00:28:23,840 --> 00:28:26,789
You should not be optimizing this using
uh by default using optimizers like uh

1328
00:28:26,789 --> 00:28:26,799
uh by default using optimizers like uh
 

1329
00:28:26,799 --> 00:28:30,149
uh by default using optimizers like uh
muon and the like. There's some emerging

1330
00:28:30,149 --> 00:28:30,159
muon and the like. There's some emerging
 

1331
00:28:30,159 --> 00:28:31,510
muon and the like. There's some emerging
work on when you should do that and not

1332
00:28:31,510 --> 00:28:31,520
work on when you should do that and not
 

1333
00:28:31,520 --> 00:28:32,470
work on when you should do that and not
do that, but I'm going to tell you a

1334
00:28:32,470 --> 00:28:32,480
do that, but I'm going to tell you a
 

1335
00:28:32,480 --> 00:28:34,389
do that, but I'm going to tell you a
default if you're wanting to do it. By

1336
00:28:34,389 --> 00:28:34,399
default if you're wanting to do it. By
 

1337
00:28:34,399 --> 00:28:36,870
default if you're wanting to do it. By
default, you should be using a u

1338
00:28:36,870 --> 00:28:36,880
default, you should be using a u
 

1339
00:28:36,880 --> 00:28:39,510
default, you should be using a u
optimized light atom w

1340
00:28:39,510 --> 00:28:39,520
optimized light atom w
 

1341
00:28:39,520 --> 00:28:42,470
optimized light atom w
you will initialize B to be zero and you

1342
00:28:42,470 --> 00:28:42,480
you will initialize B to be zero and you
 

1343
00:28:42,480 --> 00:28:47,110
you will initialize B to be zero and you
initialize A to be non zero.

1344
00:28:47,110 --> 00:28:47,120
initialize A to be non zero.
 

1345
00:28:47,120 --> 00:28:49,909
initialize A to be non zero.
And then you're going to use two

1346
00:28:49,909 --> 00:28:49,919
And then you're going to use two
 

1347
00:28:49,919 --> 00:28:54,070
And then you're going to use two
different learning rates for B and A in

1348
00:28:54,070 --> 00:28:54,080
different learning rates for B and A in
 

1349
00:28:54,080 --> 00:28:56,789
different learning rates for B and A in
the updates in which case you'll learn

1350
00:28:56,789 --> 00:28:56,799
the updates in which case you'll learn
 

1351
00:28:56,799 --> 00:29:00,389
the updates in which case you'll learn
use a larger learning rate for B by some

1352
00:29:00,389 --> 00:29:00,399
use a larger learning rate for B by some
 

1353
00:29:00,399 --> 00:29:03,029
use a larger learning rate for B by some
factor lambda and lambda is something

1354
00:29:03,029 --> 00:29:03,039
factor lambda and lambda is something
 

1355
00:29:03,039 --> 00:29:05,110
factor lambda and lambda is something
which you have to learn for a family of

1356
00:29:05,110 --> 00:29:05,120
which you have to learn for a family of
 

1357
00:29:05,120 --> 00:29:07,110
which you have to learn for a family of
problems. People have published what

1358
00:29:07,110 --> 00:29:07,120
problems. People have published what
 

1359
00:29:07,120 --> 00:29:08,870
problems. People have published what
they think that lambda is like for a

1360
00:29:08,870 --> 00:29:08,880
they think that lambda is like for a
 

1361
00:29:08,880 --> 00:29:10,310
they think that lambda is like for a
certain family of problems. It's like

1362
00:29:10,310 --> 00:29:10,320
certain family of problems. It's like
 

1363
00:29:10,320 --> 00:29:13,990
certain family of problems. It's like
four 10. So things like that is what

1364
00:29:13,990 --> 00:29:14,000
four 10. So things like that is what
 

1365
00:29:14,000 --> 00:29:17,669
four 10. So things like that is what
what it looks like empirically.

1366
00:29:17,669 --> 00:29:17,679
what it looks like empirically.
 

1367
00:29:17,679 --> 00:29:19,830
what it looks like empirically.
then you use for A. But both A and B

1368
00:29:19,830 --> 00:29:19,840
then you use for A. But both A and B
 

1369
00:29:19,840 --> 00:29:22,230
then you use for A. But both A and B
will update.

1370
00:29:22,230 --> 00:29:22,240
will update.
 

1371
00:29:22,240 --> 00:29:23,669
will update.
Okay, so we're going to go into why this

1372
00:29:23,669 --> 00:29:23,679
Okay, so we're going to go into why this
 

1373
00:29:23,679 --> 00:29:25,110
Okay, so we're going to go into why this
might be later, but like put that out

1374
00:29:25,110 --> 00:29:25,120
might be later, but like put that out
 

1375
00:29:25,120 --> 00:29:26,950
might be later, but like put that out
there so you know what the kind of thing

1376
00:29:26,950 --> 00:29:26,960
there so you know what the kind of thing
 

1377
00:29:26,960 --> 00:29:28,549
there so you know what the kind of thing
is that one does when you're doing lower

1378
00:29:28,549 --> 00:29:28,559
is that one does when you're doing lower
 

1379
00:29:28,559 --> 00:29:30,870
is that one does when you're doing lower
training. So we could stop here and just

1380
00:29:30,870 --> 00:29:30,880
training. So we could stop here and just
 

1381
00:29:30,880 --> 00:29:35,110
training. So we could stop here and just
say this is the recipe. Do this for and

1382
00:29:35,110 --> 00:29:35,120
say this is the recipe. Do this for and
 

1383
00:29:35,120 --> 00:29:38,549
say this is the recipe. Do this for and
then this is how you do a fine tune.

1384
00:29:38,549 --> 00:29:38,559
then this is how you do a fine tune.
 

1385
00:29:38,559 --> 00:29:40,950
then this is how you do a fine tune.
Okay, so any question on like what the

1386
00:29:40,950 --> 00:29:40,960
Okay, so any question on like what the
 

1387
00:29:40,960 --> 00:29:43,669
Okay, so any question on like what the
kind of recipe here looks like?

1388
00:29:43,669 --> 00:29:43,679
kind of recipe here looks like?
 

1389
00:29:43,679 --> 00:29:46,710
kind of recipe here looks like?
>> Yeah.

1390
00:29:46,710 --> 00:29:46,720

 

1391
00:29:46,720 --> 00:29:48,710

Yeah, it's a good question. It's a a a

1392
00:29:48,710 --> 00:29:48,720
Yeah, it's a good question. It's a a a
 

1393
00:29:48,720 --> 00:29:50,950
Yeah, it's a good question. It's a a a
topic of active research of trying to

1394
00:29:50,950 --> 00:29:50,960
topic of active research of trying to
 

1395
00:29:50,960 --> 00:29:52,710
topic of active research of trying to
understand it exactly what's going on.

1396
00:29:52,710 --> 00:29:52,720
understand it exactly what's going on.
 

1397
00:29:52,720 --> 00:29:55,350
understand it exactly what's going on.
It's not understood exactly why

1398
00:29:55,350 --> 00:29:55,360
It's not understood exactly why
 

1399
00:29:55,360 --> 00:29:57,430
It's not understood exactly why
different things I can give you one kind

1400
00:29:57,430 --> 00:29:57,440
different things I can give you one kind
 

1401
00:29:57,440 --> 00:30:00,230
different things I can give you one kind
of basic intuition. Um the basic

1402
00:30:00,230 --> 00:30:00,240
of basic intuition. Um the basic
 

1403
00:30:00,240 --> 00:30:05,269
of basic intuition. Um the basic
intuition is that

1404
00:30:05,269 --> 00:30:05,279

 

1405
00:30:05,279 --> 00:30:09,430

it's not clear that

1406
00:30:09,430 --> 00:30:09,440
it's not clear that
 

1407
00:30:09,440 --> 00:30:14,630
it's not clear that
the induced norm of A is the right

1408
00:30:14,630 --> 00:30:14,640
the induced norm of A is the right
 

1409
00:30:14,640 --> 00:30:17,430
the induced norm of A is the right
object given that you know it's going to

1410
00:30:17,430 --> 00:30:17,440
object given that you know it's going to
 

1411
00:30:17,440 --> 00:30:21,110
object given that you know it's going to
be definitely followed by a B

1412
00:30:21,110 --> 00:30:21,120
be definitely followed by a B
 

1413
00:30:21,120 --> 00:30:24,470
be definitely followed by a B
and is acting in the context of this.

1414
00:30:24,470 --> 00:30:24,480
and is acting in the context of this.
 

1415
00:30:24,480 --> 00:30:26,230
and is acting in the context of this.
It's just not clear that that's the

1416
00:30:26,230 --> 00:30:26,240
It's just not clear that that's the
 

1417
00:30:26,240 --> 00:30:29,590
It's just not clear that that's the
right way to look at what the

1418
00:30:29,590 --> 00:30:29,600
right way to look at what the
 

1419
00:30:29,600 --> 00:30:31,909
right way to look at what the
update is because it's not like a

1420
00:30:31,909 --> 00:30:31,919
update is because it's not like a
 

1421
00:30:31,919 --> 00:30:34,870
update is because it's not like a
generic I'm just you know in when you

1422
00:30:34,870 --> 00:30:34,880
generic I'm just you know in when you
 

1423
00:30:34,880 --> 00:30:36,630
generic I'm just you know in when you
use muon generally you have these

1424
00:30:36,630 --> 00:30:36,640
use muon generally you have these
 

1425
00:30:36,640 --> 00:30:38,070
use muon generally you have these
matrices and they're acting and then

1426
00:30:38,070 --> 00:30:38,080
matrices and they're acting and then
 

1427
00:30:38,080 --> 00:30:40,149
matrices and they're acting and then
they fall some kind of some nonlinear

1428
00:30:40,149 --> 00:30:40,159
they fall some kind of some nonlinear
 

1429
00:30:40,159 --> 00:30:42,230
they fall some kind of some nonlinear
thing is happening afterwards or soon

1430
00:30:42,230 --> 00:30:42,240
thing is happening afterwards or soon
 

1431
00:30:42,240 --> 00:30:45,190
thing is happening afterwards or soon
thereafter here it's there's a lot of

1432
00:30:45,190 --> 00:30:45,200
thereafter here it's there's a lot of
 

1433
00:30:45,200 --> 00:30:46,950
thereafter here it's there's a lot of
very linear stuff happening around it

1434
00:30:46,950 --> 00:30:46,960
very linear stuff happening around it
 

1435
00:30:46,960 --> 00:30:49,510
very linear stuff happening around it
and so it's not clear that the intuition

1436
00:30:49,510 --> 00:30:49,520
and so it's not clear that the intuition
 

1437
00:30:49,520 --> 00:30:51,909
and so it's not clear that the intuition
is still holds. Um that said, people

1438
00:30:51,909 --> 00:30:51,919
is still holds. Um that said, people
 

1439
00:30:51,919 --> 00:30:53,350
is still holds. Um that said, people
have tried it and done experiments.

1440
00:30:53,350 --> 00:30:53,360
have tried it and done experiments.
 

1441
00:30:53,360 --> 00:30:54,710
have tried it and done experiments.
There's a very interesting experiment in

1442
00:30:54,710 --> 00:30:54,720
There's a very interesting experiment in
 

1443
00:30:54,720 --> 00:30:58,470
There's a very interesting experiment in
the Kimmy papers where they said that um

1444
00:30:58,470 --> 00:30:58,480
the Kimmy papers where they said that um
 

1445
00:30:58,480 --> 00:30:59,830
the Kimmy papers where they said that um
they're doing a full fine tune. So I

1446
00:30:59,830 --> 00:30:59,840
they're doing a full fine tune. So I
 

1447
00:30:59,840 --> 00:31:00,789
they're doing a full fine tune. So I
don't know if they've done the same

1448
00:31:00,789 --> 00:31:00,799
don't know if they've done the same
 

1449
00:31:00,799 --> 00:31:03,190
don't know if they've done the same
experiment for Allora that a full

1450
00:31:03,190 --> 00:31:03,200
experiment for Allora that a full
 

1451
00:31:03,200 --> 00:31:05,990
experiment for Allora that a full
fine-tune on a model that was trained

1452
00:31:05,990 --> 00:31:06,000
fine-tune on a model that was trained
 

1453
00:31:06,000 --> 00:31:09,669
fine-tune on a model that was trained
with muon using muon works better than a

1454
00:31:09,669 --> 00:31:09,679
with muon using muon works better than a
 

1455
00:31:09,679 --> 00:31:12,950
with muon using muon works better than a
full fine-tune of a model that was

1456
00:31:12,950 --> 00:31:12,960
full fine-tune of a model that was
 

1457
00:31:12,960 --> 00:31:15,830
full fine-tune of a model that was
trained using atom w. But you do the

1458
00:31:15,830 --> 00:31:15,840
trained using atom w. But you do the
 

1459
00:31:15,840 --> 00:31:18,710
trained using atom w. But you do the
fine-tune fully using muon

1460
00:31:18,710 --> 00:31:18,720
fine-tune fully using muon
 

1461
00:31:18,720 --> 00:31:21,909
fine-tune fully using muon
like the same optimizer seems to work a

1462
00:31:21,909 --> 00:31:21,919
like the same optimizer seems to work a
 

1463
00:31:21,919 --> 00:31:24,389
like the same optimizer seems to work a
little bit better. So but it's an

1464
00:31:24,389 --> 00:31:24,399
little bit better. So but it's an
 

1465
00:31:24,399 --> 00:31:25,830
little bit better. So but it's an
emerging area like we don't know the

1466
00:31:25,830 --> 00:31:25,840
emerging area like we don't know the
 

1467
00:31:25,840 --> 00:31:28,070
emerging area like we don't know the
answer. I I don't know the answer. Might

1468
00:31:28,070 --> 00:31:28,080
answer. I I don't know the answer. Might
 

1469
00:31:28,080 --> 00:31:29,669
answer. I I don't know the answer. Might
be some recent papers that that tell me

1470
00:31:29,669 --> 00:31:29,679
be some recent papers that that tell me
 

1471
00:31:29,679 --> 00:31:33,909
be some recent papers that that tell me
the answer but I don't know it. Um

1472
00:31:33,909 --> 00:31:33,919
the answer but I don't know it. Um
 

1473
00:31:33,919 --> 00:31:36,070
the answer but I don't know it. Um
okay.

1474
00:31:36,070 --> 00:31:36,080
okay.
 

1475
00:31:36,080 --> 00:31:38,710
okay.
So that's for full fine tune for Laura.

1476
00:31:38,710 --> 00:31:38,720
So that's for full fine tune for Laura.
 

1477
00:31:38,720 --> 00:31:41,750
So that's for full fine tune for Laura.
I'm not not sure. I I can tell you more

1478
00:31:41,750 --> 00:31:41,760
I'm not not sure. I I can tell you more
 

1479
00:31:41,760 --> 00:31:45,029
I'm not not sure. I I can tell you more
a little bit in a little bit about it.

1480
00:31:45,029 --> 00:31:45,039
a little bit in a little bit about it.
 

1481
00:31:45,039 --> 00:31:49,269
a little bit in a little bit about it.
Okay, so now let's talk about this in a

1482
00:31:49,269 --> 00:31:49,279
Okay, so now let's talk about this in a
 

1483
00:31:49,279 --> 00:31:50,470
Okay, so now let's talk about this in a
little bit more detail. So you know what

1484
00:31:50,470 --> 00:31:50,480
little bit more detail. So you know what
 

1485
00:31:50,480 --> 00:31:52,710
little bit more detail. So you know what
to do in practice, you know, follow this

1486
00:31:52,710 --> 00:31:52,720
to do in practice, you know, follow this
 

1487
00:31:52,720 --> 00:31:54,789
to do in practice, you know, follow this
recipe.

1488
00:31:54,789 --> 00:31:54,799
recipe.
 

1489
00:31:54,799 --> 00:31:57,269
recipe.
So a comment is important which is after

1490
00:31:57,269 --> 00:31:57,279
So a comment is important which is after
 

1491
00:31:57,279 --> 00:31:58,789
So a comment is important which is after
you've done the Laura fine tune. So

1492
00:31:58,789 --> 00:31:58,799
you've done the Laura fine tune. So
 

1493
00:31:58,799 --> 00:32:01,190
you've done the Laura fine tune. So
you've done all this Laura fine tuning,

1494
00:32:01,190 --> 00:32:01,200
you've done all this Laura fine tuning,
 

1495
00:32:01,200 --> 00:32:04,389
you've done all this Laura fine tuning,
you've now got this A and the B and now

1496
00:32:04,389 --> 00:32:04,399
you've now got this A and the B and now
 

1497
00:32:04,399 --> 00:32:08,149
you've now got this A and the B and now
you want to use the model for some new

1498
00:32:08,149 --> 00:32:08,159
you want to use the model for some new
 

1499
00:32:08,159 --> 00:32:09,669
you want to use the model for some new
task.

1500
00:32:09,669 --> 00:32:09,679
task.
 

1501
00:32:09,679 --> 00:32:12,789
task.
You don't have to do this calculation

1502
00:32:12,789 --> 00:32:12,799
You don't have to do this calculation
 

1503
00:32:12,799 --> 00:32:15,590
You don't have to do this calculation
and this calculation.

1504
00:32:15,590 --> 00:32:15,600
and this calculation.
 

1505
00:32:15,600 --> 00:32:18,470
and this calculation.
What you can do is just multiply

1506
00:32:18,470 --> 00:32:18,480
What you can do is just multiply
 

1507
00:32:18,480 --> 00:32:21,909
What you can do is just multiply
A and B together, add them into war and

1508
00:32:21,909 --> 00:32:21,919
A and B together, add them into war and
 

1509
00:32:21,919 --> 00:32:26,230
A and B together, add them into war and
have a new W and just use that new W for

1510
00:32:26,230 --> 00:32:26,240
have a new W and just use that new W for
 

1511
00:32:26,240 --> 00:32:29,990
have a new W and just use that new W for
inference. So Allora fine-tune run at

1512
00:32:29,990 --> 00:32:30,000
inference. So Allora fine-tune run at
 

1513
00:32:30,000 --> 00:32:32,310
inference. So Allora fine-tune run at
inference time costs the same as running

1514
00:32:32,310 --> 00:32:32,320
inference time costs the same as running
 

1515
00:32:32,320 --> 00:32:34,070
inference time costs the same as running
the original model. It's not different.

1516
00:32:34,070 --> 00:32:34,080
the original model. It's not different.
 

1517
00:32:34,080 --> 00:32:35,669
the original model. It's not different.
It doesn't cost you more because you had

1518
00:32:35,669 --> 00:32:35,679
It doesn't cost you more because you had
 

1519
00:32:35,679 --> 00:32:37,830
It doesn't cost you more because you had
this. This is just for during training

1520
00:32:37,830 --> 00:32:37,840
this. This is just for during training
 

1521
00:32:37,840 --> 00:32:39,669
this. This is just for during training
to keep track of these parameters are

1522
00:32:39,669 --> 00:32:39,679
to keep track of these parameters are
 

1523
00:32:39,679 --> 00:32:44,310
to keep track of these parameters are
frozen and this is what's learned.

1524
00:32:44,310 --> 00:32:44,320
frozen and this is what's learned.
 

1525
00:32:44,320 --> 00:32:50,470
frozen and this is what's learned.
Okay.

1526
00:32:50,470 --> 00:32:50,480

 

1527
00:32:50,480 --> 00:32:51,909

Okay. So now I want to talk about

1528
00:32:51,909 --> 00:32:51,919
Okay. So now I want to talk about
 

1529
00:32:51,919 --> 00:32:55,509
Okay. So now I want to talk about
initialization of this. So

1530
00:32:55,509 --> 00:32:55,519
initialization of this. So
 

1531
00:32:55,519 --> 00:32:57,509
initialization of this. So
I've told you this particular uh

1532
00:32:57,509 --> 00:32:57,519
I've told you this particular uh
 

1533
00:32:57,519 --> 00:33:00,549
I've told you this particular uh
initialization is what when does but

1534
00:33:00,549 --> 00:33:00,559
initialization is what when does but
 

1535
00:33:00,559 --> 00:33:02,070
initialization is what when does but
let's talk about if you didn't know that

1536
00:33:02,070 --> 00:33:02,080
let's talk about if you didn't know that
 

1537
00:33:02,080 --> 00:33:04,549
let's talk about if you didn't know that
that was the right answer.

1538
00:33:04,549 --> 00:33:04,559
that was the right answer.
 

1539
00:33:04,559 --> 00:33:07,190
that was the right answer.
You know your default initialization

1540
00:33:07,190 --> 00:33:07,200
You know your default initialization
 

1541
00:33:07,200 --> 00:33:09,350
You know your default initialization
would not be that your default

1542
00:33:09,350 --> 00:33:09,360
would not be that your default
 

1543
00:33:09,360 --> 00:33:11,509
would not be that your default
initializations would be let's try a few

1544
00:33:11,509 --> 00:33:11,519
initializations would be let's try a few
 

1545
00:33:11,519 --> 00:33:14,630
initializations would be let's try a few
initializations. So initialization idea

1546
00:33:14,630 --> 00:33:14,640
initializations. So initialization idea
 

1547
00:33:14,640 --> 00:33:17,830
initializations. So initialization idea
zero would be

1548
00:33:17,830 --> 00:33:17,840
zero would be
 

1549
00:33:17,840 --> 00:33:19,509
zero would be
in it

1550
00:33:19,509 --> 00:33:19,519
in it
 

1551
00:33:19,519 --> 00:33:23,269
in it
a equals 0

1552
00:33:23,269 --> 00:33:23,279
a equals 0
 

1553
00:33:23,279 --> 00:33:27,590
a equals 0
b equals z. Just start with zero.

1554
00:33:27,590 --> 00:33:27,600
b equals z. Just start with zero.
 

1555
00:33:27,600 --> 00:33:32,870
b equals z. Just start with zero.
And why doesn't this work? Anyone know?

1556
00:33:32,870 --> 00:33:32,880
And why doesn't this work? Anyone know?
 

1557
00:33:32,880 --> 00:33:40,630
And why doesn't this work? Anyone know?
Why not?

1558
00:33:40,630 --> 00:33:40,640

 

1559
00:33:40,640 --> 00:33:43,669

Yeah.

1560
00:33:43,669 --> 00:33:43,679

 

1561
00:33:43,679 --> 00:33:56,710

>> Yeah. Gradient will be zero.

1562
00:33:56,710 --> 00:33:56,720

 

1563
00:33:56,720 --> 00:33:58,710

So the gradients are zero because

1564
00:33:58,710 --> 00:33:58,720
So the gradients are zero because
 

1565
00:33:58,720 --> 00:34:00,149
So the gradients are zero because
remember the gradients are all these out

1566
00:34:00,149 --> 00:34:00,159
remember the gradients are all these out
 

1567
00:34:00,159 --> 00:34:02,149
remember the gradients are all these out
outer products right on matrices. You

1568
00:34:02,149 --> 00:34:02,159
outer products right on matrices. You
 

1569
00:34:02,159 --> 00:34:03,590
outer products right on matrices. You
did this in the homework. the outer

1570
00:34:03,590 --> 00:34:03,600
did this in the homework. the outer
 

1571
00:34:03,600 --> 00:34:06,149
did this in the homework. the outer
product of what was the input and the

1572
00:34:06,149 --> 00:34:06,159
product of what was the input and the
 

1573
00:34:06,159 --> 00:34:10,149
product of what was the input and the
gradient's coming back. So on B the

1574
00:34:10,149 --> 00:34:10,159
gradient's coming back. So on B the
 

1575
00:34:10,159 --> 00:34:12,310
gradient's coming back. So on B the
gradient is zero because this one if A

1576
00:34:12,310 --> 00:34:12,320
gradient is zero because this one if A
 

1577
00:34:12,320 --> 00:34:14,550
gradient is zero because this one if A
is zero this is zero and then it doesn't

1578
00:34:14,550 --> 00:34:14,560
is zero this is zero and then it doesn't
 

1579
00:34:14,560 --> 00:34:16,310
is zero this is zero and then it doesn't
matter what you're out outer producting

1580
00:34:16,310 --> 00:34:16,320
matter what you're out outer producting
 

1581
00:34:16,320 --> 00:34:17,829
matter what you're out outer producting
a zero vector with you're going to get a

1582
00:34:17,829 --> 00:34:17,839
a zero vector with you're going to get a
 

1583
00:34:17,839 --> 00:34:20,710
a zero vector with you're going to get a
zero matrix right so the gradients on B

1584
00:34:20,710 --> 00:34:20,720
zero matrix right so the gradients on B
 

1585
00:34:20,720 --> 00:34:22,869
zero matrix right so the gradients on B
are zero

1586
00:34:22,869 --> 00:34:22,879
are zero
 

1587
00:34:22,879 --> 00:34:26,710
are zero
for A. If B is zero, whatever gradient

1588
00:34:26,710 --> 00:34:26,720
for A. If B is zero, whatever gradient
 

1589
00:34:26,720 --> 00:34:29,109
for A. If B is zero, whatever gradient
has back propped through B is now zero.

1590
00:34:29,109 --> 00:34:29,119
has back propped through B is now zero.
 

1591
00:34:29,119 --> 00:34:30,230
has back propped through B is now zero.
And so it doesn't matter there's

1592
00:34:30,230 --> 00:34:30,240
And so it doesn't matter there's
 

1593
00:34:30,240 --> 00:34:32,950
And so it doesn't matter there's
activation here that's zero and the

1594
00:34:32,950 --> 00:34:32,960
activation here that's zero and the
 

1595
00:34:32,960 --> 00:34:34,550
activation here that's zero and the
outer product is going to be zero again.

1596
00:34:34,550 --> 00:34:34,560
outer product is going to be zero again.
 

1597
00:34:34,560 --> 00:34:36,629
outer product is going to be zero again.
So the gradients on A are zero. So both

1598
00:34:36,629 --> 00:34:36,639
So the gradients on A are zero. So both
 

1599
00:34:36,639 --> 00:34:38,629
So the gradients on A are zero. So both
zero doesn't work. The gradients are

1600
00:34:38,629 --> 00:34:38,639
zero doesn't work. The gradients are
 

1601
00:34:38,639 --> 00:34:40,869
zero doesn't work. The gradients are
always zero and you can't get out of

1602
00:34:40,869 --> 00:34:40,879
always zero and you can't get out of
 

1603
00:34:40,879 --> 00:34:42,950
always zero and you can't get out of
that.

1604
00:34:42,950 --> 00:34:42,960
that.
 

1605
00:34:42,960 --> 00:34:45,589
that.
So now let's try that didn't work. So

1606
00:34:45,589 --> 00:34:45,599
So now let's try that didn't work. So
 

1607
00:34:45,599 --> 00:34:50,230
So now let's try that didn't work. So
now let's try the other idea.

1608
00:34:50,230 --> 00:34:50,240

 

1609
00:34:50,240 --> 00:34:55,109

Hit it a

1610
00:34:55,109 --> 00:34:55,119

 

1611
00:34:55,119 --> 00:35:00,950

Xavier B Xavier.

1612
00:35:00,950 --> 00:35:00,960

 

1613
00:35:00,960 --> 00:35:03,990

Okay, so just initialize A and B the

1614
00:35:03,990 --> 00:35:04,000
Okay, so just initialize A and B the
 

1615
00:35:04,000 --> 00:35:05,910
Okay, so just initialize A and B the
same way you'd initialize any matrix.

1616
00:35:05,910 --> 00:35:05,920
same way you'd initialize any matrix.
 

1617
00:35:05,920 --> 00:35:08,630
same way you'd initialize any matrix.
You know, look at the appropriate fan in

1618
00:35:08,630 --> 00:35:08,640
You know, look at the appropriate fan in
 

1619
00:35:08,640 --> 00:35:11,270
You know, look at the appropriate fan in
choose Xavier initialization, choose a

1620
00:35:11,270 --> 00:35:11,280
choose Xavier initialization, choose a
 

1621
00:35:11,280 --> 00:35:14,390
choose Xavier initialization, choose a
Gaussian, just fill it up. Everyone

1622
00:35:14,390 --> 00:35:14,400
Gaussian, just fill it up. Everyone
 

1623
00:35:14,400 --> 00:35:16,230
Gaussian, just fill it up. Everyone
understand what what this idea is? Just

1624
00:35:16,230 --> 00:35:16,240
understand what what this idea is? Just
 

1625
00:35:16,240 --> 00:35:17,349
understand what what this idea is? Just
do it the way you would do anything

1626
00:35:17,349 --> 00:35:17,359
do it the way you would do anything
 

1627
00:35:17,359 --> 00:35:31,589
do it the way you would do anything
else. Why not?

1628
00:35:31,589 --> 00:35:31,599

 

1629
00:35:31,599 --> 00:35:32,870

I want to pick someone different than

1630
00:35:32,870 --> 00:35:32,880
I want to pick someone different than
 

1631
00:35:32,880 --> 00:35:37,109
I want to pick someone different than
people who've answered already.

1632
00:35:37,109 --> 00:35:37,119

 

1633
00:35:37,119 --> 00:35:38,390

So remember this in the context of doing

1634
00:35:38,390 --> 00:35:38,400
So remember this in the context of doing
 

1635
00:35:38,400 --> 00:35:40,790
So remember this in the context of doing
a fine tune. So you have your model,

1636
00:35:40,790 --> 00:35:40,800
a fine tune. So you have your model,
 

1637
00:35:40,800 --> 00:35:43,030
a fine tune. So you have your model,
it's been trained, do some pre some

1638
00:35:43,030 --> 00:35:43,040
it's been trained, do some pre some
 

1639
00:35:43,040 --> 00:35:44,790
it's been trained, do some pre some
pre-training task, it does something,

1640
00:35:44,790 --> 00:35:44,800
pre-training task, it does something,
 

1641
00:35:44,800 --> 00:35:47,109
pre-training task, it does something,
learned a bunch of stuff. Now you've

1642
00:35:47,109 --> 00:35:47,119
learned a bunch of stuff. Now you've
 

1643
00:35:47,119 --> 00:35:49,670
learned a bunch of stuff. Now you've
chosen a bunch of uh matrices that

1644
00:35:49,670 --> 00:35:49,680
chosen a bunch of uh matrices that
 

1645
00:35:49,680 --> 00:35:51,670
chosen a bunch of uh matrices that
you're going to do lauras on. So you're

1646
00:35:51,670 --> 00:35:51,680
you're going to do lauras on. So you're
 

1647
00:35:51,680 --> 00:35:53,109
you're going to do lauras on. So you're
going to choose to adjust adopt those

1648
00:35:53,109 --> 00:35:53,119
going to choose to adjust adopt those
 

1649
00:35:53,119 --> 00:35:55,270
going to choose to adjust adopt those
matrices in the network. You choose a

1650
00:35:55,270 --> 00:35:55,280
matrices in the network. You choose a
 

1651
00:35:55,280 --> 00:35:57,670
matrices in the network. You choose a
particular rank R. You set up these A's

1652
00:35:57,670 --> 00:35:57,680
particular rank R. You set up these A's
 

1653
00:35:57,680 --> 00:35:59,030
particular rank R. You set up these A's
and B's. And now you're going to

1654
00:35:59,030 --> 00:35:59,040
and B's. And now you're going to
 

1655
00:35:59,040 --> 00:36:00,390
and B's. And now you're going to
initialize them. Then you're going to

1656
00:36:00,390 --> 00:36:00,400
initialize them. Then you're going to
 

1657
00:36:00,400 --> 00:36:05,430
initialize them. Then you're going to
run it and do updates. Okay. So now

1658
00:36:05,430 --> 00:36:05,440
run it and do updates. Okay. So now
 

1659
00:36:05,440 --> 00:36:06,950
run it and do updates. Okay. So now
what's wrong with initializing these

1660
00:36:06,950 --> 00:36:06,960
what's wrong with initializing these
 

1661
00:36:06,960 --> 00:36:09,990
what's wrong with initializing these
both using Xavier?

1662
00:36:09,990 --> 00:36:10,000
both using Xavier?
 

1663
00:36:10,000 --> 00:36:11,190
both using Xavier?
>> Yeah.

1664
00:36:11,190 --> 00:36:11,200
>> Yeah.
 

1665
00:36:11,200 --> 00:36:14,470
>> Yeah.
>> Could it be that Xavier was assuming

1666
00:36:14,470 --> 00:36:14,480
>> Could it be that Xavier was assuming
 

1667
00:36:14,480 --> 00:36:15,670
>> Could it be that Xavier was assuming
that

1668
00:36:15,670 --> 00:36:15,680
that
 

1669
00:36:15,680 --> 00:36:26,310
that
standard. But then if you multip

1670
00:36:26,310 --> 00:36:26,320

 

1671
00:36:26,320 --> 00:36:27,510

interesting objection so the interesting

1672
00:36:27,510 --> 00:36:27,520
interesting objection so the interesting
 

1673
00:36:27,520 --> 00:36:28,950
interesting objection so the interesting
objection is that if I do Xavier

1674
00:36:28,950 --> 00:36:28,960
objection is that if I do Xavier
 

1675
00:36:28,960 --> 00:36:30,710
objection is that if I do Xavier
initialization for A and B maybe the

1676
00:36:30,710 --> 00:36:30,720
initialization for A and B maybe the
 

1677
00:36:30,720 --> 00:36:33,430
initialization for A and B maybe the
product is very weird. Okay, that's a

1678
00:36:33,430 --> 00:36:33,440
product is very weird. Okay, that's a
 

1679
00:36:33,440 --> 00:36:35,829
product is very weird. Okay, that's a
good idea. Okay, it turns out the

1680
00:36:35,829 --> 00:36:35,839
good idea. Okay, it turns out the
 

1681
00:36:35,839 --> 00:36:38,470
good idea. Okay, it turns out the
product um will be definitely low rank.

1682
00:36:38,470 --> 00:36:38,480
product um will be definitely low rank.
 

1683
00:36:38,480 --> 00:36:39,910
product um will be definitely low rank.
So that from a matrix point of view is

1684
00:36:39,910 --> 00:36:39,920
So that from a matrix point of view is
 

1685
00:36:39,920 --> 00:36:43,190
So that from a matrix point of view is
definitely weird but it's not um it's

1686
00:36:43,190 --> 00:36:43,200
definitely weird but it's not um it's
 

1687
00:36:43,200 --> 00:36:46,630
definitely weird but it's not um it's
not particularly harmful in any way in

1688
00:36:46,630 --> 00:36:46,640
not particularly harmful in any way in
 

1689
00:36:46,640 --> 00:36:49,430
not particularly harmful in any way in
that a random vector in will come out

1690
00:36:49,430 --> 00:36:49,440
that a random vector in will come out
 

1691
00:36:49,440 --> 00:36:51,589
that a random vector in will come out
using with Xavier initialization was

1692
00:36:51,589 --> 00:36:51,599
using with Xavier initialization was
 

1693
00:36:51,599 --> 00:36:53,349
using with Xavier initialization was
such that a random RMS norm vector in

1694
00:36:53,349 --> 00:36:53,359
such that a random RMS norm vector in
 

1695
00:36:53,359 --> 00:36:55,910
such that a random RMS norm vector in
would have RMS norm out of one that will

1696
00:36:55,910 --> 00:36:55,920
would have RMS norm out of one that will
 

1697
00:36:55,920 --> 00:36:58,230
would have RMS norm out of one that will
still be true because it'll be in RMS

1698
00:36:58,230 --> 00:36:58,240
still be true because it'll be in RMS
 

1699
00:36:58,240 --> 00:37:01,750
still be true because it'll be in RMS
norm one again RMS norm one out so it's

1700
00:37:01,750 --> 00:37:01,760
norm one again RMS norm one out so it's
 

1701
00:37:01,760 --> 00:37:03,990
norm one again RMS norm one out so it's
not weird from the logic of Xavier

1702
00:37:03,990 --> 00:37:04,000
not weird from the logic of Xavier
 

1703
00:37:04,000 --> 00:37:09,349
not weird from the logic of Xavier
initialization per

1704
00:37:09,349 --> 00:37:09,359

 

1705
00:37:09,359 --> 00:37:11,349

Um

1706
00:37:11,349 --> 00:37:11,359
Um
 

1707
00:37:11,359 --> 00:37:12,069
Um
yeah

1708
00:37:12,069 --> 00:37:12,079
yeah
 

1709
00:37:12,079 --> 00:37:14,550
yeah
>> does it have something to do with the R

1710
00:37:14,550 --> 00:37:14,560
>> does it have something to do with the R
 

1711
00:37:14,560 --> 00:37:16,310
>> does it have something to do with the R
is a learnable parameter?

1712
00:37:16,310 --> 00:37:16,320
is a learnable parameter?
 

1713
00:37:16,320 --> 00:37:19,030
is a learnable parameter?
>> So R is not a learnable parameter. R is

1714
00:37:19,030 --> 00:37:19,040
>> So R is not a learnable parameter. R is
 

1715
00:37:19,040 --> 00:37:20,950
>> So R is not a learnable parameter. R is
a parameter that we chose. We say how

1716
00:37:20,950 --> 00:37:20,960
a parameter that we chose. We say how
 

1717
00:37:20,960 --> 00:37:22,710
a parameter that we chose. We say how
many par what is the amount of

1718
00:37:22,710 --> 00:37:22,720
many par what is the amount of
 

1719
00:37:22,720 --> 00:37:24,150
many par what is the amount of
parameters I want. So maybe I should go

1720
00:37:24,150 --> 00:37:24,160
parameters I want. So maybe I should go
 

1721
00:37:24,160 --> 00:37:28,390
parameters I want. So maybe I should go
back a little bit talk about R. Sorry.

1722
00:37:28,390 --> 00:37:28,400
back a little bit talk about R. Sorry.
 

1723
00:37:28,400 --> 00:37:33,109
back a little bit talk about R. Sorry.
So here

1724
00:37:33,109 --> 00:37:33,119

 

1725
00:37:33,119 --> 00:37:35,670

note

1726
00:37:35,670 --> 00:37:35,680
note
 

1727
00:37:35,680 --> 00:37:37,349
note
r

1728
00:37:37,349 --> 00:37:37,359
r
 

1729
00:37:37,359 --> 00:37:39,430
r
oops

1730
00:37:39,430 --> 00:37:39,440
oops
 

1731
00:37:39,440 --> 00:37:44,870
oops
times 16k parameters

1732
00:37:44,870 --> 00:37:44,880

 

1733
00:37:44,880 --> 00:37:47,030

this is

1734
00:37:47,030 --> 00:37:47,040
this is
 

1735
00:37:47,040 --> 00:37:52,710
this is
r * 4k parameters.

1736
00:37:52,710 --> 00:37:52,720

 

1737
00:37:52,720 --> 00:37:54,790

So total

1738
00:37:54,790 --> 00:37:54,800
So total
 

1739
00:37:54,800 --> 00:37:58,790
So total
is

1740
00:37:58,790 --> 00:37:58,800

 

1741
00:37:58,800 --> 00:38:00,390

r

1742
00:38:00,390 --> 00:38:00,400
r
 

1743
00:38:00,400 --> 00:38:03,270
r
* 4k

1744
00:38:03,270 --> 00:38:03,280
* 4k
 

1745
00:38:03,280 --> 00:38:05,990
* 4k
pull the 4k out

1746
00:38:05,990 --> 00:38:06,000
pull the 4k out
 

1747
00:38:06,000 --> 00:38:09,109
pull the 4k out
times

1748
00:38:09,109 --> 00:38:09,119

 

1749
00:38:09,119 --> 00:38:15,510

4 + 1 in this case parameters.

1750
00:38:15,510 --> 00:38:15,520

 

1751
00:38:15,520 --> 00:38:17,750

Okay so stop calculate parameters here.

1752
00:38:17,750 --> 00:38:17,760
Okay so stop calculate parameters here.
 

1753
00:38:17,760 --> 00:38:19,430
Okay so stop calculate parameters here.
Oh a little bigger. Sorry I made it

1754
00:38:19,430 --> 00:38:19,440
Oh a little bigger. Sorry I made it
 

1755
00:38:19,440 --> 00:38:24,470
Oh a little bigger. Sorry I made it
small. Okay. So if I think about this,

1756
00:38:24,470 --> 00:38:24,480
small. Okay. So if I think about this,
 

1757
00:38:24,480 --> 00:38:28,150
small. Okay. So if I think about this,
this is like five R times 4k parameters.

1758
00:38:28,150 --> 00:38:28,160
this is like five R times 4k parameters.
 

1759
00:38:28,160 --> 00:38:30,790
this is like five R times 4k parameters.
So if R is like three, this is far

1760
00:38:30,790 --> 00:38:30,800
So if R is like three, this is far
 

1761
00:38:30,800 --> 00:38:32,550
So if R is like three, this is far
smaller than 64 million. So this is why

1762
00:38:32,550 --> 00:38:32,560
smaller than 64 million. So this is why
 

1763
00:38:32,560 --> 00:38:36,470
smaller than 64 million. So this is why
it's parameter efficient. Okay. So R

1764
00:38:36,470 --> 00:38:36,480
it's parameter efficient. Okay. So R
 

1765
00:38:36,480 --> 00:38:38,630
it's parameter efficient. Okay. So R
adjusts the number of parameters that I

1766
00:38:38,630 --> 00:38:38,640
adjusts the number of parameters that I
 

1767
00:38:38,640 --> 00:38:41,349
adjusts the number of parameters that I
have to tune. Nothing that can have an

1768
00:38:41,349 --> 00:38:41,359
have to tune. Nothing that can have an
 

1769
00:38:41,359 --> 00:38:43,910
have to tune. Nothing that can have an
impact on the number of parameters can

1770
00:38:43,910 --> 00:38:43,920
impact on the number of parameters can
 

1771
00:38:43,920 --> 00:38:46,390
impact on the number of parameters can
be a learnable parameter

1772
00:38:46,390 --> 00:38:46,400
be a learnable parameter
 

1773
00:38:46,400 --> 00:38:49,349
be a learnable parameter
using uh PyTorch

1774
00:38:49,349 --> 00:38:49,359
using uh PyTorch
 

1775
00:38:49,359 --> 00:38:51,109
using uh PyTorch
and standard deep learning gradient

1776
00:38:51,109 --> 00:38:51,119
and standard deep learning gradient
 

1777
00:38:51,119 --> 00:38:54,230
and standard deep learning gradient
based methods. Okay, because that's just

1778
00:38:54,230 --> 00:38:54,240
based methods. Okay, because that's just
 

1779
00:38:54,240 --> 00:38:56,470
based methods. Okay, because that's just
you can't have the number of parameters

1780
00:38:56,470 --> 00:38:56,480
you can't have the number of parameters
 

1781
00:38:56,480 --> 00:38:58,870
you can't have the number of parameters
be learnable using gradients because

1782
00:38:58,870 --> 00:38:58,880
be learnable using gradients because
 

1783
00:38:58,880 --> 00:39:00,790
be learnable using gradients because
it's integer.

1784
00:39:00,790 --> 00:39:00,800
it's integer.
 

1785
00:39:00,800 --> 00:39:02,710
it's integer.
Okay, so R is definitely not a learnable

1786
00:39:02,710 --> 00:39:02,720
Okay, so R is definitely not a learnable
 

1787
00:39:02,720 --> 00:39:06,630
Okay, so R is definitely not a learnable
parameter. Um let's go back. So, we're

1788
00:39:06,630 --> 00:39:06,640
parameter. Um let's go back. So, we're
 

1789
00:39:06,640 --> 00:39:08,710
parameter. Um let's go back. So, we're
back to this question like why is this

1790
00:39:08,710 --> 00:39:08,720
back to this question like why is this
 

1791
00:39:08,720 --> 00:39:12,870
back to this question like why is this
not a good idea?

1792
00:39:12,870 --> 00:39:12,880

 

1793
00:39:12,880 --> 00:39:14,950

Okay, so I see most only a few people

1794
00:39:14,950 --> 00:39:14,960
Okay, so I see most only a few people
 

1795
00:39:14,960 --> 00:39:17,670
Okay, so I see most only a few people
are answering. So, I want to like stop

1796
00:39:17,670 --> 00:39:17,680
are answering. So, I want to like stop
 

1797
00:39:17,680 --> 00:39:19,750
are answering. So, I want to like stop
at this point. Pause, talk to your

1798
00:39:19,750 --> 00:39:19,760
at this point. Pause, talk to your
 

1799
00:39:19,760 --> 00:39:22,390
at this point. Pause, talk to your
neighbors and discuss

1800
00:39:22,390 --> 00:39:22,400
neighbors and discuss
 

1801
00:39:22,400 --> 00:39:25,109
neighbors and discuss
what else can go wrong

1802
00:39:25,109 --> 00:39:25,119
what else can go wrong
 

1803
00:39:25,119 --> 00:39:27,990
what else can go wrong
besides zero gradients

1804
00:39:27,990 --> 00:39:28,000
besides zero gradients
 

1805
00:39:28,000 --> 00:39:30,150
besides zero gradients
in the fine-tuning context. Okay,

1806
00:39:30,150 --> 00:39:30,160
in the fine-tuning context. Okay,
 

1807
00:39:30,160 --> 00:39:31,270
in the fine-tuning context. Okay,
remember that's what we're trying to do.

1808
00:39:31,270 --> 00:39:31,280
remember that's what we're trying to do.
 

1809
00:39:31,280 --> 00:39:32,950
remember that's what we're trying to do.
We're saying it's not the right answer,

1810
00:39:32,950 --> 00:39:32,960
We're saying it's not the right answer,
 

1811
00:39:32,960 --> 00:39:35,270
We're saying it's not the right answer,
which means something goes wrong. We're

1812
00:39:35,270 --> 00:39:35,280
which means something goes wrong. We're
 

1813
00:39:35,280 --> 00:39:37,430
which means something goes wrong. We're
doing fine-tuning and the problem isn't

1814
00:39:37,430 --> 00:39:37,440
doing fine-tuning and the problem isn't
 

1815
00:39:37,440 --> 00:39:39,589
doing fine-tuning and the problem isn't
zero gradients. So you have a gradient.

1816
00:39:39,589 --> 00:39:39,599
zero gradients. So you have a gradient.
 

1817
00:39:39,599 --> 00:39:41,829
zero gradients. So you have a gradient.
So something else has to go wrong. So

1818
00:39:41,829 --> 00:39:41,839
So something else has to go wrong. So
 

1819
00:39:41,839 --> 00:39:44,150
So something else has to go wrong. So
talk to your neighbor for a minute and

1820
00:39:44,150 --> 00:39:44,160
talk to your neighbor for a minute and
 

1821
00:39:44,160 --> 00:40:14,230
talk to your neighbor for a minute and
ask what else could possibly go wrong.

1822
00:40:14,230 --> 00:40:14,240

 

1823
00:40:14,240 --> 00:40:27,430

Yeah,

1824
00:40:27,430 --> 00:40:27,440

 

1825
00:40:27,440 --> 00:41:01,750

I was thinking

1826
00:41:01,750 --> 00:41:01,760

 

1827
00:41:01,760 --> 00:41:03,750

Okay,

1828
00:41:03,750 --> 00:41:03,760
Okay,
 

1829
00:41:03,760 --> 00:41:05,670
Okay,
>> so I wrote a hint. We ran out of ran out

1830
00:41:05,670 --> 00:41:05,680
>> so I wrote a hint. We ran out of ran out
 

1831
00:41:05,680 --> 00:41:09,910
>> so I wrote a hint. We ran out of ran out
of our minute fast. So the hint I was uh

1832
00:41:09,910 --> 00:41:09,920
of our minute fast. So the hint I was uh
 

1833
00:41:09,920 --> 00:41:13,910
of our minute fast. So the hint I was uh
going to say was remember how we think

1834
00:41:13,910 --> 00:41:13,920
going to say was remember how we think
 

1835
00:41:13,920 --> 00:41:16,390
going to say was remember how we think
about learning is the reason we're using

1836
00:41:16,390 --> 00:41:16,400
about learning is the reason we're using
 

1837
00:41:16,400 --> 00:41:17,990
about learning is the reason we're using
gradients at all is we're taking a local

1838
00:41:17,990 --> 00:41:18,000
gradients at all is we're taking a local
 

1839
00:41:18,000 --> 00:41:20,470
gradients at all is we're taking a local
perspective, right? We want to think of

1840
00:41:20,470 --> 00:41:20,480
perspective, right? We want to think of
 

1841
00:41:20,480 --> 00:41:21,750
perspective, right? We want to think of
our model as having some local

1842
00:41:21,750 --> 00:41:21,760
our model as having some local
 

1843
00:41:21,760 --> 00:41:23,829
our model as having some local
linearity. We then you know take steps

1844
00:41:23,829 --> 00:41:23,839
linearity. We then you know take steps
 

1845
00:41:23,839 --> 00:41:25,030
linearity. We then you know take steps
such that things are good. Remember

1846
00:41:25,030 --> 00:41:25,040
such that things are good. Remember
 

1847
00:41:25,040 --> 00:41:26,630
such that things are good. Remember
that's how we're thinking about learning

1848
00:41:26,630 --> 00:41:26,640
that's how we're thinking about learning
 

1849
00:41:26,640 --> 00:41:29,510
that's how we're thinking about learning
at a big level. So now after you guys

1850
00:41:29,510 --> 00:41:29,520
at a big level. So now after you guys
 

1851
00:41:29,520 --> 00:41:32,150
at a big level. So now after you guys
have t thought about it what is wrong

1852
00:41:32,150 --> 00:41:32,160
have t thought about it what is wrong
 

1853
00:41:32,160 --> 00:41:36,630
have t thought about it what is wrong
with initializing A and B to not zero

1854
00:41:36,630 --> 00:41:36,640
with initializing A and B to not zero
 

1855
00:41:36,640 --> 00:41:39,109
with initializing A and B to not zero
meaning just both Xavier

1856
00:41:39,109 --> 00:41:39,119
meaning just both Xavier
 

1857
00:41:39,119 --> 00:41:40,710
meaning just both Xavier
from the perspective of doing

1858
00:41:40,710 --> 00:41:40,720
from the perspective of doing
 

1859
00:41:40,720 --> 00:41:43,349
from the perspective of doing
fine-tuning. Is anyone able to talk it

1860
00:41:43,349 --> 00:41:43,359
fine-tuning. Is anyone able to talk it
 

1861
00:41:43,359 --> 00:41:46,390
fine-tuning. Is anyone able to talk it
through and get an idea of what might be

1862
00:41:46,390 --> 00:41:46,400
through and get an idea of what might be
 

1863
00:41:46,400 --> 00:41:48,630
through and get an idea of what might be
going wrong?

1864
00:41:48,630 --> 00:41:48,640
going wrong?
 

1865
00:41:48,640 --> 00:41:54,630
going wrong?
So gradients aren't zero. Um yeah

1866
00:41:54,630 --> 00:41:54,640
So gradients aren't zero. Um yeah
 

1867
00:41:54,640 --> 00:42:01,349
So gradients aren't zero. Um yeah
>> I think at the beginning if not

1868
00:42:01,349 --> 00:42:01,359

 

1869
00:42:01,359 --> 00:42:12,790

something

1870
00:42:12,790 --> 00:42:12,800

 

1871
00:42:12,800 --> 00:42:16,150

>> delta w

1872
00:42:16,150 --> 00:42:16,160
>> delta w
 

1873
00:42:16,160 --> 00:42:23,109
>> delta w
would be initialized

1874
00:42:23,109 --> 00:42:23,119

 

1875
00:42:23,119 --> 00:42:29,829

to something big.

1876
00:42:29,829 --> 00:42:29,839

 

1877
00:42:29,839 --> 00:42:31,990

Okay. And because the delta w is

1878
00:42:31,990 --> 00:42:32,000
Okay. And because the delta w is
 

1879
00:42:32,000 --> 00:42:34,069
Okay. And because the delta w is
initialized something big, you start off

1880
00:42:34,069 --> 00:42:34,079
initialized something big, you start off
 

1881
00:42:34,079 --> 00:42:36,390
initialized something big, you start off
not where you wanted to start. So you

1882
00:42:36,390 --> 00:42:36,400
not where you wanted to start. So you
 

1883
00:42:36,400 --> 00:42:37,990
not where you wanted to start. So you
want to fine-tune a model, which means

1884
00:42:37,990 --> 00:42:38,000
want to fine-tune a model, which means
 

1885
00:42:38,000 --> 00:42:40,870
want to fine-tune a model, which means
you want to start at the model as it is,

1886
00:42:40,870 --> 00:42:40,880
you want to start at the model as it is,
 

1887
00:42:40,880 --> 00:42:42,390
you want to start at the model as it is,
but you're going to initialize it to

1888
00:42:42,390 --> 00:42:42,400
but you're going to initialize it to
 

1889
00:42:42,400 --> 00:42:44,230
but you're going to initialize it to
something that's not there, but

1890
00:42:44,230 --> 00:42:44,240
something that's not there, but
 

1891
00:42:44,240 --> 00:42:46,470
something that's not there, but
somewhere else.

1892
00:42:46,470 --> 00:42:46,480
somewhere else.
 

1893
00:42:46,480 --> 00:42:48,069
somewhere else.
and that somewhere else you don't know

1894
00:42:48,069 --> 00:42:48,079
and that somewhere else you don't know
 

1895
00:42:48,079 --> 00:42:50,309
and that somewhere else you don't know
is good at anything.

1896
00:42:50,309 --> 00:42:50,319
is good at anything.
 

1897
00:42:50,319 --> 00:42:52,550
is good at anything.
So reviewing a fine-tune or on a

1898
00:42:52,550 --> 00:42:52,560
So reviewing a fine-tune or on a
 

1899
00:42:52,560 --> 00:42:55,670
So reviewing a fine-tune or on a
foundation model the implicit idea is

1900
00:42:55,670 --> 00:42:55,680
foundation model the implicit idea is
 

1901
00:42:55,680 --> 00:42:58,150
foundation model the implicit idea is
that this model has good embeddings. It

1902
00:42:58,150 --> 00:42:58,160
that this model has good embeddings. It
 

1903
00:42:58,160 --> 00:43:00,150
that this model has good embeddings. It
has learned how to do a lot of stuff. We

1904
00:43:00,150 --> 00:43:00,160
has learned how to do a lot of stuff. We
 

1905
00:43:00,160 --> 00:43:03,190
has learned how to do a lot of stuff. We
just don't know how to do exactly this.

1906
00:43:03,190 --> 00:43:03,200
just don't know how to do exactly this.
 

1907
00:43:03,200 --> 00:43:04,870
just don't know how to do exactly this.
I want to keep all the stuff that it

1908
00:43:04,870 --> 00:43:04,880
I want to keep all the stuff that it
 

1909
00:43:04,880 --> 00:43:08,230
I want to keep all the stuff that it
knows and I want to adapt it. But by

1910
00:43:08,230 --> 00:43:08,240
knows and I want to adapt it. But by
 

1911
00:43:08,240 --> 00:43:10,150
knows and I want to adapt it. But by
starting off with delta W is initialized

1912
00:43:10,150 --> 00:43:10,160
starting off with delta W is initialized
 

1913
00:43:10,160 --> 00:43:11,430
starting off with delta W is initialized
to something big, you're taking the

1914
00:43:11,430 --> 00:43:11,440
to something big, you're taking the
 

1915
00:43:11,440 --> 00:43:13,990
to something big, you're taking the
entire model and shaking it far away.

1916
00:43:13,990 --> 00:43:14,000
entire model and shaking it far away.
 

1917
00:43:14,000 --> 00:43:16,230
entire model and shaking it far away.
And now it's lost potentially all the

1918
00:43:16,230 --> 00:43:16,240
And now it's lost potentially all the
 

1919
00:43:16,240 --> 00:43:18,230
And now it's lost potentially all the
stuff it knew how to do. And so you

1920
00:43:18,230 --> 00:43:18,240
stuff it knew how to do. And so you
 

1921
00:43:18,240 --> 00:43:19,190
stuff it knew how to do. And so you
might as well have started from

1922
00:43:19,190 --> 00:43:19,200
might as well have started from
 

1923
00:43:19,200 --> 00:43:20,630
might as well have started from
initializing the entire model to

1924
00:43:20,630 --> 00:43:20,640
initializing the entire model to
 

1925
00:43:20,640 --> 00:43:22,870
initializing the entire model to
someplace randomish.

1926
00:43:22,870 --> 00:43:22,880
someplace randomish.
 

1927
00:43:22,880 --> 00:43:24,069
someplace randomish.
And so you're not going to get this

1928
00:43:24,069 --> 00:43:24,079
And so you're not going to get this
 

1929
00:43:24,079 --> 00:43:25,990
And so you're not going to get this
benefit.

1930
00:43:25,990 --> 00:43:26,000
benefit.
 

1931
00:43:26,000 --> 00:43:31,589
benefit.
Okay. So this doesn't work.

1932
00:43:31,589 --> 00:43:31,599

 

1933
00:43:31,599 --> 00:43:35,910

So that leads to

1934
00:43:35,910 --> 00:43:35,920

 

1935
00:43:35,920 --> 00:43:37,829

you know two

1936
00:43:37,829 --> 00:43:37,839
you know two
 

1937
00:43:37,839 --> 00:43:48,550
you know two
One should be zero

1938
00:43:48,550 --> 00:43:48,560

 

1939
00:43:48,560 --> 00:43:51,190

because if one of them is zero they're

1940
00:43:51,190 --> 00:43:51,200
because if one of them is zero they're
 

1941
00:43:51,200 --> 00:43:53,510
because if one of them is zero they're
both uh the product is going to be zero

1942
00:43:53,510 --> 00:43:53,520
both uh the product is going to be zero
 

1943
00:43:53,520 --> 00:43:56,630
both uh the product is going to be zero
and you'll start at zero.

1944
00:43:56,630 --> 00:43:56,640
and you'll start at zero.
 

1945
00:43:56,640 --> 00:44:00,870
and you'll start at zero.
Okay. So and for a long time people

1946
00:44:00,870 --> 00:44:00,880
Okay. So and for a long time people
 

1947
00:44:00,880 --> 00:44:03,910
Okay. So and for a long time people
thought yeah this is good. Okay one of

1948
00:44:03,910 --> 00:44:03,920
thought yeah this is good. Okay one of
 

1949
00:44:03,920 --> 00:44:07,670
thought yeah this is good. Okay one of
them is zero. uh this is fine. So a

1950
00:44:07,670 --> 00:44:07,680
them is zero. uh this is fine. So a
 

1951
00:44:07,680 --> 00:44:12,150
them is zero. uh this is fine. So a
comment here people were doing

1952
00:44:12,150 --> 00:44:12,160
comment here people were doing
 

1953
00:44:12,160 --> 00:44:13,589
comment here people were doing
this particular approach that's

1954
00:44:13,589 --> 00:44:13,599
this particular approach that's
 

1955
00:44:13,599 --> 00:44:15,910
this particular approach that's
described initializing B to zero and A

1956
00:44:15,910 --> 00:44:15,920
described initializing B to zero and A
 

1957
00:44:15,920 --> 00:44:20,790
described initializing B to zero and A
to non zero from the start

1958
00:44:20,790 --> 00:44:20,800
to non zero from the start
 

1959
00:44:20,800 --> 00:44:23,990
to non zero from the start
mostly as breaking the symmetry and the

1960
00:44:23,990 --> 00:44:24,000
mostly as breaking the symmetry and the
 

1961
00:44:24,000 --> 00:44:25,109
mostly as breaking the symmetry and the
reason for that you might think why

1962
00:44:25,109 --> 00:44:25,119
reason for that you might think why
 

1963
00:44:25,119 --> 00:44:27,589
reason for that you might think why
would you make B zero and that's just

1964
00:44:27,589 --> 00:44:27,599
would you make B zero and that's just
 

1965
00:44:27,599 --> 00:44:29,670
would you make B zero and that's just
because I'm writing everything with this

1966
00:44:29,670 --> 00:44:29,680
because I'm writing everything with this
 

1967
00:44:29,680 --> 00:44:32,630
because I'm writing everything with this
kind of column perspective on vectors

1968
00:44:32,630 --> 00:44:32,640
kind of column perspective on vectors
 

1969
00:44:32,640 --> 00:44:34,309
kind of column perspective on vectors
but a lot of the implementations are

1970
00:44:34,309 --> 00:44:34,319
but a lot of the implementations are
 

1971
00:44:34,319 --> 00:44:36,470
but a lot of the implementations are
defaulting kind of to a row perspective

1972
00:44:36,470 --> 00:44:36,480
defaulting kind of to a row perspective
 

1973
00:44:36,480 --> 00:44:38,950
defaulting kind of to a row perspective
on vectors just because they're shapes

1974
00:44:38,950 --> 00:44:38,960
on vectors just because they're shapes
 

1975
00:44:38,960 --> 00:44:41,510
on vectors just because they're shapes
and so writing and I write it as B times

1976
00:44:41,510 --> 00:44:41,520
and so writing and I write it as B times
 

1977
00:44:41,520 --> 00:44:45,030
and so writing and I write it as B times
A, right? But a lot of people write A

1978
00:44:45,030 --> 00:44:45,040
A, right? But a lot of people write A
 

1979
00:44:45,040 --> 00:44:46,790
A, right? But a lot of people write A
time B.

1980
00:44:46,790 --> 00:44:46,800
time B.
 

1981
00:44:46,800 --> 00:44:50,069
time B.
And so from that point of view, you

1982
00:44:50,069 --> 00:44:50,079
And so from that point of view, you
 

1983
00:44:50,079 --> 00:44:52,550
And so from that point of view, you
know, they were they were often

1984
00:44:52,550 --> 00:44:52,560
know, they were they were often
 

1985
00:44:52,560 --> 00:44:56,470
know, they were they were often
initializing the so a yeah they were

1986
00:44:56,470 --> 00:44:56,480
initializing the so a yeah they were
 

1987
00:44:56,480 --> 00:45:00,309
initializing the so a yeah they were
initializing it so that uh the

1988
00:45:00,309 --> 00:45:00,319
initializing it so that uh the
 

1989
00:45:00,319 --> 00:45:02,630
initializing it so that uh the
the first thing was zero from their

1990
00:45:02,630 --> 00:45:02,640
the first thing was zero from their
 

1991
00:45:02,640 --> 00:45:05,190
the first thing was zero from their
point of view. Okay. in the in that way

1992
00:45:05,190 --> 00:45:05,200
point of view. Okay. in the in that way
 

1993
00:45:05,200 --> 00:45:07,750
point of view. Okay. in the in that way
of doing it. But so what people did, you

1994
00:45:07,750 --> 00:45:07,760
of doing it. But so what people did, you
 

1995
00:45:07,760 --> 00:45:08,950
of doing it. But so what people did, you
could either do it empirically. They

1996
00:45:08,950 --> 00:45:08,960
could either do it empirically. They
 

1997
00:45:08,960 --> 00:45:11,510
could either do it empirically. They
said I can try both

1998
00:45:11,510 --> 00:45:11,520
said I can try both
 

1999
00:45:11,520 --> 00:45:13,750
said I can try both
and see which one works better. People

2000
00:45:13,750 --> 00:45:13,760
and see which one works better. People
 

2001
00:45:13,760 --> 00:45:15,990
and see which one works better. People
expected it not to matter. But it turned

2002
00:45:15,990 --> 00:45:16,000
expected it not to matter. But it turned
 

2003
00:45:16,000 --> 00:45:18,069
expected it not to matter. But it turned
out actually this one works a little bit

2004
00:45:18,069 --> 00:45:18,079
out actually this one works a little bit
 

2005
00:45:18,079 --> 00:45:21,349
out actually this one works a little bit
better. So you can just take it at that

2006
00:45:21,349 --> 00:45:21,359
better. So you can just take it at that
 

2007
00:45:21,359 --> 00:45:26,309
better. So you can just take it at that
uh as one approach. Now the

2008
00:45:26,309 --> 00:45:26,319
uh as one approach. Now the
 

2009
00:45:26,319 --> 00:45:29,910
uh as one approach. Now the
the idea that these things

2010
00:45:29,910 --> 00:45:29,920
the idea that these things
 

2011
00:45:29,920 --> 00:45:32,470
the idea that these things
using being trained with atom W might

2012
00:45:32,470 --> 00:45:32,480
using being trained with atom W might
 

2013
00:45:32,480 --> 00:45:34,390
using being trained with atom W might
need a different learning rate than you

2014
00:45:34,390 --> 00:45:34,400
need a different learning rate than you
 

2015
00:45:34,400 --> 00:45:36,150
need a different learning rate than you
would use that you than you used that

2016
00:45:36,150 --> 00:45:36,160
would use that you than you used that
 

2017
00:45:36,160 --> 00:45:38,069
would use that you than you used that
you found during pre-training. That

2018
00:45:38,069 --> 00:45:38,079
you found during pre-training. That
 

2019
00:45:38,079 --> 00:45:40,870
you found during pre-training. That
should also be potentially clear to you

2020
00:45:40,870 --> 00:45:40,880
should also be potentially clear to you
 

2021
00:45:40,880 --> 00:45:42,230
should also be potentially clear to you
because you're treat you're training a

2022
00:45:42,230 --> 00:45:42,240
because you're treat you're training a
 

2023
00:45:42,240 --> 00:45:44,069
because you're treat you're training a
different model,

2024
00:45:44,069 --> 00:45:44,079
different model,
 

2025
00:45:44,079 --> 00:45:45,670
different model,
right? It has it's very different than

2026
00:45:45,670 --> 00:45:45,680
right? It has it's very different than
 

2027
00:45:45,680 --> 00:45:47,030
right? It has it's very different than
what you were doing during pre-training.

2028
00:45:47,030 --> 00:45:47,040
what you were doing during pre-training.
 

2029
00:45:47,040 --> 00:45:48,550
what you were doing during pre-training.
So using this pre-training learning

2030
00:45:48,550 --> 00:45:48,560
So using this pre-training learning
 

2031
00:45:48,560 --> 00:45:50,230
So using this pre-training learning
right here might not be the right

2032
00:45:50,230 --> 00:45:50,240
right here might not be the right
 

2033
00:45:50,240 --> 00:45:52,309
right here might not be the right
answer. Remember if you have to sweep

2034
00:45:52,309 --> 00:45:52,319
answer. Remember if you have to sweep
 

2035
00:45:52,319 --> 00:45:53,910
answer. Remember if you have to sweep
one hyperparameter the hyperparameter

2036
00:45:53,910 --> 00:45:53,920
one hyperparameter the hyperparameter
 

2037
00:45:53,920 --> 00:45:56,230
one hyperparameter the hyperparameter
you sweep is learning rate. So the idea

2038
00:45:56,230 --> 00:45:56,240
you sweep is learning rate. So the idea
 

2039
00:45:56,240 --> 00:45:57,510
you sweep is learning rate. So the idea
you'd have to sweep the hyperparameter

2040
00:45:57,510 --> 00:45:57,520
you'd have to sweep the hyperparameter
 

2041
00:45:57,520 --> 00:46:00,470
you'd have to sweep the hyperparameter
here is kind of natural. Okay. So using

2042
00:46:00,470 --> 00:46:00,480
here is kind of natural. Okay. So using
 

2043
00:46:00,480 --> 00:46:01,910
here is kind of natural. Okay. So using
a different hyperparameter people knew

2044
00:46:01,910 --> 00:46:01,920
a different hyperparameter people knew
 

2045
00:46:01,920 --> 00:46:03,589
a different hyperparameter people knew
from the beginning of Lauras that use a

2046
00:46:03,589 --> 00:46:03,599
from the beginning of Lauras that use a
 

2047
00:46:03,599 --> 00:46:05,589
from the beginning of Lauras that use a
different one.

2048
00:46:05,589 --> 00:46:05,599
different one.
 

2049
00:46:05,599 --> 00:46:08,390
different one.
What emerged more recently is this idea

2050
00:46:08,390 --> 00:46:08,400
What emerged more recently is this idea
 

2051
00:46:08,400 --> 00:46:11,190
What emerged more recently is this idea
that often you you do better by using a

2052
00:46:11,190 --> 00:46:11,200
that often you you do better by using a
 

2053
00:46:11,200 --> 00:46:13,510
that often you you do better by using a
different learning rate for A and B

2054
00:46:13,510 --> 00:46:13,520
different learning rate for A and B
 

2055
00:46:13,520 --> 00:46:17,910
different learning rate for A and B
using atom w. Now

2056
00:46:17,910 --> 00:46:17,920
using atom w. Now
 

2057
00:46:17,920 --> 00:46:21,430
using atom w. Now
from your point of view, how you've been

2058
00:46:21,430 --> 00:46:21,440
from your point of view, how you've been
 

2059
00:46:21,440 --> 00:46:23,750
from your point of view, how you've been
taught, this should not be surprising to

2060
00:46:23,750 --> 00:46:23,760
taught, this should not be surprising to
 

2061
00:46:23,760 --> 00:46:25,430
taught, this should not be surprising to
you.

2062
00:46:25,430 --> 00:46:25,440
you.
 

2063
00:46:25,440 --> 00:46:28,069
you.
Okay? Because when you were taught uh

2064
00:46:28,069 --> 00:46:28,079
Okay? Because when you were taught uh
 

2065
00:46:28,079 --> 00:46:31,670
Okay? Because when you were taught uh
thinking about MUP, we said, "Hey, for

2066
00:46:31,670 --> 00:46:31,680
thinking about MUP, we said, "Hey, for
 

2067
00:46:31,680 --> 00:46:34,150
thinking about MUP, we said, "Hey, for
doing atom, the learning rate you want

2068
00:46:34,150 --> 00:46:34,160
doing atom, the learning rate you want
 

2069
00:46:34,160 --> 00:46:36,230
doing atom, the learning rate you want
to use should depend on the fan in of

2070
00:46:36,230 --> 00:46:36,240
to use should depend on the fan in of
 

2071
00:46:36,240 --> 00:46:38,950
to use should depend on the fan in of
the matrix,

2072
00:46:38,950 --> 00:46:38,960
the matrix,
 

2073
00:46:38,960 --> 00:46:42,230
the matrix,
right? And B and A have very, very

2074
00:46:42,230 --> 00:46:42,240
right? And B and A have very, very
 

2075
00:46:42,240 --> 00:46:44,390
right? And B and A have very, very
different fanins.

2076
00:46:44,390 --> 00:46:44,400
different fanins.
 

2077
00:46:44,400 --> 00:46:47,430
different fanins.
Do you guys see that

2078
00:46:47,430 --> 00:46:47,440
Do you guys see that
 

2079
00:46:47,440 --> 00:46:50,150
Do you guys see that
A's fan in is for in this example is

2080
00:46:50,150 --> 00:46:50,160
A's fan in is for in this example is
 

2081
00:46:50,160 --> 00:46:54,069
A's fan in is for in this example is
4,96

2082
00:46:54,069 --> 00:46:54,079

 

2083
00:46:54,079 --> 00:46:58,550

right and B's fan in is R

2084
00:46:58,550 --> 00:46:58,560
right and B's fan in is R
 

2085
00:46:58,560 --> 00:47:02,390
right and B's fan in is R
which is like five or 10 they're very

2086
00:47:02,390 --> 00:47:02,400
which is like five or 10 they're very
 

2087
00:47:02,400 --> 00:47:05,910
which is like five or 10 they're very
different so from a MUP perspective it's

2088
00:47:05,910 --> 00:47:05,920
different so from a MUP perspective it's
 

2089
00:47:05,920 --> 00:47:07,670
different so from a MUP perspective it's
not surprising to expect a different

2090
00:47:07,670 --> 00:47:07,680
not surprising to expect a different
 

2091
00:47:07,680 --> 00:47:09,750
not surprising to expect a different
learning rate

2092
00:47:09,750 --> 00:47:09,760
learning rate
 

2093
00:47:09,760 --> 00:47:11,910
learning rate
does everyone see that so originally

2094
00:47:11,910 --> 00:47:11,920
does everyone see that so originally
 

2095
00:47:11,920 --> 00:47:13,510
does everyone see that so originally
people were using one learning rate but

2096
00:47:13,510 --> 00:47:13,520
people were using one learning rate but
 

2097
00:47:13,520 --> 00:47:17,910
people were using one learning rate but
Now it's a a different learning rate is

2098
00:47:17,910 --> 00:47:17,920
Now it's a a different learning rate is
 

2099
00:47:17,920 --> 00:47:20,790
Now it's a a different learning rate is
reasonable. Now

2100
00:47:20,790 --> 00:47:20,800
reasonable. Now
 

2101
00:47:20,800 --> 00:47:24,069
reasonable. Now
the surprising thing is that you can't

2102
00:47:24,069 --> 00:47:24,079
the surprising thing is that you can't
 

2103
00:47:24,079 --> 00:47:27,750
the surprising thing is that you can't
just use the MUP scaling to get the

2104
00:47:27,750 --> 00:47:27,760
just use the MUP scaling to get the
 

2105
00:47:27,760 --> 00:47:30,630
just use the MUP scaling to get the
right learning rates here. Part of this

2106
00:47:30,630 --> 00:47:30,640
right learning rates here. Part of this
 

2107
00:47:30,640 --> 00:47:32,069
right learning rates here. Part of this
is what I was describing before. Induced

2108
00:47:32,069 --> 00:47:32,079
is what I was describing before. Induced
 

2109
00:47:32,079 --> 00:47:33,910
is what I was describing before. Induced
norms are not quite exactly the right

2110
00:47:33,910 --> 00:47:33,920
norms are not quite exactly the right
 

2111
00:47:33,920 --> 00:47:35,829
norms are not quite exactly the right
thing of what's going on here. But the

2112
00:47:35,829 --> 00:47:35,839
thing of what's going on here. But the
 

2113
00:47:35,839 --> 00:47:40,069
thing of what's going on here. But the
easier way of understanding it is that

2114
00:47:40,069 --> 00:47:40,079
easier way of understanding it is that
 

2115
00:47:40,079 --> 00:47:42,230
easier way of understanding it is that
you know all of the arguments we make

2116
00:47:42,230 --> 00:47:42,240
you know all of the arguments we make
 

2117
00:47:42,240 --> 00:47:43,910
you know all of the arguments we make
about what happening with an induced

2118
00:47:43,910 --> 00:47:43,920
about what happening with an induced
 

2119
00:47:43,920 --> 00:47:46,150
about what happening with an induced
norm and the size we're implicitly doing

2120
00:47:46,150 --> 00:47:46,160
norm and the size we're implicitly doing
 

2121
00:47:46,160 --> 00:47:48,230
norm and the size we're implicitly doing
all these expectations.

2122
00:47:48,230 --> 00:47:48,240
all these expectations.
 

2123
00:47:48,240 --> 00:47:49,910
all these expectations.
Remember like we're doing this like

2124
00:47:49,910 --> 00:47:49,920
Remember like we're doing this like
 

2125
00:47:49,920 --> 00:47:52,230
Remember like we're doing this like
assume that everything is Gaussian how

2126
00:47:52,230 --> 00:47:52,240
assume that everything is Gaussian how
 

2127
00:47:52,240 --> 00:47:55,109
assume that everything is Gaussian how
do variances add that kind of thing.

2128
00:47:55,109 --> 00:47:55,119
do variances add that kind of thing.
 

2129
00:47:55,119 --> 00:47:58,390
do variances add that kind of thing.
Okay. And you know expectation type

2130
00:47:58,390 --> 00:47:58,400
Okay. And you know expectation type
 

2131
00:47:58,400 --> 00:48:01,589
Okay. And you know expectation type
calculations work when you get

2132
00:48:01,589 --> 00:48:01,599
calculations work when you get
 

2133
00:48:01,599 --> 00:48:03,750
calculations work when you get
concentration when you're adding enough

2134
00:48:03,750 --> 00:48:03,760
concentration when you're adding enough
 

2135
00:48:03,760 --> 00:48:07,190
concentration when you're adding enough
stuff together. So in the MUP paper uh

2136
00:48:07,190 --> 00:48:07,200
stuff together. So in the MUP paper uh
 

2137
00:48:07,200 --> 00:48:09,270
stuff together. So in the MUP paper uh
you saw when we asked you to look at it

2138
00:48:09,270 --> 00:48:09,280
you saw when we asked you to look at it
 

2139
00:48:09,280 --> 00:48:12,230
you saw when we asked you to look at it
that you should use a particular size

2140
00:48:12,230 --> 00:48:12,240
that you should use a particular size
 

2141
00:48:12,240 --> 00:48:14,470
that you should use a particular size
but it can't be too small to get the

2142
00:48:14,470 --> 00:48:14,480
but it can't be too small to get the
 

2143
00:48:14,480 --> 00:48:16,230
but it can't be too small to get the
hyperparameter transfer. You have to

2144
00:48:16,230 --> 00:48:16,240
hyperparameter transfer. You have to
 

2145
00:48:16,240 --> 00:48:19,030
hyperparameter transfer. You have to
make things small but big enough. And

2146
00:48:19,030 --> 00:48:19,040
make things small but big enough. And
 

2147
00:48:19,040 --> 00:48:20,550
make things small but big enough. And
the big enough rule of thumb I think in

2148
00:48:20,550 --> 00:48:20,560
the big enough rule of thumb I think in
 

2149
00:48:20,560 --> 00:48:24,069
the big enough rule of thumb I think in
the MP paper was like a 100ish.

2150
00:48:24,069 --> 00:48:24,079
the MP paper was like a 100ish.
 

2151
00:48:24,079 --> 00:48:27,270
the MP paper was like a 100ish.
But the Laura's the size we use is much

2152
00:48:27,270 --> 00:48:27,280
But the Laura's the size we use is much
 

2153
00:48:27,280 --> 00:48:32,390
But the Laura's the size we use is much
smaller often, right? It's like 10 one

2154
00:48:32,390 --> 00:48:32,400
smaller often, right? It's like 10 one
 

2155
00:48:32,400 --> 00:48:35,589
smaller often, right? It's like 10 one
four numbers like this are often used

2156
00:48:35,589 --> 00:48:35,599
four numbers like this are often used
 

2157
00:48:35,599 --> 00:48:39,109
four numbers like this are often used
and that's too small.

2158
00:48:39,109 --> 00:48:39,119
and that's too small.
 

2159
00:48:39,119 --> 00:48:40,950
and that's too small.
And so from that point of view you can

2160
00:48:40,950 --> 00:48:40,960
And so from that point of view you can
 

2161
00:48:40,960 --> 00:48:43,190
And so from that point of view you can
think of okay wait

2162
00:48:43,190 --> 00:48:43,200
think of okay wait
 

2163
00:48:43,200 --> 00:48:45,829
think of okay wait
the just using the mup scaling might be

2164
00:48:45,829 --> 00:48:45,839
the just using the mup scaling might be
 

2165
00:48:45,839 --> 00:48:47,190
the just using the mup scaling might be
too aggressive for the difference in

2166
00:48:47,190 --> 00:48:47,200
too aggressive for the difference in
 

2167
00:48:47,200 --> 00:48:48,710
too aggressive for the difference in
learning rate of these two things and

2168
00:48:48,710 --> 00:48:48,720
learning rate of these two things and
 

2169
00:48:48,720 --> 00:48:50,870
learning rate of these two things and
one has to think about it. So the

2170
00:48:50,870 --> 00:48:50,880
one has to think about it. So the
 

2171
00:48:50,880 --> 00:48:53,910
one has to think about it. So the
practical answer is an empirical factor

2172
00:48:53,910 --> 00:48:53,920
practical answer is an empirical factor
 

2173
00:48:53,920 --> 00:48:56,309
practical answer is an empirical factor
that uh seems to vary and you have to

2174
00:48:56,309 --> 00:48:56,319
that uh seems to vary and you have to
 

2175
00:48:56,319 --> 00:48:57,829
that uh seems to vary and you have to
figure it out for that kind of problem

2176
00:48:57,829 --> 00:48:57,839
figure it out for that kind of problem
 

2177
00:48:57,839 --> 00:49:01,430
figure it out for that kind of problem
what it is.

2178
00:49:01,430 --> 00:49:01,440

 

2179
00:49:01,440 --> 00:49:04,710

Okay.

2180
00:49:04,710 --> 00:49:04,720

 

2181
00:49:04,720 --> 00:49:08,549

So now you understand um what's

2182
00:49:08,549 --> 00:49:08,559
So now you understand um what's
 

2183
00:49:08,559 --> 00:49:10,069
So now you understand um what's
happening here. There is an intuition

2184
00:49:10,069 --> 00:49:10,079
happening here. There is an intuition
 

2185
00:49:10,079 --> 00:49:12,309
happening here. There is an intuition
that people have which I'm not sure is

2186
00:49:12,309 --> 00:49:12,319
that people have which I'm not sure is
 

2187
00:49:12,319 --> 00:49:16,950
that people have which I'm not sure is
um actually correct but you'll hear it

2188
00:49:16,950 --> 00:49:16,960
um actually correct but you'll hear it
 

2189
00:49:16,960 --> 00:49:20,230
um actually correct but you'll hear it
that says well B was initialized to zero

2190
00:49:20,230 --> 00:49:20,240
that says well B was initialized to zero
 

2191
00:49:20,240 --> 00:49:21,829
that says well B was initialized to zero
so I need a larger learning rate to move

2192
00:49:21,829 --> 00:49:21,839
so I need a larger learning rate to move
 

2193
00:49:21,839 --> 00:49:24,150
so I need a larger learning rate to move
it somewhere else.

2194
00:49:24,150 --> 00:49:24,160
it somewhere else.
 

2195
00:49:24,160 --> 00:49:28,390
it somewhere else.
That's kind of fishy. Okay, it's not

2196
00:49:28,390 --> 00:49:28,400
That's kind of fishy. Okay, it's not
 

2197
00:49:28,400 --> 00:49:30,230
That's kind of fishy. Okay, it's not
because B is initialized to zero which

2198
00:49:30,230 --> 00:49:30,240
because B is initialized to zero which
 

2199
00:49:30,240 --> 00:49:31,750
because B is initialized to zero which
means what? A's gradients are going to

2200
00:49:31,750 --> 00:49:31,760
means what? A's gradients are going to
 

2201
00:49:31,760 --> 00:49:33,670
means what? A's gradients are going to
be very small when it starts off but

2202
00:49:33,670 --> 00:49:33,680
be very small when it starts off but
 

2203
00:49:33,680 --> 00:49:35,109
be very small when it starts off but
you're using atoms so that'll get scaled

2204
00:49:35,109 --> 00:49:35,119
you're using atoms so that'll get scaled
 

2205
00:49:35,119 --> 00:49:39,670
you're using atoms so that'll get scaled
out. So it's a little bit iffy. Okay,

2206
00:49:39,670 --> 00:49:39,680
out. So it's a little bit iffy. Okay,
 

2207
00:49:39,680 --> 00:49:44,470
out. So it's a little bit iffy. Okay,
but okay. So so far

2208
00:49:44,470 --> 00:49:44,480
but okay. So so far
 

2209
00:49:44,480 --> 00:49:47,270
but okay. So so far
all we have is that the initialization

2210
00:49:47,270 --> 00:49:47,280
all we have is that the initialization
 

2211
00:49:47,280 --> 00:49:51,030
all we have is that the initialization
of one of them namely uh a should be non

2212
00:49:51,030 --> 00:49:51,040
of one of them namely uh a should be non
 

2213
00:49:51,040 --> 00:49:53,990
of one of them namely uh a should be non
zero. Now the question is but

2214
00:49:53,990 --> 00:49:54,000
zero. Now the question is but
 

2215
00:49:54,000 --> 00:49:57,030
zero. Now the question is but
initialized to what?

2216
00:49:57,030 --> 00:49:57,040
initialized to what?
 

2217
00:49:57,040 --> 00:49:59,030
initialized to what?
Does everyone see that? It's a question.

2218
00:49:59,030 --> 00:49:59,040
Does everyone see that? It's a question.
 

2219
00:49:59,040 --> 00:50:16,309
Does everyone see that? It's a question.
How do we initialize it?

2220
00:50:16,309 --> 00:50:16,319

 

2221
00:50:16,319 --> 00:50:19,430

So there's a few choices.

2222
00:50:19,430 --> 00:50:19,440
So there's a few choices.
 

2223
00:50:19,440 --> 00:50:26,630
So there's a few choices.
The default one is just do Xavier.

2224
00:50:26,630 --> 00:50:26,640

 

2225
00:50:26,640 --> 00:50:28,630

This works and is the common one done in

2226
00:50:28,630 --> 00:50:28,640
This works and is the common one done in
 

2227
00:50:28,640 --> 00:50:30,230
This works and is the common one done in
practice.

2228
00:50:30,230 --> 00:50:30,240
practice.
 

2229
00:50:30,240 --> 00:50:33,829
practice.
Okay. However, there's papers that

2230
00:50:33,829 --> 00:50:33,839
Okay. However, there's papers that
 

2231
00:50:33,839 --> 00:50:35,430
Okay. However, there's papers that
suggest that there's other

2232
00:50:35,430 --> 00:50:35,440
suggest that there's other
 

2233
00:50:35,440 --> 00:50:39,190
suggest that there's other
initializations that can do better.

2234
00:50:39,190 --> 00:50:39,200
initializations that can do better.
 

2235
00:50:39,200 --> 00:50:41,270
initializations that can do better.
So

2236
00:50:41,270 --> 00:50:41,280
So
 

2237
00:50:41,280 --> 00:50:43,589
So
the next initialization

2238
00:50:43,589 --> 00:50:43,599
the next initialization
 

2239
00:50:43,599 --> 00:50:47,829
the next initialization
says that look all of this remember all

2240
00:50:47,829 --> 00:50:47,839
says that look all of this remember all
 

2241
00:50:47,839 --> 00:50:50,790
says that look all of this remember all
of this is a

2242
00:50:50,790 --> 00:50:50,800
of this is a
 

2243
00:50:50,800 --> 00:50:54,630
of this is a
war plus delta w that's what we're doing

2244
00:50:54,630 --> 00:50:54,640
war plus delta w that's what we're doing
 

2245
00:50:54,640 --> 00:51:04,230
war plus delta w that's what we're doing
here okay war star plus delta w

2246
00:51:04,230 --> 00:51:04,240

 

2247
00:51:04,240 --> 00:51:07,829

you saw the uh derivation of mu p and

2248
00:51:07,829 --> 00:51:07,839
you saw the uh derivation of mu p and
 

2249
00:51:07,839 --> 00:51:09,349
you saw the uh derivation of mu p and
thinking about what's happening even in

2250
00:51:09,349 --> 00:51:09,359
thinking about what's happening even in
 

2251
00:51:09,359 --> 00:51:12,950
thinking about what's happening even in
muon what you're seeing is that the

2252
00:51:12,950 --> 00:51:12,960
muon what you're seeing is that the
 

2253
00:51:12,960 --> 00:51:15,109
muon what you're seeing is that the
gradients that are coming are typically

2254
00:51:15,109 --> 00:51:15,119
gradients that are coming are typically
 

2255
00:51:15,119 --> 00:51:18,950
gradients that are coming are typically
not full rank matrices and even after

2256
00:51:18,950 --> 00:51:18,960
not full rank matrices and even after
 

2257
00:51:18,960 --> 00:51:20,470
not full rank matrices and even after
and this is a fine-tuning setting so

2258
00:51:20,470 --> 00:51:20,480
and this is a fine-tuning setting so
 

2259
00:51:20,480 --> 00:51:21,990
and this is a fine-tuning setting so
you're going to be adjusting somewhere

2260
00:51:21,990 --> 00:51:22,000
you're going to be adjusting somewhere
 

2261
00:51:22,000 --> 00:51:25,750
you're going to be adjusting somewhere
here so one perspective says that the

2262
00:51:25,750 --> 00:51:25,760
here so one perspective says that the
 

2263
00:51:25,760 --> 00:51:29,750
here so one perspective says that the
gradients that are coming are not

2264
00:51:29,750 --> 00:51:29,760
gradients that are coming are not
 

2265
00:51:29,760 --> 00:51:33,109
gradients that are coming are not
uh full rank uh matrices

2266
00:51:33,109 --> 00:51:33,119
uh full rank uh matrices
 

2267
00:51:33,119 --> 00:51:37,030
uh full rank uh matrices
so why am I setting initializing a to be

2268
00:51:37,030 --> 00:51:37,040
so why am I setting initializing a to be
 

2269
00:51:37,040 --> 00:51:40,470
so why am I setting initializing a to be
or I A to be random,

2270
00:51:40,470 --> 00:51:40,480
or I A to be random,
 

2271
00:51:40,480 --> 00:51:43,030
or I A to be random,
shouldn't I try to align A with the

2272
00:51:43,030 --> 00:51:43,040
shouldn't I try to align A with the
 

2273
00:51:43,040 --> 00:51:44,710
shouldn't I try to align A with the
directions that I'm actually expecting

2274
00:51:44,710 --> 00:51:44,720
directions that I'm actually expecting
 

2275
00:51:44,720 --> 00:51:46,790
directions that I'm actually expecting
to see gradients pushing me in? Does

2276
00:51:46,790 --> 00:51:46,800
to see gradients pushing me in? Does
 

2277
00:51:46,800 --> 00:51:49,349
to see gradients pushing me in? Does
everyone see this?

2278
00:51:49,349 --> 00:51:49,359
everyone see this?
 

2279
00:51:49,359 --> 00:51:51,829
everyone see this?
So

2280
00:51:51,829 --> 00:51:51,839
So
 

2281
00:51:51,839 --> 00:51:58,470
So
the second idea is the following.

2282
00:51:58,470 --> 00:51:58,480

 

2283
00:51:58,480 --> 00:52:01,829

compute a

2284
00:52:01,829 --> 00:52:01,839
compute a
 

2285
00:52:01,839 --> 00:52:06,390
compute a
batch of large

2286
00:52:06,390 --> 00:52:06,400
batch of large
 

2287
00:52:06,400 --> 00:52:11,430
batch of large
batch of gradients

2288
00:52:11,430 --> 00:52:11,440

 

2289
00:52:11,440 --> 00:52:14,150

for W. Just compute them for the entire

2290
00:52:14,150 --> 00:52:14,160
for W. Just compute them for the entire
 

2291
00:52:14,160 --> 00:52:16,309
for W. Just compute them for the entire
W. Remember back prop is computing them

2292
00:52:16,309 --> 00:52:16,319
W. Remember back prop is computing them
 

2293
00:52:16,319 --> 00:52:18,069
W. Remember back prop is computing them
anyway effectively. Just compute them

2294
00:52:18,069 --> 00:52:18,079
anyway effectively. Just compute them
 

2295
00:52:18,079 --> 00:52:24,069
anyway effectively. Just compute them
for W.

2296
00:52:24,069 --> 00:52:24,079

 

2297
00:52:24,079 --> 00:52:30,549

Take the SVD.

2298
00:52:30,549 --> 00:52:30,559

 

2299
00:52:30,559 --> 00:52:32,950

This is probably not going to have a

2300
00:52:32,950 --> 00:52:32,960
This is probably not going to have a
 

2301
00:52:32,960 --> 00:52:36,309
This is probably not going to have a
bunch of equal things.

2302
00:52:36,309 --> 00:52:36,319
bunch of equal things.
 

2303
00:52:36,319 --> 00:52:37,829
bunch of equal things.
It's going to have some singular values

2304
00:52:37,829 --> 00:52:37,839
It's going to have some singular values
 

2305
00:52:37,839 --> 00:52:39,510
It's going to have some singular values
that are big and some singular values

2306
00:52:39,510 --> 00:52:39,520
that are big and some singular values
 

2307
00:52:39,520 --> 00:52:42,230
that are big and some singular values
that are small. This is the direction

2308
00:52:42,230 --> 00:52:42,240
that are small. This is the direction
 

2309
00:52:42,240 --> 00:52:44,950
that are small. This is the direction
that for your fine-tuning data set that

2310
00:52:44,950 --> 00:52:44,960
that for your fine-tuning data set that
 

2311
00:52:44,960 --> 00:52:47,670
that for your fine-tuning data set that
you want to move in a full fine-tune

2312
00:52:47,670 --> 00:52:47,680
you want to move in a full fine-tune
 

2313
00:52:47,680 --> 00:52:50,870
you want to move in a full fine-tune
this W matrix.

2314
00:52:50,870 --> 00:52:50,880
this W matrix.
 

2315
00:52:50,880 --> 00:53:00,150
this W matrix.
What it says is just pick the top

2316
00:53:00,150 --> 00:53:00,160

 

2317
00:53:00,160 --> 00:53:03,430

Okay, initialize your matrix to pick the

2318
00:53:03,430 --> 00:53:03,440
Okay, initialize your matrix to pick the
 

2319
00:53:03,440 --> 00:53:05,349
Okay, initialize your matrix to pick the
top R

2320
00:53:05,349 --> 00:53:05,359
top R
 

2321
00:53:05,359 --> 00:53:07,910
top R
uh singular directions for a batch of

2322
00:53:07,910 --> 00:53:07,920
uh singular directions for a batch of
 

2323
00:53:07,920 --> 00:53:10,150
uh singular directions for a batch of
gradients on W for your matrix. Yeah,

2324
00:53:10,150 --> 00:53:10,160
gradients on W for your matrix. Yeah,
 

2325
00:53:10,160 --> 00:53:12,870
gradients on W for your matrix. Yeah,
>> it says on W star.

2326
00:53:12,870 --> 00:53:12,880
>> it says on W star.
 

2327
00:53:12,880 --> 00:53:15,990
>> it says on W star.
>> Yeah, for Yeah, good good point. For

2328
00:53:15,990 --> 00:53:16,000
>> Yeah, for Yeah, good good point. For
 

2329
00:53:16,000 --> 00:53:18,630
>> Yeah, for Yeah, good good point. For
war, meaning just hold yourself at war.

2330
00:53:18,630 --> 00:53:18,640
war, meaning just hold yourself at war.
 

2331
00:53:18,640 --> 00:53:20,150
war, meaning just hold yourself at war.
Don't take any steps. Just compute the

2332
00:53:20,150 --> 00:53:20,160
Don't take any steps. Just compute the
 

2333
00:53:20,160 --> 00:53:22,549
Don't take any steps. Just compute the
gradients on war. So you just take a

2334
00:53:22,549 --> 00:53:22,559
gradients on war. So you just take a
 

2335
00:53:22,559 --> 00:53:24,710
gradients on war. So you just take a
pass through your data, compute the

2336
00:53:24,710 --> 00:53:24,720
pass through your data, compute the
 

2337
00:53:24,720 --> 00:53:27,190
pass through your data, compute the
gradients on bar,

2338
00:53:27,190 --> 00:53:27,200
gradients on bar,
 

2339
00:53:27,200 --> 00:53:29,430
gradients on bar,
take a pass on for all the matrices that

2340
00:53:29,430 --> 00:53:29,440
take a pass on for all the matrices that
 

2341
00:53:29,440 --> 00:53:30,470
take a pass on for all the matrices that
you want to do. Just compute these

2342
00:53:30,470 --> 00:53:30,480
you want to do. Just compute these
 

2343
00:53:30,480 --> 00:53:32,309
you want to do. Just compute these
gradients. If you can't hold them all in

2344
00:53:32,309 --> 00:53:32,319
gradients. If you can't hold them all in
 

2345
00:53:32,319 --> 00:53:33,750
gradients. If you can't hold them all in
memory, then just do them a few at a

2346
00:53:33,750 --> 00:53:33,760
memory, then just do them a few at a
 

2347
00:53:33,760 --> 00:53:35,589
memory, then just do them a few at a
time. This is like tied to the number of

2348
00:53:35,589 --> 00:53:35,599
time. This is like tied to the number of
 

2349
00:53:35,599 --> 00:53:38,150
time. This is like tied to the number of
matrices you're doing. So you can afford

2350
00:53:38,150 --> 00:53:38,160
matrices you're doing. So you can afford
 

2351
00:53:38,160 --> 00:53:39,670
matrices you're doing. So you can afford
if you can afford one pass through your

2352
00:53:39,670 --> 00:53:39,680
if you can afford one pass through your
 

2353
00:53:39,680 --> 00:53:41,109
if you can afford one pass through your
data, you can do this or one pass

2354
00:53:41,109 --> 00:53:41,119
data, you can do this or one pass
 

2355
00:53:41,119 --> 00:53:43,349
data, you can do this or one pass
through a sub subset of your data.

2356
00:53:43,349 --> 00:53:43,359
through a sub subset of your data.
 

2357
00:53:43,359 --> 00:53:45,349
through a sub subset of your data.
Compute these gradients, compute the

2358
00:53:45,349 --> 00:53:45,359
Compute these gradients, compute the
 

2359
00:53:45,359 --> 00:53:48,549
Compute these gradients, compute the
SVD, pick the top R, and use that for

2360
00:53:48,549 --> 00:53:48,559
SVD, pick the top R, and use that for
 

2361
00:53:48,559 --> 00:53:53,990
SVD, pick the top R, and use that for
your initialization.

2362
00:53:53,990 --> 00:53:54,000

 

2363
00:53:54,000 --> 00:53:57,510

What that does is it starts you off with

2364
00:53:57,510 --> 00:53:57,520
What that does is it starts you off with
 

2365
00:53:57,520 --> 00:53:59,589
What that does is it starts you off with
a being such that it's moving in the

2366
00:53:59,589 --> 00:53:59,599
a being such that it's moving in the
 

2367
00:53:59,599 --> 00:54:00,790
a being such that it's moving in the
direction the gradient would want to

2368
00:54:00,790 --> 00:54:00,800
direction the gradient would want to
 

2369
00:54:00,800 --> 00:54:04,069
direction the gradient would want to
move largely and then w can move can

2370
00:54:04,069 --> 00:54:04,079
move largely and then w can move can
 

2371
00:54:04,079 --> 00:54:05,910
move largely and then w can move can
choose to move in specifically how much

2372
00:54:05,910 --> 00:54:05,920
choose to move in specifically how much
 

2373
00:54:05,920 --> 00:54:07,349
choose to move in specifically how much
it wants to move out in that direction

2374
00:54:07,349 --> 00:54:07,359
it wants to move out in that direction
 

2375
00:54:07,359 --> 00:54:08,710
it wants to move out in that direction
but then you're allowing a to be a

2376
00:54:08,710 --> 00:54:08,720
but then you're allowing a to be a
 

2377
00:54:08,720 --> 00:54:10,309
but then you're allowing a to be a
learnable parameter so this can adjust

2378
00:54:10,309 --> 00:54:10,319
learnable parameter so this can adjust
 

2379
00:54:10,319 --> 00:54:12,470
learnable parameter so this can adjust
during training but just initialization

2380
00:54:12,470 --> 00:54:12,480
during training but just initialization
 

2381
00:54:12,480 --> 00:54:15,109
during training but just initialization
to get you motion at the beginning this

2382
00:54:15,109 --> 00:54:15,119
to get you motion at the beginning this
 

2383
00:54:15,119 --> 00:54:19,990
to get you motion at the beginning this
is one approach

2384
00:54:19,990 --> 00:54:20,000

 

2385
00:54:20,000 --> 00:54:27,750

Alternate approach says well

2386
00:54:27,750 --> 00:54:27,760

 

2387
00:54:27,760 --> 00:54:31,270

war star is probably also not something

2388
00:54:31,270 --> 00:54:31,280
war star is probably also not something
 

2389
00:54:31,280 --> 00:54:34,790
war star is probably also not something
whose singular values are all balanced.

2390
00:54:34,790 --> 00:54:34,800
whose singular values are all balanced.
 

2391
00:54:34,800 --> 00:54:37,030
whose singular values are all balanced.
It might also have an approximately low

2392
00:54:37,030 --> 00:54:37,040
It might also have an approximately low
 

2393
00:54:37,040 --> 00:54:39,750
It might also have an approximately low
rank structure to it. Not fully but

2394
00:54:39,750 --> 00:54:39,760
rank structure to it. Not fully but
 

2395
00:54:39,760 --> 00:54:41,829
rank structure to it. Not fully but
might have you know a lot of its energy

2396
00:54:41,829 --> 00:54:41,839
might have you know a lot of its energy
 

2397
00:54:41,839 --> 00:54:44,230
might have you know a lot of its energy
in in some particular direction. So

2398
00:54:44,230 --> 00:54:44,240
in in some particular direction. So
 

2399
00:54:44,240 --> 00:54:46,710
in in some particular direction. So
instead of doing instead of going and

2400
00:54:46,710 --> 00:54:46,720
instead of doing instead of going and
 

2401
00:54:46,720 --> 00:54:50,309
instead of doing instead of going and
computing a batch of gradients just say

2402
00:54:50,309 --> 00:54:50,319
computing a batch of gradients just say
 

2403
00:54:50,319 --> 00:54:52,390
computing a batch of gradients just say
warst star came to be the way it is

2404
00:54:52,390 --> 00:54:52,400
warst star came to be the way it is
 

2405
00:54:52,400 --> 00:54:54,549
warst star came to be the way it is
because of batches of gradients landing

2406
00:54:54,549 --> 00:54:54,559
because of batches of gradients landing
 

2407
00:54:54,559 --> 00:54:56,630
because of batches of gradients landing
here.

2408
00:54:56,630 --> 00:54:56,640
here.
 

2409
00:54:56,640 --> 00:54:58,710
here.
Whatever the directions are that are

2410
00:54:58,710 --> 00:54:58,720
Whatever the directions are that are
 

2411
00:54:58,720 --> 00:55:00,150
Whatever the directions are that are
preferred here might be the same

2412
00:55:00,150 --> 00:55:00,160
preferred here might be the same
 

2413
00:55:00,160 --> 00:55:02,309
preferred here might be the same
directions to start in here for this

2414
00:55:02,309 --> 00:55:02,319
directions to start in here for this
 

2415
00:55:02,319 --> 00:55:04,470
directions to start in here for this
problem.

2416
00:55:04,470 --> 00:55:04,480
problem.
 

2417
00:55:04,480 --> 00:55:10,630
problem.
So this approach says

2418
00:55:10,630 --> 00:55:10,640

 

2419
00:55:10,640 --> 00:55:12,549

take SVD

2420
00:55:12,549 --> 00:55:12,559
take SVD
 

2421
00:55:12,559 --> 00:55:14,470
take SVD
of

2422
00:55:14,470 --> 00:55:14,480
of
 

2423
00:55:14,480 --> 00:55:21,109
of
war itself

2424
00:55:21,109 --> 00:55:21,119

 

2425
00:55:21,119 --> 00:55:24,630

pick top R

2426
00:55:24,630 --> 00:55:24,640
pick top R
 

2427
00:55:24,640 --> 00:55:26,470
pick top R
and use that to initialize your update.

2428
00:55:26,470 --> 00:55:26,480
and use that to initialize your update.
 

2429
00:55:26,480 --> 00:55:27,910
and use that to initialize your update.
This says you're going to start off

2430
00:55:27,910 --> 00:55:27,920
This says you're going to start off
 

2431
00:55:27,920 --> 00:55:30,230
This says you're going to start off
moving align to the directions that W

2432
00:55:30,230 --> 00:55:30,240
moving align to the directions that W
 

2433
00:55:30,240 --> 00:55:32,710
moving align to the directions that W
star is moving a lot in is is is is

2434
00:55:32,710 --> 00:55:32,720
star is moving a lot in is is is is
 

2435
00:55:32,720 --> 00:55:36,069
star is moving a lot in is is is is
amplifying a lot.

2436
00:55:36,069 --> 00:55:36,079
amplifying a lot.
 

2437
00:55:36,079 --> 00:55:38,870
amplifying a lot.
Okay, these are and you can take hybrids

2438
00:55:38,870 --> 00:55:38,880
Okay, these are and you can take hybrids
 

2439
00:55:38,880 --> 00:55:40,870
Okay, these are and you can take hybrids
of these.

2440
00:55:40,870 --> 00:55:40,880
of these.
 

2441
00:55:40,880 --> 00:55:43,510
of these.
Okay, so you can take this approach and

2442
00:55:43,510 --> 00:55:43,520
Okay, so you can take this approach and
 

2443
00:55:43,520 --> 00:55:45,990
Okay, so you can take this approach and
say well I won't take R. I'll just take

2444
00:55:45,990 --> 00:55:46,000
say well I won't take R. I'll just take
 

2445
00:55:46,000 --> 00:55:47,589
say well I won't take R. I'll just take
some of the tops and I'll make the rest

2446
00:55:47,589 --> 00:55:47,599
some of the tops and I'll make the rest
 

2447
00:55:47,599 --> 00:55:50,870
some of the tops and I'll make the rest
random. And people explore this uh for

2448
00:55:50,870 --> 00:55:50,880
random. And people explore this uh for
 

2449
00:55:50,880 --> 00:55:53,510
random. And people explore this uh for
kind of how to do the initialization of

2450
00:55:53,510 --> 00:55:53,520
kind of how to do the initialization of
 

2451
00:55:53,520 --> 00:55:57,510
kind of how to do the initialization of
a

2452
00:55:57,510 --> 00:55:57,520

 

2453
00:55:57,520 --> 00:55:58,789

So you can look in the literature but

2454
00:55:58,789 --> 00:55:58,799
So you can look in the literature but
 

2455
00:55:58,799 --> 00:56:00,630
So you can look in the literature but
this kind of gets you set to the kind of

2456
00:56:00,630 --> 00:56:00,640
this kind of gets you set to the kind of
 

2457
00:56:00,640 --> 00:56:02,069
this kind of gets you set to the kind of
main components of what people talk

2458
00:56:02,069 --> 00:56:02,079
main components of what people talk
 

2459
00:56:02,079 --> 00:56:13,030
main components of what people talk
about. Yeah.

2460
00:56:13,030 --> 00:56:13,040

 

2461
00:56:13,040 --> 00:56:15,109

>> Certainly this is not random. This is

2462
00:56:15,109 --> 00:56:15,119
>> Certainly this is not random. This is
 

2463
00:56:15,119 --> 00:56:16,950
>> Certainly this is not random. This is
this is definitely not random, right?

2464
00:56:16,950 --> 00:56:16,960
this is definitely not random, right?
 

2465
00:56:16,960 --> 00:56:20,549
this is definitely not random, right?
Yeah. You start somewhere. Yes, that's

2466
00:56:20,549 --> 00:56:20,559
Yeah. You start somewhere. Yes, that's
 

2467
00:56:20,559 --> 00:56:23,589
Yeah. You start somewhere. Yes, that's
true. Um so you won't get diversity if

2468
00:56:23,589 --> 00:56:23,599
true. Um so you won't get diversity if
 

2469
00:56:23,599 --> 00:56:24,950
true. Um so you won't get diversity if
you like do it again with a different

2470
00:56:24,950 --> 00:56:24,960
you like do it again with a different
 

2471
00:56:24,960 --> 00:56:27,589
you like do it again with a different
seed on where you start. You still start

2472
00:56:27,589 --> 00:56:27,599
seed on where you start. You still start
 

2473
00:56:27,599 --> 00:56:30,230
seed on where you start. You still start
there. Um you still have the randomness

2474
00:56:30,230 --> 00:56:30,240
there. Um you still have the randomness
 

2475
00:56:30,240 --> 00:56:33,030
there. Um you still have the randomness
in your back shuffling that can happen.

2476
00:56:33,030 --> 00:56:33,040
in your back shuffling that can happen.
 

2477
00:56:33,040 --> 00:56:34,950
in your back shuffling that can happen.
So your actual evolution path is still

2478
00:56:34,950 --> 00:56:34,960
So your actual evolution path is still
 

2479
00:56:34,960 --> 00:56:37,349
So your actual evolution path is still
random but yeah this is the initial

2480
00:56:37,349 --> 00:56:37,359
random but yeah this is the initial
 

2481
00:56:37,359 --> 00:56:38,710
random but yeah this is the initial
condition is far less random. Both of

2482
00:56:38,710 --> 00:56:38,720
condition is far less random. Both of
 

2483
00:56:38,720 --> 00:56:42,150
condition is far less random. Both of
these are less random.

2484
00:56:42,150 --> 00:56:42,160
these are less random.
 

2485
00:56:42,160 --> 00:56:43,349
these are less random.
This is definitely fully deterministic

2486
00:56:43,349 --> 00:56:43,359
This is definitely fully deterministic
 

2487
00:56:43,359 --> 00:56:44,710
This is definitely fully deterministic
and this is also deterministic if you go

2488
00:56:44,710 --> 00:56:44,720
and this is also deterministic if you go
 

2489
00:56:44,720 --> 00:56:45,750
and this is also deterministic if you go
through the entire batch. It only

2490
00:56:45,750 --> 00:56:45,760
through the entire batch. It only
 

2491
00:56:45,760 --> 00:56:47,030
through the entire batch. It only
depends on the batch. So if you

2492
00:56:47,030 --> 00:56:47,040
depends on the batch. So if you
 

2493
00:56:47,040 --> 00:56:48,470
depends on the batch. So if you
subsample the batch there's a randomness

2494
00:56:48,470 --> 00:56:48,480
subsample the batch there's a randomness
 

2495
00:56:48,480 --> 00:56:49,829
subsample the batch there's a randomness
from subsampling the batch but nothing

2496
00:56:49,829 --> 00:56:49,839
from subsampling the batch but nothing
 

2497
00:56:49,839 --> 00:56:51,510
from subsampling the batch but nothing
else.

2498
00:56:51,510 --> 00:56:51,520
else.
 

2499
00:56:51,520 --> 00:56:55,589
else.
Yeah, that's that is true. Um, so does

2500
00:56:55,589 --> 00:56:55,599
Yeah, that's that is true. Um, so does
 

2501
00:56:55,599 --> 00:56:57,270
Yeah, that's that is true. Um, so does
everyone understand that comment? The

2502
00:56:57,270 --> 00:56:57,280
everyone understand that comment? The
 

2503
00:56:57,280 --> 00:56:58,549
everyone understand that comment? The
comment was that we're definitely doing

2504
00:56:58,549 --> 00:56:58,559
comment was that we're definitely doing
 

2505
00:56:58,559 --> 00:57:01,030
comment was that we're definitely doing
something not random. Does this matter?

2506
00:57:01,030 --> 00:57:01,040
something not random. Does this matter?
 

2507
00:57:01,040 --> 00:57:03,030
something not random. Does this matter?
And the answer is it definitely matters

2508
00:57:03,030 --> 00:57:03,040
And the answer is it definitely matters
 

2509
00:57:03,040 --> 00:57:04,870
And the answer is it definitely matters
if you wanted to sample like different

2510
00:57:04,870 --> 00:57:04,880
if you wanted to sample like different
 

2511
00:57:04,880 --> 00:57:06,390
if you wanted to sample like different
initializations to see how well you

2512
00:57:06,390 --> 00:57:06,400
initializations to see how well you
 

2513
00:57:06,400 --> 00:57:08,630
initializations to see how well you
might be doing. But in deep learning,

2514
00:57:08,630 --> 00:57:08,640
might be doing. But in deep learning,
 

2515
00:57:08,640 --> 00:57:10,470
might be doing. But in deep learning,
the reality is many times unless we're

2516
00:57:10,470 --> 00:57:10,480
the reality is many times unless we're
 

2517
00:57:10,480 --> 00:57:12,630
the reality is many times unless we're
doing experiments, we're not actually

2518
00:57:12,630 --> 00:57:12,640
doing experiments, we're not actually
 

2519
00:57:12,640 --> 00:57:13,510
doing experiments, we're not actually
sampling lots of different

2520
00:57:13,510 --> 00:57:13,520
sampling lots of different
 

2521
00:57:13,520 --> 00:57:16,150
sampling lots of different
initializations in practice

2522
00:57:16,150 --> 00:57:16,160
initializations in practice
 

2523
00:57:16,160 --> 00:57:18,309
initializations in practice
um for production

2524
00:57:18,309 --> 00:57:18,319
um for production
 

2525
00:57:18,319 --> 00:57:21,030
um for production
because it's too expensive.

2526
00:57:21,030 --> 00:57:21,040
because it's too expensive.
 

2527
00:57:21,040 --> 00:57:23,430
because it's too expensive.
So

2528
00:57:23,430 --> 00:57:23,440
So
 

2529
00:57:23,440 --> 00:57:25,030
So
people don't mind that you're losing

2530
00:57:25,030 --> 00:57:25,040
people don't mind that you're losing
 

2531
00:57:25,040 --> 00:57:26,950
people don't mind that you're losing
this if you can get a better

2532
00:57:26,950 --> 00:57:26,960
this if you can get a better
 

2533
00:57:26,960 --> 00:57:28,309
this if you can get a better
initialization that goes the right

2534
00:57:28,309 --> 00:57:28,319
initialization that goes the right
 

2535
00:57:28,319 --> 00:57:29,829
initialization that goes the right
place.

2536
00:57:29,829 --> 00:57:29,839
place.
 

2537
00:57:29,839 --> 00:57:33,349
place.
Now again there's not a there's papers

2538
00:57:33,349 --> 00:57:33,359
Now again there's not a there's papers
 

2539
00:57:33,359 --> 00:57:35,589
Now again there's not a there's papers
that show this approaches work better.

2540
00:57:35,589 --> 00:57:35,599
that show this approaches work better.
 

2541
00:57:35,599 --> 00:57:38,950
that show this approaches work better.
There's other papers arguing that

2542
00:57:38,950 --> 00:57:38,960
There's other papers arguing that
 

2543
00:57:38,960 --> 00:57:40,549
There's other papers arguing that
these

2544
00:57:40,549 --> 00:57:40,559
these
 

2545
00:57:40,559 --> 00:57:41,910
these
don't give you that much of an

2546
00:57:41,910 --> 00:57:41,920
don't give you that much of an
 

2547
00:57:41,920 --> 00:57:45,510
don't give you that much of an
improvement uh or and some argue they

2548
00:57:45,510 --> 00:57:45,520
improvement uh or and some argue they
 

2549
00:57:45,520 --> 00:57:47,750
improvement uh or and some argue they
might even be harmful because they think

2550
00:57:47,750 --> 00:57:47,760
might even be harmful because they think
 

2551
00:57:47,760 --> 00:57:49,910
might even be harmful because they think
that the actual updates that you need to

2552
00:57:49,910 --> 00:57:49,920
that the actual updates that you need to
 

2553
00:57:49,920 --> 00:57:51,589
that the actual updates that you need to
move in are not necessarily in the

2554
00:57:51,589 --> 00:57:51,599
move in are not necessarily in the
 

2555
00:57:51,599 --> 00:57:53,589
move in are not necessarily in the
principal directions

2556
00:57:53,589 --> 00:57:53,599
principal directions
 

2557
00:57:53,599 --> 00:57:56,069
principal directions
but uh in terms of what's delivering

2558
00:57:56,069 --> 00:57:56,079
but uh in terms of what's delivering
 

2559
00:57:56,079 --> 00:57:57,829
but uh in terms of what's delivering
performance

2560
00:57:57,829 --> 00:57:57,839
performance
 

2561
00:57:57,839 --> 00:58:01,910
performance
but this is what's known

2562
00:58:01,910 --> 00:58:01,920
but this is what's known
 

2563
00:58:01,920 --> 00:58:03,510
but this is what's known
as standard different choices for

2564
00:58:03,510 --> 00:58:03,520
as standard different choices for
 

2565
00:58:03,520 --> 00:58:06,390
as standard different choices for
initialization.

2566
00:58:06,390 --> 00:58:06,400

 

2567
00:58:06,400 --> 00:58:11,430

Okay, any questions on Laura's

2568
00:58:11,430 --> 00:58:11,440

 

2569
00:58:11,440 --> 00:58:14,069

uh covered

2570
00:58:14,069 --> 00:58:14,079
uh covered
 

2571
00:58:14,079 --> 00:58:17,829
uh covered
this in the detail I wanted.

2572
00:58:17,829 --> 00:58:17,839
this in the detail I wanted.
 

2573
00:58:17,839 --> 00:58:19,990
this in the detail I wanted.
So again, if you look in the literature,

2574
00:58:19,990 --> 00:58:20,000
So again, if you look in the literature,
 

2575
00:58:20,000 --> 00:58:21,510
So again, if you look in the literature,
there's lots of variations on luras

2576
00:58:21,510 --> 00:58:21,520
there's lots of variations on luras
 

2577
00:58:21,520 --> 00:58:23,510
there's lots of variations on luras
because they're very practical. I mean

2578
00:58:23,510 --> 00:58:23,520
because they're very practical. I mean
 

2579
00:58:23,520 --> 00:58:25,349
because they're very practical. I mean
effectively

2580
00:58:25,349 --> 00:58:25,359
effectively
 

2581
00:58:25,359 --> 00:58:27,750
effectively
practically most fine tunes that are

2582
00:58:27,750 --> 00:58:27,760
practically most fine tunes that are
 

2583
00:58:27,760 --> 00:58:31,910
practically most fine tunes that are
done of models are done using loras.

2584
00:58:31,910 --> 00:58:31,920
done of models are done using loras.
 

2585
00:58:31,920 --> 00:58:33,910
done of models are done using loras.
uh I would say empirically this is this

2586
00:58:33,910 --> 00:58:33,920
uh I would say empirically this is this
 

2587
00:58:33,920 --> 00:58:37,349
uh I would say empirically this is this
the way it is. Um so for example I mean

2588
00:58:37,349 --> 00:58:37,359
the way it is. Um so for example I mean
 

2589
00:58:37,359 --> 00:58:38,789
the way it is. Um so for example I mean
I'm talking about transformer models but

2590
00:58:38,789 --> 00:58:38,799
I'm talking about transformer models but
 

2591
00:58:38,799 --> 00:58:41,750
I'm talking about transformer models but
in diffusion models people will do uh

2592
00:58:41,750 --> 00:58:41,760
in diffusion models people will do uh
 

2593
00:58:41,760 --> 00:58:44,870
in diffusion models people will do uh
alla that says I'm going to tune my

2594
00:58:44,870 --> 00:58:44,880
alla that says I'm going to tune my
 

2595
00:58:44,880 --> 00:58:47,030
alla that says I'm going to tune my
model to be better at generating you

2596
00:58:47,030 --> 00:58:47,040
model to be better at generating you
 

2597
00:58:47,040 --> 00:58:49,910
model to be better at generating you
know cats I have lots of pictures of

2598
00:58:49,910 --> 00:58:49,920
know cats I have lots of pictures of
 

2599
00:58:49,920 --> 00:58:51,589
know cats I have lots of pictures of
cats I want to be better at making cats

2600
00:58:51,589 --> 00:58:51,599
cats I want to be better at making cats
 

2601
00:58:51,599 --> 00:58:54,630
cats I want to be better at making cats
so here or this kind of cats and they'll

2602
00:58:54,630 --> 00:58:54,640
so here or this kind of cats and they'll
 

2603
00:58:54,640 --> 00:58:57,589
so here or this kind of cats and they'll
like just do do Laura fine tuning

2604
00:58:57,589 --> 00:58:57,599
like just do do Laura fine tuning
 

2605
00:58:57,599 --> 00:58:59,109
like just do do Laura fine tuning
so

2606
00:58:59,109 --> 00:58:59,119
so
 

2607
00:58:59,119 --> 00:59:01,030
so
luras are very practical people have

2608
00:59:01,030 --> 00:59:01,040
luras are very practical people have
 

2609
00:59:01,040 --> 00:59:03,109
luras are very practical people have
found that given the same parameter

2610
00:59:03,109 --> 00:59:03,119
found that given the same parameter
 

2611
00:59:03,119 --> 00:59:06,230
found that given the same parameter
count a Laura will tend to do better

2612
00:59:06,230 --> 00:59:06,240
count a Laura will tend to do better
 

2613
00:59:06,240 --> 00:59:11,270
count a Laura will tend to do better
than a soft prompt or a prefix uh soft

2614
00:59:11,270 --> 00:59:11,280
than a soft prompt or a prefix uh soft
 

2615
00:59:11,280 --> 00:59:14,630
than a soft prompt or a prefix uh soft
prefix. Lauras however uh for running at

2616
00:59:14,630 --> 00:59:14,640
prefix. Lauras however uh for running at
 

2617
00:59:14,640 --> 00:59:16,309
prefix. Lauras however uh for running at
inference time require you to actually

2618
00:59:16,309 --> 00:59:16,319
inference time require you to actually
 

2619
00:59:16,319 --> 00:59:18,710
inference time require you to actually
do this merging of into the weights. So

2620
00:59:18,710 --> 00:59:18,720
do this merging of into the weights. So
 

2621
00:59:18,720 --> 00:59:20,710
do this merging of into the weights. So
that's different. So there's system

2622
00:59:20,710 --> 00:59:20,720
that's different. So there's system
 

2623
00:59:20,720 --> 00:59:22,630
that's different. So there's system
level considerations that can be

2624
00:59:22,630 --> 00:59:22,640
level considerations that can be
 

2625
00:59:22,640 --> 00:59:25,750
level considerations that can be
different in using LA.

2626
00:59:25,750 --> 00:59:25,760
different in using LA.
 

2627
00:59:25,760 --> 00:59:28,230
different in using LA.
Okay,

2628
00:59:28,230 --> 00:59:28,240
Okay,
 

2629
00:59:28,240 --> 00:59:29,990
Okay,
good.

2630
00:59:29,990 --> 00:59:30,000
good.
 

2631
00:59:30,000 --> 00:59:33,190
good.
Um I will make one comment here before I

2632
00:59:33,190 --> 00:59:33,200
Um I will make one comment here before I
 

2633
00:59:33,200 --> 00:59:35,109
Um I will make one comment here before I
move on.

2634
00:59:35,109 --> 00:59:35,119
move on.
 

2635
00:59:35,119 --> 00:59:37,829
move on.
The fact that Laura's worked

2636
00:59:37,829 --> 00:59:37,839
The fact that Laura's worked
 

2637
00:59:37,839 --> 00:59:41,589
The fact that Laura's worked
made people also think about, wait a

2638
00:59:41,589 --> 00:59:41,599
made people also think about, wait a
 

2639
00:59:41,599 --> 00:59:46,390
made people also think about, wait a
minute, I have a a I have a a model. I

2640
00:59:46,390 --> 00:59:46,400
minute, I have a a I have a a model. I
 

2641
00:59:46,400 --> 00:59:49,430
minute, I have a a I have a a model. I
did a Laura on it to make it be better

2642
00:59:49,430 --> 00:59:49,440
did a Laura on it to make it be better
 

2643
00:59:49,440 --> 00:59:52,309
did a Laura on it to make it be better
at this kind of task.

2644
00:59:52,309 --> 00:59:52,319
at this kind of task.
 

2645
00:59:52,319 --> 00:59:54,630
at this kind of task.
I also the same model. I did a lura on

2646
00:59:54,630 --> 00:59:54,640
I also the same model. I did a lura on
 

2647
00:59:54,640 --> 00:59:59,510
I also the same model. I did a lura on
it to get better at this other task.

2648
00:59:59,510 --> 00:59:59,520
it to get better at this other task.
 

2649
00:59:59,520 --> 01:00:01,829
it to get better at this other task.
It got better at both those tasks from

2650
01:00:01,829 --> 01:00:01,839
It got better at both those tasks from
 

2651
01:00:01,839 --> 01:00:04,069
It got better at both those tasks from
doing Lauras from this one place. I

2652
01:00:04,069 --> 01:00:04,079
doing Lauras from this one place. I
 

2653
01:00:04,079 --> 01:00:05,430
doing Lauras from this one place. I
trained them separately, but I was

2654
01:00:05,430 --> 01:00:05,440
trained them separately, but I was
 

2655
01:00:05,440 --> 01:00:08,549
trained them separately, but I was
anchored at the same place.

2656
01:00:08,549 --> 01:00:08,559
anchored at the same place.
 

2657
01:00:08,559 --> 01:00:13,030
anchored at the same place.
Why don't I add both of them in?

2658
01:00:13,030 --> 01:00:13,040
Why don't I add both of them in?
 

2659
01:00:13,040 --> 01:00:15,670
Why don't I add both of them in?
Right? So, the fact that Laura's worked

2660
01:00:15,670 --> 01:00:15,680
Right? So, the fact that Laura's worked
 

2661
01:00:15,680 --> 01:00:18,870
Right? So, the fact that Laura's worked
made people be more willing to explore

2662
01:00:18,870 --> 01:00:18,880
made people be more willing to explore
 

2663
01:00:18,880 --> 01:00:20,390
made people be more willing to explore
the idea of doing model level

2664
01:00:20,390 --> 01:00:20,400
the idea of doing model level
 

2665
01:00:20,400 --> 01:00:22,630
the idea of doing model level
arithmetic. Take the entire parameters

2666
01:00:22,630 --> 01:00:22,640
arithmetic. Take the entire parameters
 

2667
01:00:22,640 --> 01:00:24,390
arithmetic. Take the entire parameters
and add them together or average them

2668
01:00:24,390 --> 01:00:24,400
and add them together or average them
 

2669
01:00:24,400 --> 01:00:26,950
and add them together or average them
together. And the amazing thing is that

2670
01:00:26,950 --> 01:00:26,960
together. And the amazing thing is that
 

2671
01:00:26,960 --> 01:00:29,670
together. And the amazing thing is that
that also works.

2672
01:00:29,670 --> 01:00:29,680
that also works.
 

2673
01:00:29,680 --> 01:00:31,990
that also works.
Okay, so we're not going to talk about

2674
01:00:31,990 --> 01:00:32,000
Okay, so we're not going to talk about
 

2675
01:00:32,000 --> 01:00:33,750
Okay, so we're not going to talk about
it um in the class. We should be aware

2676
01:00:33,750 --> 01:00:33,760
it um in the class. We should be aware
 

2677
01:00:33,760 --> 01:00:34,789
it um in the class. We should be aware
this is like a right point to make you

2678
01:00:34,789 --> 01:00:34,799
this is like a right point to make you
 

2679
01:00:34,799 --> 01:00:39,270
this is like a right point to make you
aware of it. There's a whole business of

2680
01:00:39,270 --> 01:00:39,280
aware of it. There's a whole business of
 

2681
01:00:39,280 --> 01:00:42,069
aware of it. There's a whole business of
merging models together of taking models

2682
01:00:42,069 --> 01:00:42,079
merging models together of taking models
 

2683
01:00:42,079 --> 01:00:43,589
merging models together of taking models
that have been tuned during Allora from

2684
01:00:43,589 --> 01:00:43,599
that have been tuned during Allora from
 

2685
01:00:43,599 --> 01:00:45,829
that have been tuned during Allora from
the same thing, another model tuned use

2686
01:00:45,829 --> 01:00:45,839
the same thing, another model tuned use
 

2687
01:00:45,839 --> 01:00:47,510
the same thing, another model tuned use
tune fine-tuned using Allora from the

2688
01:00:47,510 --> 01:00:47,520
tune fine-tuned using Allora from the
 

2689
01:00:47,520 --> 01:00:49,510
tune fine-tuned using Allora from the
same place and then like mixing them

2690
01:00:49,510 --> 01:00:49,520
same place and then like mixing them
 

2691
01:00:49,520 --> 01:00:51,190
same place and then like mixing them
together.

2692
01:00:51,190 --> 01:00:51,200
together.
 

2693
01:00:51,200 --> 01:00:53,109
together.
Um so then you you can adjust how much

2694
01:00:53,109 --> 01:00:53,119
Um so then you you can adjust how much
 

2695
01:00:53,119 --> 01:00:55,030
Um so then you you can adjust how much
of this one and how much of that one.

2696
01:00:55,030 --> 01:00:55,040
of this one and how much of that one.
 

2697
01:00:55,040 --> 01:00:57,109
of this one and how much of that one.
What exactly is the right geometry to

2698
01:00:57,109 --> 01:00:57,119
What exactly is the right geometry to
 

2699
01:00:57,119 --> 01:00:59,510
What exactly is the right geometry to
add them? A little bit of subtlety here.

2700
01:00:59,510 --> 01:00:59,520
add them? A little bit of subtlety here.
 

2701
01:00:59,520 --> 01:01:01,589
add them? A little bit of subtlety here.
Um in connection should I think about as

2702
01:01:01,589 --> 01:01:01,599
Um in connection should I think about as
 

2703
01:01:01,599 --> 01:01:03,510
Um in connection should I think about as
adding on a sphere? Should I add on

2704
01:01:03,510 --> 01:01:03,520
adding on a sphere? Should I add on
 

2705
01:01:03,520 --> 01:01:05,030
adding on a sphere? Should I add on
uklidian space? But the important thing

2706
01:01:05,030 --> 01:01:05,040
uklidian space? But the important thing
 

2707
01:01:05,040 --> 01:01:07,430
uklidian space? But the important thing
is even if you add a uklidian space and

2708
01:01:07,430 --> 01:01:07,440
is even if you add a uklidian space and
 

2709
01:01:07,440 --> 01:01:09,190
is even if you add a uklidian space and
you just tune how much you want of each

2710
01:01:09,190 --> 01:01:09,200
you just tune how much you want of each
 

2711
01:01:09,200 --> 01:01:11,190
you just tune how much you want of each
of them you can sometimes get models

2712
01:01:11,190 --> 01:01:11,200
of them you can sometimes get models
 

2713
01:01:11,200 --> 01:01:14,549
of them you can sometimes get models
that have now acquired both capabilities

2714
01:01:14,549 --> 01:01:14,559
that have now acquired both capabilities
 

2715
01:01:14,559 --> 01:01:16,069
that have now acquired both capabilities
and sometimes you know synergistic

2716
01:01:16,069 --> 01:01:16,079
and sometimes you know synergistic
 

2717
01:01:16,079 --> 01:01:18,150
and sometimes you know synergistic
capabilities too. So people play around

2718
01:01:18,150 --> 01:01:18,160
capabilities too. So people play around
 

2719
01:01:18,160 --> 01:01:19,910
capabilities too. So people play around
with this, but Laura's tell you why this

2720
01:01:19,910 --> 01:01:19,920
with this, but Laura's tell you why this
 

2721
01:01:19,920 --> 01:01:21,990
with this, but Laura's tell you why this
might be reasonable kind of at a high

2722
01:01:21,990 --> 01:01:22,000
might be reasonable kind of at a high
 

2723
01:01:22,000 --> 01:01:23,109
might be reasonable kind of at a high
level. Yeah.

2724
01:01:23,109 --> 01:01:23,119
level. Yeah.
 

2725
01:01:23,119 --> 01:01:26,549
level. Yeah.
>> Yeah. Um I had a question about So it

2726
01:01:26,549 --> 01:01:26,559
>> Yeah. Um I had a question about So it
 

2727
01:01:26,559 --> 01:01:28,069
>> Yeah. Um I had a question about So it
seems like kind of at a high level what

2728
01:01:28,069 --> 01:01:28,079
seems like kind of at a high level what
 

2729
01:01:28,079 --> 01:01:30,230
seems like kind of at a high level what
we're trying to do is like figure out

2730
01:01:30,230 --> 01:01:30,240
we're trying to do is like figure out
 

2731
01:01:30,240 --> 01:01:32,390
we're trying to do is like figure out
the knobs and levers that have been

2732
01:01:32,390 --> 01:01:32,400
the knobs and levers that have been
 

2733
01:01:32,400 --> 01:01:34,710
the knobs and levers that have been
learned and then kind of turn those

2734
01:01:34,710 --> 01:01:34,720
learned and then kind of turn those
 

2735
01:01:34,720 --> 01:01:36,309
learned and then kind of turn those
knobs and levers rather than trying to

2736
01:01:36,309 --> 01:01:36,319
knobs and levers rather than trying to
 

2737
01:01:36,319 --> 01:01:39,190
knobs and levers rather than trying to
retrain the entire model. Um and I know

2738
01:01:39,190 --> 01:01:39,200
retrain the entire model. Um and I know
 

2739
01:01:39,200 --> 01:01:40,630
retrain the entire model. Um and I know
one of the ways or one of the things

2740
01:01:40,630 --> 01:01:40,640
one of the ways or one of the things
 

2741
01:01:40,640 --> 01:01:42,630
one of the ways or one of the things
I've heard about a lot is people doing

2742
01:01:42,630 --> 01:01:42,640
I've heard about a lot is people doing
 

2743
01:01:42,640 --> 01:01:44,390
I've heard about a lot is people doing
sparse autocoders for interpretability.

2744
01:01:44,390 --> 01:01:44,400
sparse autocoders for interpretability.
 

2745
01:01:44,400 --> 01:01:45,990
sparse autocoders for interpretability.
Yes. trying to understand and it looks

2746
01:01:45,990 --> 01:01:46,000
Yes. trying to understand and it looks
 

2747
01:01:46,000 --> 01:01:48,230
Yes. trying to understand and it looks
kind of like Aurora is an auto looks

2748
01:01:48,230 --> 01:01:48,240
kind of like Aurora is an auto looks
 

2749
01:01:48,240 --> 01:01:49,829
kind of like Aurora is an auto looks
kind of like an autoenccoder. So I'm

2750
01:01:49,829 --> 01:01:49,839
kind of like an autoenccoder. So I'm
 

2751
01:01:49,839 --> 01:01:51,829
kind of like an autoenccoder. So I'm
wondering do people do these with sparse

2752
01:01:51,829 --> 01:01:51,839
wondering do people do these with sparse
 

2753
01:01:51,839 --> 01:01:53,670
wondering do people do these with sparse
auto encoders where you're projecting

2754
01:01:53,670 --> 01:01:53,680
auto encoders where you're projecting
 

2755
01:01:53,680 --> 01:01:55,190
auto encoders where you're projecting
into a higher dimensional space but

2756
01:01:55,190 --> 01:01:55,200
into a higher dimensional space but
 

2757
01:01:55,200 --> 01:01:57,349
into a higher dimensional space but
enforcing some sort of sparity and then

2758
01:01:57,349 --> 01:01:57,359
enforcing some sort of sparity and then
 

2759
01:01:57,359 --> 01:01:59,349
enforcing some sort of sparity and then
trying to train on that or I don't know

2760
01:01:59,349 --> 01:01:59,359
trying to train on that or I don't know
 

2761
01:01:59,359 --> 01:02:01,190
trying to train on that or I don't know
if that even makes sense but

2762
01:02:01,190 --> 01:02:01,200
if that even makes sense but
 

2763
01:02:01,200 --> 01:02:02,630
if that even makes sense but
>> okay so I'm going to budget myself only

2764
01:02:02,630 --> 01:02:02,640
>> okay so I'm going to budget myself only
 

2765
01:02:02,640 --> 01:02:03,990
>> okay so I'm going to budget myself only
a minute to answer your question because

2766
01:02:03,990 --> 01:02:04,000
a minute to answer your question because
 

2767
01:02:04,000 --> 01:02:06,309
a minute to answer your question because
I want to get to something else but um

2768
01:02:06,309 --> 01:02:06,319
I want to get to something else but um
 

2769
01:02:06,319 --> 01:02:09,510
I want to get to something else but um
so at a high level the so people

2770
01:02:09,510 --> 01:02:09,520
so at a high level the so people
 

2771
01:02:09,520 --> 01:02:10,549
so at a high level the so people
understand the question the question was

2772
01:02:10,549 --> 01:02:10,559
understand the question the question was
 

2773
01:02:10,559 --> 01:02:13,589
understand the question the question was
saying look in interpretability when

2774
01:02:13,589 --> 01:02:13,599
saying look in interpretability when
 

2775
01:02:13,599 --> 01:02:14,710
saying look in interpretability when
you're trying to understand what a model

2776
01:02:14,710 --> 01:02:14,720
you're trying to understand what a model
 

2777
01:02:14,720 --> 01:02:16,309
you're trying to understand what a model
is doing you can train a sparse

2778
01:02:16,309 --> 01:02:16,319
is doing you can train a sparse
 

2779
01:02:16,319 --> 01:02:18,789
is doing you can train a sparse
autoenccoder where you take it you train

2780
01:02:18,789 --> 01:02:18,799
autoenccoder where you take it you train
 

2781
01:02:18,799 --> 01:02:21,990
autoenccoder where you take it you train
a model a little little autoenccoder for

2782
01:02:21,990 --> 01:02:22,000
a model a little little autoenccoder for
 

2783
01:02:22,000 --> 01:02:23,990
a model a little little autoenccoder for
some part of the model that says I would

2784
01:02:23,990 --> 01:02:24,000
some part of the model that says I would
 

2785
01:02:24,000 --> 01:02:26,789
some part of the model that says I would
like to run this through a vector that's

2786
01:02:26,789 --> 01:02:26,799
like to run this through a vector that's
 

2787
01:02:26,799 --> 01:02:28,390
like to run this through a vector that's
sparse so I can see what different

2788
01:02:28,390 --> 01:02:28,400
sparse so I can see what different
 

2789
01:02:28,400 --> 01:02:31,109
sparse so I can see what different
things might correspond to. So when you

2790
01:02:31,109 --> 01:02:31,119
things might correspond to. So when you
 

2791
01:02:31,119 --> 01:02:33,190
things might correspond to. So when you
do sparse autoenccoders what you're

2792
01:02:33,190 --> 01:02:33,200
do sparse autoenccoders what you're
 

2793
01:02:33,200 --> 01:02:34,390
do sparse autoenccoders what you're
trying to do is you're saying I would

2794
01:02:34,390 --> 01:02:34,400
trying to do is you're saying I would
 

2795
01:02:34,400 --> 01:02:36,069
trying to do is you're saying I would
like to decompose what's happening into

2796
01:02:36,069 --> 01:02:36,079
like to decompose what's happening into
 

2797
01:02:36,079 --> 01:02:39,270
like to decompose what's happening into
a sum of low rank subspaces

2798
01:02:39,270 --> 01:02:39,280
a sum of low rank subspaces
 

2799
01:02:39,280 --> 01:02:41,589
a sum of low rank subspaces
that where each subspace is hopefully a

2800
01:02:41,589 --> 01:02:41,599
that where each subspace is hopefully a
 

2801
01:02:41,599 --> 01:02:44,150
that where each subspace is hopefully a
lot is I try where I'm expecting motion

2802
01:02:44,150 --> 01:02:44,160
lot is I try where I'm expecting motion
 

2803
01:02:44,160 --> 01:02:46,470
lot is I try where I'm expecting motion
to be only in low rank ways as opposed

2804
01:02:46,470 --> 01:02:46,480
to be only in low rank ways as opposed
 

2805
01:02:46,480 --> 01:02:48,789
to be only in low rank ways as opposed
to full. So this is usually done on

2806
01:02:48,789 --> 01:02:48,799
to full. So this is usually done on
 

2807
01:02:48,799 --> 01:02:50,789
to full. So this is usually done on
activations for example. So if you

2808
01:02:50,789 --> 01:02:50,799
activations for example. So if you
 

2809
01:02:50,799 --> 01:02:53,109
activations for example. So if you
expect activations have a low rank space

2810
01:02:53,109 --> 01:02:53,119
expect activations have a low rank space
 

2811
01:02:53,119 --> 01:02:55,430
expect activations have a low rank space
that have corresponds to meaning then

2812
01:02:55,430 --> 01:02:55,440
that have corresponds to meaning then
 

2813
01:02:55,440 --> 01:02:57,270
that have corresponds to meaning then
alla also makes sense for the same

2814
01:02:57,270 --> 01:02:57,280
alla also makes sense for the same
 

2815
01:02:57,280 --> 01:02:59,270
alla also makes sense for the same
reason because it says if this is the

2816
01:02:59,270 --> 01:02:59,280
reason because it says if this is the
 

2817
01:02:59,280 --> 01:03:01,109
reason because it says if this is the
direction which corresponds to being

2818
01:03:01,109 --> 01:03:01,119
direction which corresponds to being
 

2819
01:03:01,119 --> 01:03:03,430
direction which corresponds to being
nice and this is the direction that

2820
01:03:03,430 --> 01:03:03,440
nice and this is the direction that
 

2821
01:03:03,440 --> 01:03:07,349
nice and this is the direction that
corresponds to um being skeptical. If I

2822
01:03:07,349 --> 01:03:07,359
corresponds to um being skeptical. If I
 

2823
01:03:07,359 --> 01:03:08,950
corresponds to um being skeptical. If I
would like to my model to be trained for

2824
01:03:08,950 --> 01:03:08,960
would like to my model to be trained for
 

2825
01:03:08,960 --> 01:03:11,109
would like to my model to be trained for
this problem to be nice and skeptical.

2826
01:03:11,109 --> 01:03:11,119
this problem to be nice and skeptical.
 

2827
01:03:11,119 --> 01:03:13,190
this problem to be nice and skeptical.
What I would like to do is to have align

2828
01:03:13,190 --> 01:03:13,200
What I would like to do is to have align
 

2829
01:03:13,200 --> 01:03:15,190
What I would like to do is to have align
to the activation direction of niceness

2830
01:03:15,190 --> 01:03:15,200
to the activation direction of niceness
 

2831
01:03:15,200 --> 01:03:17,109
to the activation direction of niceness
and arise aligned to the direction of sp

2832
01:03:17,109 --> 01:03:17,119
and arise aligned to the direction of sp
 

2833
01:03:17,119 --> 01:03:20,950
and arise aligned to the direction of sp
of of skeptical like the right motion.

2834
01:03:20,950 --> 01:03:20,960
of of skeptical like the right motion.
 

2835
01:03:20,960 --> 01:03:22,390
of of skeptical like the right motion.
I'm gonna act in this lower lower

2836
01:03:22,390 --> 01:03:22,400
I'm gonna act in this lower lower
 

2837
01:03:22,400 --> 01:03:25,589
I'm gonna act in this lower lower
dimensional space. So yes, that is true.

2838
01:03:25,589 --> 01:03:25,599
dimensional space. So yes, that is true.
 

2839
01:03:25,599 --> 01:03:27,190
dimensional space. So yes, that is true.
Um

2840
01:03:27,190 --> 01:03:27,200
Um
 

2841
01:03:27,200 --> 01:03:29,670
Um
so this is all so we we pointed you to

2842
01:03:29,670 --> 01:03:29,680
so this is all so we we pointed you to
 

2843
01:03:29,680 --> 01:03:30,789
so this is all so we we pointed you to
papers in the project for

2844
01:03:30,789 --> 01:03:30,799
papers in the project for
 

2845
01:03:30,799 --> 01:03:33,029
papers in the project for
interpretability on you know this linear

2846
01:03:33,029 --> 01:03:33,039
interpretability on you know this linear
 

2847
01:03:33,039 --> 01:03:34,630
interpretability on you know this linear
representation hypothesis and things

2848
01:03:34,630 --> 01:03:34,640
representation hypothesis and things
 

2849
01:03:34,640 --> 01:03:36,470
representation hypothesis and things
like this. So yeah, this is definitely

2850
01:03:36,470 --> 01:03:36,480
like this. So yeah, this is definitely
 

2851
01:03:36,480 --> 01:03:38,870
like this. So yeah, this is definitely
connected and this is also connected to

2852
01:03:38,870 --> 01:03:38,880
connected and this is also connected to
 

2853
01:03:38,880 --> 01:03:40,150
connected and this is also connected to
the idea that you why you might be able

2854
01:03:40,150 --> 01:03:40,160
the idea that you why you might be able
 

2855
01:03:40,160 --> 01:03:42,230
the idea that you why you might be able
to do model arithmetic of add different

2856
01:03:42,230 --> 01:03:42,240
to do model arithmetic of add different
 

2857
01:03:42,240 --> 01:03:43,990
to do model arithmetic of add different
models together and get things to you

2858
01:03:43,990 --> 01:03:44,000
models together and get things to you
 

2859
01:03:44,000 --> 01:03:46,710
models together and get things to you
know combine learn both things again an

2860
01:03:46,710 --> 01:03:46,720
know combine learn both things again an
 

2861
01:03:46,720 --> 01:03:50,470
know combine learn both things again an
active area of research uh to understand

2862
01:03:50,470 --> 01:03:50,480
active area of research uh to understand
 

2863
01:03:50,480 --> 01:03:53,029
active area of research uh to understand
how to you know what's going on here why

2864
01:03:53,029 --> 01:03:53,039
how to you know what's going on here why
 

2865
01:03:53,039 --> 01:03:55,990
how to you know what's going on here why
this works

2866
01:03:55,990 --> 01:03:56,000
this works
 

2867
01:03:56,000 --> 01:03:57,510
this works
okay so we've talked about

2868
01:03:57,510 --> 01:03:57,520
okay so we've talked about
 

2869
01:03:57,520 --> 01:03:58,950
okay so we've talked about
initialization and we talked about

2870
01:03:58,950 --> 01:03:58,960
initialization and we talked about
 

2871
01:03:58,960 --> 01:04:01,910
initialization and we talked about
Laura's I want to change uh gears and

2872
01:04:01,910 --> 01:04:01,920
Laura's I want to change uh gears and
 

2873
01:04:01,920 --> 01:04:04,150
Laura's I want to change uh gears and
talk about something different

2874
01:04:04,150 --> 01:04:04,160
talk about something different
 

2875
01:04:04,160 --> 01:04:08,549
talk about something different
So this is the idea of metalarning.

2876
01:04:08,549 --> 01:04:08,559
So this is the idea of metalarning.
 

2877
01:04:08,559 --> 01:04:11,190
So this is the idea of metalarning.
So we've just told you that foundation

2878
01:04:11,190 --> 01:04:11,200
So we've just told you that foundation
 

2879
01:04:11,200 --> 01:04:13,910
So we've just told you that foundation
models are models that we expect to be

2880
01:04:13,910 --> 01:04:13,920
models are models that we expect to be
 

2881
01:04:13,920 --> 01:04:16,710
models are models that we expect to be
good at being fine-tuned.

2882
01:04:16,710 --> 01:04:16,720
good at being fine-tuned.
 

2883
01:04:16,720 --> 01:04:20,069
good at being fine-tuned.
So we train on some task that's broad in

2884
01:04:20,069 --> 01:04:20,079
So we train on some task that's broad in
 

2885
01:04:20,079 --> 01:04:22,150
So we train on some task that's broad in
general. It gets good at doing that and

2886
01:04:22,150 --> 01:04:22,160
general. It gets good at doing that and
 

2887
01:04:22,160 --> 01:04:23,589
general. It gets good at doing that and
then you say okay this is a good place

2888
01:04:23,589 --> 01:04:23,599
then you say okay this is a good place
 

2889
01:04:23,599 --> 01:04:26,309
then you say okay this is a good place
to start for doing a fine tune.

2890
01:04:26,309 --> 01:04:26,319
to start for doing a fine tune.
 

2891
01:04:26,319 --> 01:04:28,549
to start for doing a fine tune.
And metalarning

2892
01:04:28,549 --> 01:04:28,559
And metalarning
 

2893
01:04:28,559 --> 01:04:31,990
And metalarning
is kind of asking the question that says

2894
01:04:31,990 --> 01:04:32,000
is kind of asking the question that says
 

2895
01:04:32,000 --> 01:04:37,430
is kind of asking the question that says
okay if I want to make a model better

2896
01:04:37,430 --> 01:04:37,440
okay if I want to make a model better
 

2897
01:04:37,440 --> 01:04:41,430
okay if I want to make a model better
at what? At being fine-tunable.

2898
01:04:41,430 --> 01:04:41,440
at what? At being fine-tunable.
 

2899
01:04:41,440 --> 01:04:43,109
at what? At being fine-tunable.
Okay, I want to make a model that's

2900
01:04:43,109 --> 01:04:43,119
Okay, I want to make a model that's
 

2901
01:04:43,119 --> 01:04:44,870
Okay, I want to make a model that's
better at being fine-tunable for certain

2902
01:04:44,870 --> 01:04:44,880
better at being fine-tunable for certain
 

2903
01:04:44,880 --> 01:04:47,589
better at being fine-tunable for certain
kinds of tasks.

2904
01:04:47,589 --> 01:04:47,599
kinds of tasks.
 

2905
01:04:47,599 --> 01:04:50,950
kinds of tasks.
How do I do it?

2906
01:04:50,950 --> 01:04:50,960
How do I do it?
 

2907
01:04:50,960 --> 01:04:52,710
How do I do it?
So you can think of it as kind of one

2908
01:04:52,710 --> 01:04:52,720
So you can think of it as kind of one
 

2909
01:04:52,720 --> 01:04:53,750
So you can think of it as kind of one
way think about it is a kind of a

2910
01:04:53,750 --> 01:04:53,760
way think about it is a kind of a
 

2911
01:04:53,760 --> 01:04:56,390
way think about it is a kind of a
hierarchical perspective. It says I have

2912
01:04:56,390 --> 01:04:56,400
hierarchical perspective. It says I have
 

2913
01:04:56,400 --> 01:04:57,910
hierarchical perspective. It says I have
a

2914
01:04:57,910 --> 01:04:57,920
a
 

2915
01:04:57,920 --> 01:04:59,829
a
foundation model which is good at lots

2916
01:04:59,829 --> 01:04:59,839
foundation model which is good at lots
 

2917
01:04:59,839 --> 01:05:02,710
foundation model which is good at lots
and lots of different things. Now I have

2918
01:05:02,710 --> 01:05:02,720
and lots of different things. Now I have
 

2919
01:05:02,720 --> 01:05:06,470
and lots of different things. Now I have
some family subf family of tasks.

2920
01:05:06,470 --> 01:05:06,480
some family subf family of tasks.
 

2921
01:05:06,480 --> 01:05:08,549
some family subf family of tasks.
I could tune the foundation model for

2922
01:05:08,549 --> 01:05:08,559
I could tune the foundation model for
 

2923
01:05:08,559 --> 01:05:10,950
I could tune the foundation model for
this every individual task in this subf

2924
01:05:10,950 --> 01:05:10,960
this every individual task in this subf
 

2925
01:05:10,960 --> 01:05:13,430
this every individual task in this subf
family. But what if I wanted to make a

2926
01:05:13,430 --> 01:05:13,440
family. But what if I wanted to make a
 

2927
01:05:13,440 --> 01:05:16,390
family. But what if I wanted to make a
model that was tuned for this entire

2928
01:05:16,390 --> 01:05:16,400
model that was tuned for this entire
 

2929
01:05:16,400 --> 01:05:18,870
model that was tuned for this entire
subf family such that I could take that

2930
01:05:18,870 --> 01:05:18,880
subf family such that I could take that
 

2931
01:05:18,880 --> 01:05:20,470
subf family such that I could take that
model and then tune it for the

2932
01:05:20,470 --> 01:05:20,480
model and then tune it for the
 

2933
01:05:20,480 --> 01:05:22,150
model and then tune it for the
individual tasks. Can I get some kind of

2934
01:05:22,150 --> 01:05:22,160
individual tasks. Can I get some kind of
 

2935
01:05:22,160 --> 01:05:25,510
individual tasks. Can I get some kind of
benefit from having all of this task as

2936
01:05:25,510 --> 01:05:25,520
benefit from having all of this task as
 

2937
01:05:25,520 --> 01:05:27,750
benefit from having all of this task as
a collection?

2938
01:05:27,750 --> 01:05:27,760
a collection?
 

2939
01:05:27,760 --> 01:05:31,670
a collection?
Okay, that's the metalarning problem.

2940
01:05:31,670 --> 01:05:31,680
Okay, that's the metalarning problem.
 

2941
01:05:31,680 --> 01:05:32,789
Okay, that's the metalarning problem.
So, everyone understand what we're

2942
01:05:32,789 --> 01:05:32,799
So, everyone understand what we're
 

2943
01:05:32,799 --> 01:05:34,549
So, everyone understand what we're
trying to do? Trying to make a model

2944
01:05:34,549 --> 01:05:34,559
trying to do? Trying to make a model
 

2945
01:05:34,559 --> 01:05:36,950
trying to do? Trying to make a model
that's better at being fine-tuned.

2946
01:05:36,950 --> 01:05:36,960
that's better at being fine-tuned.
 

2947
01:05:36,960 --> 01:05:39,829
that's better at being fine-tuned.
So, the question the first question that

2948
01:05:39,829 --> 01:05:39,839
So, the question the first question that
 

2949
01:05:39,839 --> 01:05:42,710
So, the question the first question that
you should have is what's a good

2950
01:05:42,710 --> 01:05:42,720
you should have is what's a good
 

2951
01:05:42,720 --> 01:05:45,109
you should have is what's a good
baseline approach like what is the right

2952
01:05:45,109 --> 01:05:45,119
baseline approach like what is the right
 

2953
01:05:45,119 --> 01:05:47,190
baseline approach like what is the right
kind of starting point for doing this?

2954
01:05:47,190 --> 01:05:47,200
kind of starting point for doing this?
 

2955
01:05:47,200 --> 01:05:50,230
kind of starting point for doing this?
So there's one baseline which is just

2956
01:05:50,230 --> 01:05:50,240
So there's one baseline which is just
 

2957
01:05:50,240 --> 01:05:55,910
So there's one baseline which is just
easy which is the zero baseline which is

2958
01:05:55,910 --> 01:05:55,920
easy which is the zero baseline which is
 

2959
01:05:55,920 --> 01:06:04,309
easy which is the zero baseline which is
nothing do nothing

2960
01:06:04,309 --> 01:06:04,319

 

2961
01:06:04,319 --> 01:06:09,750

random initialization.

2962
01:06:09,750 --> 01:06:09,760

 

2963
01:06:09,760 --> 01:06:12,950

Okay. So this is whatever your task is

2964
01:06:12,950 --> 01:06:12,960
Okay. So this is whatever your task is
 

2965
01:06:12,960 --> 01:06:14,309
Okay. So this is whatever your task is
just start with random initialization

2966
01:06:14,309 --> 01:06:14,319
just start with random initialization
 

2967
01:06:14,319 --> 01:06:17,109
just start with random initialization
and train a model for it. Okay, so

2968
01:06:17,109 --> 01:06:17,119
and train a model for it. Okay, so
 

2969
01:06:17,119 --> 01:06:20,390
and train a model for it. Okay, so
that's like one. Um,

2970
01:06:20,390 --> 01:06:20,400
that's like one. Um,
 

2971
01:06:20,400 --> 01:06:25,510
that's like one. Um,
another baseline approach is

2972
01:06:25,510 --> 01:06:25,520

 

2973
01:06:25,520 --> 01:06:32,870

general foundation model.

2974
01:06:32,870 --> 01:06:32,880

 

2975
01:06:32,880 --> 01:06:36,710

So just have very very high level

2976
01:06:36,710 --> 01:06:36,720
So just have very very high level
 

2977
01:06:36,720 --> 01:06:38,390
So just have very very high level
foundation model that encompasses the

2978
01:06:38,390 --> 01:06:38,400
foundation model that encompasses the
 

2979
01:06:38,400 --> 01:06:40,309
foundation model that encompasses the
family of tasks you're interested in.

2980
01:06:40,309 --> 01:06:40,319
family of tasks you're interested in.
 

2981
01:06:40,319 --> 01:06:43,670
family of tasks you're interested in.
Use that. Don't do anything special.

2982
01:06:43,670 --> 01:06:43,680
Use that. Don't do anything special.
 

2983
01:06:43,680 --> 01:06:46,549
Use that. Don't do anything special.
So these are like very very basic

2984
01:06:46,549 --> 01:06:46,559
So these are like very very basic
 

2985
01:06:46,559 --> 01:06:48,549
So these are like very very basic
baselines. So now let's do the next one

2986
01:06:48,549 --> 01:06:48,559
baselines. So now let's do the next one
 

2987
01:06:48,559 --> 01:06:52,069
baselines. So now let's do the next one
is what I want to know. So what comes

2988
01:06:52,069 --> 01:06:52,079
is what I want to know. So what comes
 

2989
01:06:52,079 --> 01:06:58,150
is what I want to know. So what comes
next?

2990
01:06:58,150 --> 01:06:58,160

 

2991
01:06:58,160 --> 01:07:00,950

So the approach I'm going to talk about

2992
01:07:00,950 --> 01:07:00,960
So the approach I'm going to talk about
 

2993
01:07:00,960 --> 01:07:03,829
So the approach I'm going to talk about
I consider to be a very very interesting

2994
01:07:03,829 --> 01:07:03,839
I consider to be a very very interesting
 

2995
01:07:03,839 --> 01:07:06,150
I consider to be a very very interesting
conceptual development. I'll preface

2996
01:07:06,150 --> 01:07:06,160
conceptual development. I'll preface
 

2997
01:07:06,160 --> 01:07:08,710
conceptual development. I'll preface
everything by saying that practically

2998
01:07:08,710 --> 01:07:08,720
everything by saying that practically
 

2999
01:07:08,720 --> 01:07:10,470
everything by saying that practically
speaking,

3000
01:07:10,470 --> 01:07:10,480
speaking,
 

3001
01:07:10,480 --> 01:07:11,910
speaking,
this particular approach I'm going to

3002
01:07:11,910 --> 01:07:11,920
this particular approach I'm going to
 

3003
01:07:11,920 --> 01:07:15,029
this particular approach I'm going to
describe is not done that much anymore,

3004
01:07:15,029 --> 01:07:15,039
describe is not done that much anymore,
 

3005
01:07:15,039 --> 01:07:17,510
describe is not done that much anymore,
but it's still very useful conceptually.

3006
01:07:17,510 --> 01:07:17,520
but it's still very useful conceptually.
 

3007
01:07:17,520 --> 01:07:19,510
but it's still very useful conceptually.
Um, so I'm going to talk about it for

3008
01:07:19,510 --> 01:07:19,520
Um, so I'm going to talk about it for
 

3009
01:07:19,520 --> 01:07:23,910
Um, so I'm going to talk about it for
that reason. So this approach is we can

3010
01:07:23,910 --> 01:07:23,920
that reason. So this approach is we can
 

3011
01:07:23,920 --> 01:07:30,789
that reason. So this approach is we can
call it has a name. It's called mammal.

3012
01:07:30,789 --> 01:07:30,799

 

3013
01:07:30,799 --> 01:07:33,910

And mammal is useful conceptually

3014
01:07:33,910 --> 01:07:33,920
And mammal is useful conceptually
 

3015
01:07:33,920 --> 01:07:37,029
And mammal is useful conceptually
because

3016
01:07:37,029 --> 01:07:37,039

 

3017
01:07:37,039 --> 01:07:40,309

before mammal came out people were

3018
01:07:40,309 --> 01:07:40,319
before mammal came out people were
 

3019
01:07:40,319 --> 01:07:42,549
before mammal came out people were
already trying to solve this metalarning

3020
01:07:42,549 --> 01:07:42,559
already trying to solve this metalarning
 

3021
01:07:42,559 --> 01:07:47,829
already trying to solve this metalarning
problem. Okay. Um maml came out before

3022
01:07:47,829 --> 01:07:47,839
problem. Okay. Um maml came out before
 

3023
01:07:47,839 --> 01:07:50,309
problem. Okay. Um maml came out before
uh foundation models were available for

3024
01:07:50,309 --> 01:07:50,319
uh foundation models were available for
 

3025
01:07:50,319 --> 01:07:52,710
uh foundation models were available for
non-image tasks.

3026
01:07:52,710 --> 01:07:52,720
non-image tasks.
 

3027
01:07:52,720 --> 01:07:55,109
non-image tasks.
Okay. So there was imageet but for other

3028
01:07:55,109 --> 01:07:55,119
Okay. So there was imageet but for other
 

3029
01:07:55,119 --> 01:07:56,870
Okay. So there was imageet but for other
kinds of problems like robotics and so

3030
01:07:56,870 --> 01:07:56,880
kinds of problems like robotics and so
 

3031
01:07:56,880 --> 01:07:58,309
kinds of problems like robotics and so
on there was nothing even remotely

3032
01:07:58,309 --> 01:07:58,319
on there was nothing even remotely
 

3033
01:07:58,319 --> 01:08:00,069
on there was nothing even remotely
approaching a foundation model. nor was

3034
01:08:00,069 --> 01:08:00,079
approaching a foundation model. nor was
 

3035
01:08:00,079 --> 01:08:01,270
approaching a foundation model. nor was
there anything approaching a foundation

3036
01:08:01,270 --> 01:08:01,280
there anything approaching a foundation
 

3037
01:08:01,280 --> 01:08:03,190
there anything approaching a foundation
model for language tasks when mammal was

3038
01:08:03,190 --> 01:08:03,200
model for language tasks when mammal was
 

3039
01:08:03,200 --> 01:08:06,150
model for language tasks when mammal was
developed. Um the main author of mammals

3040
01:08:06,150 --> 01:08:06,160
developed. Um the main author of mammals
 

3041
01:08:06,160 --> 01:08:07,910
developed. Um the main author of mammals
is Chelsea Finn who was a graduate

3042
01:08:07,910 --> 01:08:07,920
is Chelsea Finn who was a graduate
 

3043
01:08:07,920 --> 01:08:09,510
is Chelsea Finn who was a graduate
student here a professor now at

3044
01:08:09,510 --> 01:08:09,520
student here a professor now at
 

3045
01:08:09,520 --> 01:08:12,470
student here a professor now at
Stanford. So

3046
01:08:12,470 --> 01:08:12,480
Stanford. So
 

3047
01:08:12,480 --> 01:08:14,309
Stanford. So
what people are already trying to solve

3048
01:08:14,309 --> 01:08:14,319
what people are already trying to solve
 

3049
01:08:14,319 --> 01:08:17,030
what people are already trying to solve
this problem and the problem was viewed

3050
01:08:17,030 --> 01:08:17,040
this problem and the problem was viewed
 

3051
01:08:17,040 --> 01:08:19,910
this problem and the problem was viewed
as I have a collection of tasks and for

3052
01:08:19,910 --> 01:08:19,920
as I have a collection of tasks and for
 

3053
01:08:19,920 --> 01:08:21,910
as I have a collection of tasks and for
every everything in this ta in this

3054
01:08:21,910 --> 01:08:21,920
every everything in this ta in this
 

3055
01:08:21,920 --> 01:08:23,269
every everything in this ta in this
collection of tasks I would like to be

3056
01:08:23,269 --> 01:08:23,279
collection of tasks I would like to be
 

3057
01:08:23,279 --> 01:08:24,309
collection of tasks I would like to be
able to fine-tune it for this

3058
01:08:24,309 --> 01:08:24,319
able to fine-tune it for this
 

3059
01:08:24,319 --> 01:08:27,590
able to fine-tune it for this
collection. Can I do it?

3060
01:08:27,590 --> 01:08:27,600
collection. Can I do it?
 

3061
01:08:27,600 --> 01:08:30,149
collection. Can I do it?
And the mammal approach you can think of

3062
01:08:30,149 --> 01:08:30,159
And the mammal approach you can think of
 

3063
01:08:30,159 --> 01:08:33,749
And the mammal approach you can think of
as being a systematic way of reasoning

3064
01:08:33,749 --> 01:08:33,759
as being a systematic way of reasoning
 

3065
01:08:33,759 --> 01:08:36,709
as being a systematic way of reasoning
about what you might do in this setting

3066
01:08:36,709 --> 01:08:36,719
about what you might do in this setting
 

3067
01:08:36,719 --> 01:08:38,789
about what you might do in this setting
and just translating that into something

3068
01:08:38,789 --> 01:08:38,799
and just translating that into something
 

3069
01:08:38,799 --> 01:08:42,070
and just translating that into something
operational. And so it's useful for that

3070
01:08:42,070 --> 01:08:42,080
operational. And so it's useful for that
 

3071
01:08:42,080 --> 01:08:45,030
operational. And so it's useful for that
um that purpose for that reason.

3072
01:08:45,030 --> 01:08:45,040
um that purpose for that reason.
 

3073
01:08:45,040 --> 01:08:47,749
um that purpose for that reason.
So before you even think about what an

3074
01:08:47,749 --> 01:08:47,759
So before you even think about what an
 

3075
01:08:47,759 --> 01:08:49,829
So before you even think about what an
approach might be that is different than

3076
01:08:49,829 --> 01:08:49,839
approach might be that is different than
 

3077
01:08:49,839 --> 01:08:52,550
approach might be that is different than
doing nothing,

3078
01:08:52,550 --> 01:08:52,560
doing nothing,
 

3079
01:08:52,560 --> 01:08:55,430
doing nothing,
you have to ask yourself the question,

3080
01:08:55,430 --> 01:08:55,440
you have to ask yourself the question,
 

3081
01:08:55,440 --> 01:08:56,950
you have to ask yourself the question,
if I'm trying to do something, I'm

3082
01:08:56,950 --> 01:08:56,960
if I'm trying to do something, I'm
 

3083
01:08:56,960 --> 01:08:58,550
if I'm trying to do something, I'm
trying to make a model better at being

3084
01:08:58,550 --> 01:08:58,560
trying to make a model better at being
 

3085
01:08:58,560 --> 01:09:01,349
trying to make a model better at being
fine-tuned for tasks, what do I have to

3086
01:09:01,349 --> 01:09:01,359
fine-tuned for tasks, what do I have to
 

3087
01:09:01,359 --> 01:09:04,070
fine-tuned for tasks, what do I have to
work with? First question, right, before

3088
01:09:04,070 --> 01:09:04,080
work with? First question, right, before
 

3089
01:09:04,080 --> 01:09:05,910
work with? First question, right, before
you want it, like I want to make a tasty

3090
01:09:05,910 --> 01:09:05,920
you want it, like I want to make a tasty
 

3091
01:09:05,920 --> 01:09:08,550
you want it, like I want to make a tasty
meal. Okay, great. What ingredients do I

3092
01:09:08,550 --> 01:09:08,560
meal. Okay, great. What ingredients do I
 

3093
01:09:08,560 --> 01:09:12,070
meal. Okay, great. What ingredients do I
have? Right? It's first question.

3094
01:09:12,070 --> 01:09:12,080
have? Right? It's first question.
 

3095
01:09:12,080 --> 01:09:25,590
have? Right? It's first question.
So what do you need?

3096
01:09:25,590 --> 01:09:25,600

 

3097
01:09:25,600 --> 01:09:27,110

So if I want to make a model better at

3098
01:09:27,110 --> 01:09:27,120
So if I want to make a model better at
 

3099
01:09:27,120 --> 01:09:29,110
So if I want to make a model better at
being fine-tuned for tasks from a

3100
01:09:29,110 --> 01:09:29,120
being fine-tuned for tasks from a
 

3101
01:09:29,120 --> 01:09:31,269
being fine-tuned for tasks from a
certain family, I should have some

3102
01:09:31,269 --> 01:09:31,279
certain family, I should have some
 

3103
01:09:31,279 --> 01:09:33,590
certain family, I should have some
examples of tasks from a certain family,

3104
01:09:33,590 --> 01:09:33,600
examples of tasks from a certain family,
 

3105
01:09:33,600 --> 01:09:36,630
examples of tasks from a certain family,
right? Does everyone see that? I'm doing

3106
01:09:36,630 --> 01:09:36,640
right? Does everyone see that? I'm doing
 

3107
01:09:36,640 --> 01:09:38,709
right? Does everyone see that? I'm doing
machine learning. I want to be good at

3108
01:09:38,709 --> 01:09:38,719
machine learning. I want to be good at
 

3109
01:09:38,719 --> 01:09:40,870
machine learning. I want to be good at
something. I have to have examples of

3110
01:09:40,870 --> 01:09:40,880
something. I have to have examples of
 

3111
01:09:40,880 --> 01:09:43,669
something. I have to have examples of
it. Everyone with me is like a very

3112
01:09:43,669 --> 01:09:43,679
it. Everyone with me is like a very
 

3113
01:09:43,679 --> 01:09:46,550
it. Everyone with me is like a very
important highle point surprisingly not

3114
01:09:46,550 --> 01:09:46,560
important highle point surprisingly not
 

3115
01:09:46,560 --> 01:09:51,349
important highle point surprisingly not
fully appreciated before um this work.

3116
01:09:51,349 --> 01:09:51,359
fully appreciated before um this work.
 

3117
01:09:51,359 --> 01:10:09,189
fully appreciated before um this work.
basic ingredient.

3118
01:10:09,189 --> 01:10:09,199

 

3119
01:10:09,199 --> 01:10:10,630

So now the question is what does it mean

3120
01:10:10,630 --> 01:10:10,640
So now the question is what does it mean
 

3121
01:10:10,640 --> 01:10:13,270
So now the question is what does it mean
to have a collection of tasks in a

3122
01:10:13,270 --> 01:10:13,280
to have a collection of tasks in a
 

3123
01:10:13,280 --> 01:10:15,030
to have a collection of tasks in a
machine learning setting? A collection

3124
01:10:15,030 --> 01:10:15,040
machine learning setting? A collection
 

3125
01:10:15,040 --> 01:10:18,470
machine learning setting? A collection
of tasks means data.

3126
01:10:18,470 --> 01:10:18,480
of tasks means data.
 

3127
01:10:18,480 --> 01:10:20,070
of tasks means data.
Okay.

3128
01:10:20,070 --> 01:10:20,080
Okay.
 

3129
01:10:20,080 --> 01:10:27,750
Okay.
So this means i.e.

3130
01:10:27,750 --> 01:10:27,760

 

3131
01:10:27,760 --> 01:10:35,350

training data

3132
01:10:35,350 --> 01:10:35,360

 

3133
01:10:35,360 --> 01:10:42,870

these different tasks.

3134
01:10:42,870 --> 01:10:42,880

 

3135
01:10:42,880 --> 01:10:44,870

So for a machine learning problem what

3136
01:10:44,870 --> 01:10:44,880
So for a machine learning problem what
 

3137
01:10:44,880 --> 01:10:49,110
So for a machine learning problem what
do you need? Is training data enough?

3138
01:10:49,110 --> 01:10:49,120
do you need? Is training data enough?
 

3139
01:10:49,120 --> 01:10:50,070
do you need? Is training data enough?
What else do you need? What's the

3140
01:10:50,070 --> 01:10:50,080
What else do you need? What's the
 

3141
01:10:50,080 --> 01:10:51,270
What else do you need? What's the
minimum requirement for a machine

3142
01:10:51,270 --> 01:10:51,280
minimum requirement for a machine
 

3143
01:10:51,280 --> 01:10:52,950
minimum requirement for a machine
learning problem? Here's some training

3144
01:10:52,950 --> 01:10:52,960
learning problem? Here's some training
 

3145
01:10:52,960 --> 01:10:54,229
learning problem? Here's some training
data.

3146
01:10:54,229 --> 01:10:54,239
data.
 

3147
01:10:54,239 --> 01:10:55,270
data.
>> Test data.

3148
01:10:55,270 --> 01:10:55,280
>> Test data.
 

3149
01:10:55,280 --> 01:10:56,709
>> Test data.
>> Test data. I can always make test data

3150
01:10:56,709 --> 01:10:56,719
>> Test data. I can always make test data
 

3151
01:10:56,719 --> 01:10:58,470
>> Test data. I can always make test data
from my training data. Like I can split

3152
01:10:58,470 --> 01:10:58,480
from my training data. Like I can split
 

3153
01:10:58,480 --> 01:11:00,310
from my training data. Like I can split
it off.

3154
01:11:00,310 --> 01:11:00,320
it off.
 

3155
01:11:00,320 --> 01:11:02,470
it off.
But you need something more than data.

3156
01:11:02,470 --> 01:11:02,480
But you need something more than data.
 

3157
01:11:02,480 --> 01:11:04,229
But you need something more than data.
>> A model.

3158
01:11:04,229 --> 01:11:04,239
>> A model.
 

3159
01:11:04,239 --> 01:11:06,470
>> A model.
A model is what I want to make. So I I

3160
01:11:06,470 --> 01:11:06,480
A model is what I want to make. So I I
 

3161
01:11:06,480 --> 01:11:08,229
A model is what I want to make. So I I
have a model architecture. Fine. But

3162
01:11:08,229 --> 01:11:08,239
have a model architecture. Fine. But
 

3163
01:11:08,239 --> 01:11:09,510
have a model architecture. Fine. But
that's what I want to make. My goal is

3164
01:11:09,510 --> 01:11:09,520
that's what I want to make. My goal is
 

3165
01:11:09,520 --> 01:11:10,950
that's what I want to make. My goal is
to have making a model that's better at

3166
01:11:10,950 --> 01:11:10,960
to have making a model that's better at
 

3167
01:11:10,960 --> 01:11:13,350
to have making a model that's better at
being fine-tuned. That's my objective.

3168
01:11:13,350 --> 01:11:13,360
being fine-tuned. That's my objective.
 

3169
01:11:13,360 --> 01:11:16,950
being fine-tuned. That's my objective.
What else do I need?

3170
01:11:16,950 --> 01:11:16,960
What else do I need?
 

3171
01:11:16,960 --> 01:11:19,510
What else do I need?
An optimizer is how I train it. But my

3172
01:11:19,510 --> 01:11:19,520
An optimizer is how I train it. But my
 

3173
01:11:19,520 --> 01:11:21,510
An optimizer is how I train it. But my
optimizer requires something to be able

3174
01:11:21,510 --> 01:11:21,520
optimizer requires something to be able
 

3175
01:11:21,520 --> 01:11:23,110
optimizer requires something to be able
to work.

3176
01:11:23,110 --> 01:11:23,120
to work.
 

3177
01:11:23,120 --> 01:11:25,590
to work.
>> I need labels. Yes. But I don't just

3178
01:11:25,590 --> 01:11:25,600
>> I need labels. Yes. But I don't just
 

3179
01:11:25,600 --> 01:11:28,550
>> I need labels. Yes. But I don't just
need labels. I need a loss that goes

3180
01:11:28,550 --> 01:11:28,560
need labels. I need a loss that goes
 

3181
01:11:28,560 --> 01:11:32,550
need labels. I need a loss that goes
with those labels. Right?

3182
01:11:32,550 --> 01:11:32,560
with those labels. Right?
 

3183
01:11:32,560 --> 01:11:35,030
with those labels. Right?
Plus loss

3184
01:11:35,030 --> 01:11:35,040
Plus loss
 

3185
01:11:35,040 --> 01:11:38,470
Plus loss
some loss function.

3186
01:11:38,470 --> 01:11:38,480
some loss function.
 

3187
01:11:38,480 --> 01:11:40,310
some loss function.
So my loss function can be different for

3188
01:11:40,310 --> 01:11:40,320
So my loss function can be different for
 

3189
01:11:40,320 --> 01:11:42,149
So my loss function can be different for
different tasks.

3190
01:11:42,149 --> 01:11:42,159
different tasks.
 

3191
01:11:42,159 --> 01:11:43,590
different tasks.
My training data can be different for

3192
01:11:43,590 --> 01:11:43,600
My training data can be different for
 

3193
01:11:43,600 --> 01:11:45,430
My training data can be different for
different tasks. But I need it to

3194
01:11:45,430 --> 01:11:45,440
different tasks. But I need it to
 

3195
01:11:45,440 --> 01:11:48,229
different tasks. But I need it to
understand what the task is.

3196
01:11:48,229 --> 01:11:48,239
understand what the task is.
 

3197
01:11:48,239 --> 01:11:51,270
understand what the task is.
Okay. So oftentimes the loss might be

3198
01:11:51,270 --> 01:11:51,280
Okay. So oftentimes the loss might be
 

3199
01:11:51,280 --> 01:11:52,950
Okay. So oftentimes the loss might be
I'll use cross entropy loss or I'll use

3200
01:11:52,950 --> 01:11:52,960
I'll use cross entropy loss or I'll use
 

3201
01:11:52,960 --> 01:11:54,870
I'll use cross entropy loss or I'll use
like squared loss or I might have some

3202
01:11:54,870 --> 01:11:54,880
like squared loss or I might have some
 

3203
01:11:54,880 --> 01:11:56,390
like squared loss or I might have some
waiting on losses. Whatever it is for

3204
01:11:56,390 --> 01:11:56,400
waiting on losses. Whatever it is for
 

3205
01:11:56,400 --> 01:11:58,070
waiting on losses. Whatever it is for
your task, it is what it is. You have to

3206
01:11:58,070 --> 01:11:58,080
your task, it is what it is. You have to
 

3207
01:11:58,080 --> 01:12:00,950
your task, it is what it is. You have to
have it though.

3208
01:12:00,950 --> 01:12:00,960
have it though.
 

3209
01:12:00,960 --> 01:12:03,110
have it though.
Okay. So I need to have a collection of

3210
01:12:03,110 --> 01:12:03,120
Okay. So I need to have a collection of
 

3211
01:12:03,120 --> 01:12:05,590
Okay. So I need to have a collection of
tasks from the family which includes um

3212
01:12:05,590 --> 01:12:05,600
tasks from the family which includes um
 

3213
01:12:05,600 --> 01:12:08,950
tasks from the family which includes um
a loss function. So

3214
01:12:08,950 --> 01:12:08,960
a loss function. So
 

3215
01:12:08,960 --> 01:12:11,189
a loss function. So
if I want to make a model that's good at

3216
01:12:11,189 --> 01:12:11,199
if I want to make a model that's good at
 

3217
01:12:11,199 --> 01:12:13,750
if I want to make a model that's good at
doing something,

3218
01:12:13,750 --> 01:12:13,760
doing something,
 

3219
01:12:13,760 --> 01:12:15,430
doing something,
I also need to know what that something

3220
01:12:15,430 --> 01:12:15,440
I also need to know what that something
 

3221
01:12:15,440 --> 01:12:17,430
I also need to know what that something
is.

3222
01:12:17,430 --> 01:12:17,440
is.
 

3223
01:12:17,440 --> 01:12:18,950
is.
Okay.

3224
01:12:18,950 --> 01:12:18,960
Okay.
 

3225
01:12:18,960 --> 01:12:22,070
Okay.
So the other thing

3226
01:12:22,070 --> 01:12:22,080
So the other thing
 

3227
01:12:22,080 --> 01:12:28,950
So the other thing
is I need to have an approach

3228
01:12:28,950 --> 01:12:28,960

 

3229
01:12:28,960 --> 01:12:34,790

to fine-tuning.

3230
01:12:34,790 --> 01:12:34,800

 

3231
01:12:34,800 --> 01:12:36,470

So I if I want to make a model better at

3232
01:12:36,470 --> 01:12:36,480
So I if I want to make a model better at
 

3233
01:12:36,480 --> 01:12:38,870
So I if I want to make a model better at
being fine-tuned for tasks, I need to

3234
01:12:38,870 --> 01:12:38,880
being fine-tuned for tasks, I need to
 

3235
01:12:38,880 --> 01:12:40,630
being fine-tuned for tasks, I need to
say, well, how am I going to fine-tune

3236
01:12:40,630 --> 01:12:40,640
say, well, how am I going to fine-tune
 

3237
01:12:40,640 --> 01:12:43,669
say, well, how am I going to fine-tune
it? Make a model that's better at blah.

3238
01:12:43,669 --> 01:12:43,679
it? Make a model that's better at blah.
 

3239
01:12:43,679 --> 01:12:47,270
it? Make a model that's better at blah.
Well, how am I going to do blah?

3240
01:12:47,270 --> 01:12:47,280
Well, how am I going to do blah?
 

3241
01:12:47,280 --> 01:12:49,910
Well, how am I going to do blah?
So this could we could just put in the

3242
01:12:49,910 --> 01:12:49,920
So this could we could just put in the
 

3243
01:12:49,920 --> 01:12:51,669
So this could we could just put in the
standard approach. It's totally fine to

3244
01:12:51,669 --> 01:12:51,679
standard approach. It's totally fine to
 

3245
01:12:51,679 --> 01:12:53,030
standard approach. It's totally fine to
put in the standard approach. We have to

3246
01:12:53,030 --> 01:12:53,040
put in the standard approach. We have to
 

3247
01:12:53,040 --> 01:13:02,070
put in the standard approach. We have to
have it. So example

3248
01:13:02,070 --> 01:13:02,080

 

3249
01:13:02,080 --> 01:13:07,510

use Allora

3250
01:13:07,510 --> 01:13:07,520

 

3251
01:13:07,520 --> 01:13:11,750

and

3252
01:13:11,750 --> 01:13:11,760

 

3253
01:13:11,760 --> 01:13:14,709

the SGD

3254
01:13:14,709 --> 01:13:14,719
the SGD
 

3255
01:13:14,719 --> 01:13:19,350
the SGD
optimizer

3256
01:13:19,350 --> 01:13:19,360

 

3257
01:13:19,360 --> 01:13:27,189

on training data.

3258
01:13:27,189 --> 01:13:27,199

 

3259
01:13:27,199 --> 01:13:30,149

Okay. So if you want to solve a problem

3260
01:13:30,149 --> 01:13:30,159
Okay. So if you want to solve a problem
 

3261
01:13:30,159 --> 01:13:33,030
Okay. So if you want to solve a problem
with machine learning

3262
01:13:33,030 --> 01:13:33,040
with machine learning
 

3263
01:13:33,040 --> 01:13:34,310
with machine learning
to even begin I have to have a

3264
01:13:34,310 --> 01:13:34,320
to even begin I have to have a
 

3265
01:13:34,320 --> 01:13:35,830
to even begin I have to have a
collection of tasks. I need to have an

3266
01:13:35,830 --> 01:13:35,840
collection of tasks. I need to have an
 

3267
01:13:35,840 --> 01:13:37,669
collection of tasks. I need to have an
approach. I want to be better at X.

3268
01:13:37,669 --> 01:13:37,679
approach. I want to be better at X.
 

3269
01:13:37,679 --> 01:13:40,550
approach. I want to be better at X.
Okay. How am I going to do X? I'm going

3270
01:13:40,550 --> 01:13:40,560
Okay. How am I going to do X? I'm going
 

3271
01:13:40,560 --> 01:13:43,030
Okay. How am I going to do X? I'm going
to do this. This is the counterpart of

3272
01:13:43,030 --> 01:13:43,040
to do this. This is the counterpart of
 

3273
01:13:43,040 --> 01:13:46,470
to do this. This is the counterpart of
what a model architecture is. Okay. And

3274
01:13:46,470 --> 01:13:46,480
what a model architecture is. Okay. And
 

3275
01:13:46,480 --> 01:13:48,149
what a model architecture is. Okay. And
an optimizer.

3276
01:13:48,149 --> 01:13:48,159
an optimizer.
 

3277
01:13:48,159 --> 01:13:51,030
an optimizer.
I also need to know a have a way of

3278
01:13:51,030 --> 01:13:51,040
I also need to know a have a way of
 

3279
01:13:51,040 --> 01:13:54,709
I also need to know a have a way of
knowing if I'm any good at it,

3280
01:13:54,709 --> 01:13:54,719
knowing if I'm any good at it,
 

3281
01:13:54,719 --> 01:13:56,470
knowing if I'm any good at it,
right? Because I would like to train a

3282
01:13:56,470 --> 01:13:56,480
right? Because I would like to train a
 

3283
01:13:56,480 --> 01:13:59,590
right? Because I would like to train a
model so it's good at blah. I have to be

3284
01:13:59,590 --> 01:13:59,600
model so it's good at blah. I have to be
 

3285
01:13:59,600 --> 01:14:02,070
model so it's good at blah. I have to be
able to say am I any good at blah or

3286
01:14:02,070 --> 01:14:02,080
able to say am I any good at blah or
 

3287
01:14:02,080 --> 01:14:04,390
able to say am I any good at blah or
not? I need to have a way of doing that.

3288
01:14:04,390 --> 01:14:04,400
not? I need to have a way of doing that.
 

3289
01:14:04,400 --> 01:14:11,510
not? I need to have a way of doing that.
So I need an approach

3290
01:14:11,510 --> 01:14:11,520

 

3291
01:14:11,520 --> 01:14:15,030

to evaluation

3292
01:14:15,030 --> 01:14:15,040

 

3293
01:14:15,040 --> 01:14:16,470

and again there's this there's a default

3294
01:14:16,470 --> 01:14:16,480
and again there's this there's a default
 

3295
01:14:16,480 --> 01:14:18,550
and again there's this there's a default
but you have to make it explicit. What's

3296
01:14:18,550 --> 01:14:18,560
but you have to make it explicit. What's
 

3297
01:14:18,560 --> 01:14:22,229
but you have to make it explicit. What's
the default? This default is

3298
01:14:22,229 --> 01:14:22,239
the default? This default is
 

3299
01:14:22,239 --> 01:14:27,669
the default? This default is
eval performance

3300
01:14:27,669 --> 01:14:27,679

 

3301
01:14:27,679 --> 01:14:35,030

unheld outset.

3302
01:14:35,030 --> 01:14:35,040

 

3303
01:14:35,040 --> 01:14:37,669

using a loss.

3304
01:14:37,669 --> 01:14:37,679
using a loss.
 

3305
01:14:37,679 --> 01:14:39,270
using a loss.
Okay,

3306
01:14:39,270 --> 01:14:39,280
Okay,
 

3307
01:14:39,280 --> 01:14:41,830
Okay,
some some loss might be a different

3308
01:14:41,830 --> 01:14:41,840
some some loss might be a different
 

3309
01:14:41,840 --> 01:14:45,430
some some loss might be a different
loss. Okay, but it is what we have.

3310
01:14:45,430 --> 01:14:45,440
loss. Okay, but it is what we have.
 

3311
01:14:45,440 --> 01:14:50,229
loss. Okay, but it is what we have.
Okay, mamml said this is the basic thing

3312
01:14:50,229 --> 01:14:50,239
Okay, mamml said this is the basic thing
 

3313
01:14:50,239 --> 01:14:53,750
Okay, mamml said this is the basic thing
I need to even begin to do this problem.

3314
01:14:53,750 --> 01:14:53,760
I need to even begin to do this problem.
 

3315
01:14:53,760 --> 01:14:55,030
I need to even begin to do this problem.
If I want to make it be good at

3316
01:14:55,030 --> 01:14:55,040
If I want to make it be good at
 

3317
01:14:55,040 --> 01:14:56,709
If I want to make it be good at
fine-tuning for a certain task, I have

3318
01:14:56,709 --> 01:14:56,719
fine-tuning for a certain task, I have
 

3319
01:14:56,719 --> 01:14:59,590
fine-tuning for a certain task, I have
to say what do I mean by fine-tuning?

3320
01:14:59,590 --> 01:14:59,600
to say what do I mean by fine-tuning?
 

3321
01:14:59,600 --> 01:15:01,830
to say what do I mean by fine-tuning?
Okay, I'm going to use the std optimizer

3322
01:15:01,830 --> 01:15:01,840
Okay, I'm going to use the std optimizer
 

3323
01:15:01,840 --> 01:15:04,470
Okay, I'm going to use the std optimizer
and a Laura on a certain model as an

3324
01:15:04,470 --> 01:15:04,480
and a Laura on a certain model as an
 

3325
01:15:04,480 --> 01:15:06,229
and a Laura on a certain model as an
example. It could be anything. I just

3326
01:15:06,229 --> 01:15:06,239
example. It could be anything. I just
 

3327
01:15:06,239 --> 01:15:08,470
example. It could be anything. I just
have a models from scratch full fine

3328
01:15:08,470 --> 01:15:08,480
have a models from scratch full fine
 

3329
01:15:08,480 --> 01:15:09,590
have a models from scratch full fine
tune. Doesn't matter. It doesn't have to

3330
01:15:09,590 --> 01:15:09,600
tune. Doesn't matter. It doesn't have to
 

3331
01:15:09,600 --> 01:15:11,030
tune. Doesn't matter. It doesn't have to
be a can be full fine tune. Has to be

3332
01:15:11,030 --> 01:15:11,040
be a can be full fine tune. Has to be
 

3333
01:15:11,040 --> 01:15:13,830
be a can be full fine tune. Has to be
something that involves gradient steps.

3334
01:15:13,830 --> 01:15:13,840
something that involves gradient steps.
 

3335
01:15:13,840 --> 01:15:15,430
something that involves gradient steps.
Then I have an approach to evaluation on

3336
01:15:15,430 --> 01:15:15,440
Then I have an approach to evaluation on
 

3337
01:15:15,440 --> 01:15:19,830
Then I have an approach to evaluation on
held outset. Wonderful.

3338
01:15:19,830 --> 01:15:19,840

 

3339
01:15:19,840 --> 01:15:22,470

The key insight

3340
01:15:22,470 --> 01:15:22,480
The key insight
 

3341
01:15:22,480 --> 01:15:26,149
The key insight
I'll use a different color for this

3342
01:15:26,149 --> 01:15:26,159
I'll use a different color for this
 

3343
01:15:26,159 --> 01:15:31,669
I'll use a different color for this
key insight.

3344
01:15:31,669 --> 01:15:31,679

 

3345
01:15:31,679 --> 01:15:35,270

is that in machine learning the default

3346
01:15:35,270 --> 01:15:35,280
is that in machine learning the default
 

3347
01:15:35,280 --> 01:15:38,149
is that in machine learning the default
the default approach to doing a problem

3348
01:15:38,149 --> 01:15:38,159
the default approach to doing a problem
 

3349
01:15:38,159 --> 01:15:41,510
the default approach to doing a problem
is to train like you're tested

3350
01:15:41,510 --> 01:15:41,520
is to train like you're tested
 

3351
01:15:41,520 --> 01:15:43,110
is to train like you're tested
with the constraint about making

3352
01:15:43,110 --> 01:15:43,120
with the constraint about making
 

3353
01:15:43,120 --> 01:15:45,510
with the constraint about making
everything differentiable.

3354
01:15:45,510 --> 01:15:45,520
everything differentiable.
 

3355
01:15:45,520 --> 01:15:47,110
everything differentiable.
Okay, that's the default. If you want to

3356
01:15:47,110 --> 01:15:47,120
Okay, that's the default. If you want to
 

3357
01:15:47,120 --> 01:15:48,630
Okay, that's the default. If you want to
make a good classifier, standard

3358
01:15:48,630 --> 01:15:48,640
make a good classifier, standard
 

3359
01:15:48,640 --> 01:15:49,669
make a good classifier, standard
classifier, what do you do? You take

3360
01:15:49,669 --> 01:15:49,679
classifier, what do you do? You take
 

3361
01:15:49,679 --> 01:15:53,110
classifier, what do you do? You take
your classifier, you run it on data, you

3362
01:15:53,110 --> 01:15:53,120
your classifier, you run it on data, you
 

3363
01:15:53,120 --> 01:15:54,709
your classifier, you run it on data, you
see what the loss is, you make sure it's

3364
01:15:54,709 --> 01:15:54,719
see what the loss is, you make sure it's
 

3365
01:15:54,719 --> 01:15:57,110
see what the loss is, you make sure it's
differentiable, and then you get better

3366
01:15:57,110 --> 01:15:57,120
differentiable, and then you get better
 

3367
01:15:57,120 --> 01:15:58,470
differentiable, and then you get better
at the thing that you want to do at

3368
01:15:58,470 --> 01:15:58,480
at the thing that you want to do at
 

3369
01:15:58,480 --> 01:16:17,990
at the thing that you want to do at
inference time.

3370
01:16:17,990 --> 01:16:18,000

 

3371
01:16:18,000 --> 01:16:19,430

Okay.

3372
01:16:19,430 --> 01:16:19,440
Okay.
 

3373
01:16:19,440 --> 01:16:22,950
Okay.
So what mammal did was just say how do I

3374
01:16:22,950 --> 01:16:22,960
So what mammal did was just say how do I
 

3375
01:16:22,960 --> 01:16:25,910
So what mammal did was just say how do I
translate this key insight that we have

3376
01:16:25,910 --> 01:16:25,920
translate this key insight that we have
 

3377
01:16:25,920 --> 01:16:29,830
translate this key insight that we have
for standard ML to this setting and so

3378
01:16:29,830 --> 01:16:29,840
for standard ML to this setting and so
 

3379
01:16:29,840 --> 01:16:31,590
for standard ML to this setting and so
what's the approach I'll talk about it

3380
01:16:31,590 --> 01:16:31,600
what's the approach I'll talk about it
 

3381
01:16:31,600 --> 01:16:33,189
what's the approach I'll talk about it
more next time but what's the approach

3382
01:16:33,189 --> 01:16:33,199
more next time but what's the approach
 

3383
01:16:33,199 --> 01:16:35,830
more next time but what's the approach
it says take the tasks that I have in my

3384
01:16:35,830 --> 01:16:35,840
it says take the tasks that I have in my
 

3385
01:16:35,840 --> 01:16:39,189
it says take the tasks that I have in my
training set of tasks

3386
01:16:39,189 --> 01:16:39,199
training set of tasks
 

3387
01:16:39,199 --> 01:16:42,310
training set of tasks
proceed to view the initialization of

3388
01:16:42,310 --> 01:16:42,320
proceed to view the initialization of
 

3389
01:16:42,320 --> 01:16:44,630
proceed to view the initialization of
them as a learnable parameter the model

3390
01:16:44,630 --> 01:16:44,640
them as a learnable parameter the model
 

3391
01:16:44,640 --> 01:16:45,910
them as a learnable parameter the model
I start with that's the learnable

3392
01:16:45,910 --> 01:16:45,920
I start with that's the learnable
 

3393
01:16:45,920 --> 01:16:48,149
I start with that's the learnable
parameter

3394
01:16:48,149 --> 01:16:48,159
parameter
 

3395
01:16:48,159 --> 01:16:51,189
parameter
fine-tune it using the SG optimizer for

3396
01:16:51,189 --> 01:16:51,199
fine-tune it using the SG optimizer for
 

3397
01:16:51,199 --> 01:16:53,430
fine-tune it using the SG optimizer for
a while,

3398
01:16:53,430 --> 01:16:53,440
a while,
 

3399
01:16:53,440 --> 01:16:57,270
a while,
evaluate it using a held out set,

3400
01:16:57,270 --> 01:16:57,280
evaluate it using a held out set,
 

3401
01:16:57,280 --> 01:17:00,070
evaluate it using a held out set,
compute the loss,

3402
01:17:00,070 --> 01:17:00,080
compute the loss,
 

3403
01:17:00,080 --> 01:17:02,790
compute the loss,
and then run gradients all the way back

3404
01:17:02,790 --> 01:17:02,800
and then run gradients all the way back
 

3405
01:17:02,800 --> 01:17:10,630
and then run gradients all the way back
through this entire process.

3406
01:17:10,630 --> 01:17:10,640

 

3407
01:17:10,640 --> 01:17:11,910

run the gradients back through the

3408
01:17:11,910 --> 01:17:11,920
run the gradients back through the
 

3409
01:17:11,920 --> 01:17:14,790
run the gradients back through the
entire learning process itself and

3410
01:17:14,790 --> 01:17:14,800
entire learning process itself and
 

3411
01:17:14,800 --> 01:17:17,189
entire learning process itself and
update the initial condition and keep

3412
01:17:17,189 --> 01:17:17,199
update the initial condition and keep
 

3413
01:17:17,199 --> 01:17:19,110
update the initial condition and keep
doing this.

3414
01:17:19,110 --> 01:17:19,120
doing this.
 

3415
01:17:19,120 --> 01:17:21,830
doing this.
Okay. So

3416
01:17:21,830 --> 01:17:21,840
Okay. So
 

3417
01:17:21,840 --> 01:17:25,270
Okay. So
second thing is re insight was second

3418
01:17:25,270 --> 01:17:25,280
second thing is re insight was second
 

3419
01:17:25,280 --> 01:17:31,910
second thing is re insight was second
insight

3420
01:17:31,910 --> 01:17:31,920

 

3421
01:17:31,920 --> 01:17:35,910

learning process

3422
01:17:35,910 --> 01:17:35,920
learning process
 

3423
01:17:35,920 --> 01:17:40,070
learning process
of SGD

3424
01:17:40,070 --> 01:17:40,080

 

3425
01:17:40,080 --> 01:17:43,830

is like an RNN.

3426
01:17:43,830 --> 01:17:43,840
is like an RNN.
 

3427
01:17:43,840 --> 01:17:47,030
is like an RNN.
So you can train it like that. Okay. So

3428
01:17:47,030 --> 01:17:47,040
So you can train it like that. Okay. So
 

3429
01:17:47,040 --> 01:17:48,550
So you can train it like that. Okay. So
we'll talk more about this next time. We

3430
01:17:48,550 --> 01:17:48,560
we'll talk more about this next time. We
 

3431
01:17:48,560 --> 01:17:50,149
we'll talk more about this next time. We
have a homework problem that walks you

3432
01:17:50,149 --> 01:17:50,159
have a homework problem that walks you
 

3433
01:17:50,159 --> 01:17:53,350
have a homework problem that walks you
through this. But uh it's good to think

3434
01:17:53,350 --> 01:17:53,360
through this. But uh it's good to think
 

3435
01:17:53,360 --> 01:17:55,510
through this. But uh it's good to think
about because this sheds insight onto

3436
01:17:55,510 --> 01:17:55,520
about because this sheds insight onto
 

3437
01:17:55,520 --> 01:17:57,910
about because this sheds insight onto
how in context learning works. Okay,

3438
01:17:57,910 --> 01:17:57,920
how in context learning works. Okay,
 

3439
01:17:57,920 --> 01:18:00,550
how in context learning works. Okay,
I'll talk about that next time.

3440
01:18:00,550 --> 01:18:00,560
I'll talk about that next time.
 

3441
01:18:00,560 --> 01:18:04,600
I'll talk about that next time.
Okay, so now I have office hours.

